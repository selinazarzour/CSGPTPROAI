Question,Answer
Geometric representation: How to represent vectors in R^4or higher geometrically if asked,"<p>The geometric interpretation of a vector $$\begin{bmatrix} \\ v_1 \\ v_2 \\ v_3 \\ v_4 \end{bmatrix}$$ is a directed line segment from </p>
<p>the initial point O (the origin) to a terminal point whose coordinates are $$(v_1, v_2, v_3, v_4)$$ , and the same applies for</p>
<p>higher-dimensional vectors. However, you can&#39;t draw them.</p>
<p>The geometric interpretation of a vector $$\begin{bmatrix} \\ v_1 \\ v_2 \\ v_3 \\ v_4 \end{bmatrix}$$ is a directed line segment from </p>
<p>the initial point, typically the origin, to a terminal point whose coordinates are $$(v_1, v_2, v_3, v_4)$$ , and the same applies for</p>
<p>higher-dimensional vectors. However, you can&#39;t draw them.</p>
We don&#39;t! We use the geometrical representation in $$\mathbb{R}^2$$ and $$\mathbb{R}^3$$ to motivate results in $$\mathbb{R}^n$$, $$n\geq 4$$, which we prove completely algebraically. "
"Does anyone have a visual representation of this to make it easy to understand: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdl9os1tne%2Faad14c2c216308c320d45b5dca2d31003ec18760371a10962a73b1028968368e%2Fimage.png"" alt=""image.pngNaN"" />","Stay tuned! I highly suspect your instructor will draw a picture which will make this all make sense. 
<p>No new picture but here is a picture I took from the course notes on Learn (:<br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2F021661594e8256ed59e78f1e8537f02a3835b3863504844a6d05cc38d20286a8%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>The projection of $$\vec v$$ onto $$\vec w$$ is the vector you get if you literally &#34;squash&#34; $$\vec v$$ onto the line drawn out by $$\vec w$$. The key point here is that in the way that it is &#34;squashed&#34;, the angle where the red arrow meets the green arrow is always a right angle (this is what is being asked to shown).<br /><br />The perpendicular of $$\vec v$$ on $$\vec w$$ is the remainder of what&#39;s left, so that<br /><br /></p>
<p>$$\text{proj}_{\vec w}(\vec v) + \text{perp}_{\vec w}(\vec v) = \vec v$$</p>"
Piazza posts: Do we get bonus marks for asking/answering questions on piazza?<div><br /></div>,No.
Tutorial slot: Is the tutorial slot to be disregarded on weeks without a quiz?,Yes
"Written problems 1: <p>Are the written problems 1 posted yet? I can&#39;t seem to find them</p>
<p></p>
<p>Thanks!</p>",They have not been posted yet
"Mobius Practice: <p>Can anyone help explain what I am doing wrong?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcd8tneb75a%2F9e666a8d05ff9b6a0062d5815f9da6b8d69fdb0754d9e545e13ad325d02becc9%2FScreenshot_2024-01-08_at_11.54.08_AM.png"" alt=""Screenshot_2024-01-08_at_11.54.08_AM.pngNaN"" width=""481"" height=""238"" /></p>","most likely need * between -7/5 and x
<p>Try what was suggested above.  (Luckily we aren&#39;t using Mobius for marks this semester.)</p>
<p></p>
<p>Update: There was a small hiccup in the code. It should work now. Let us know if it doesn&#39;t!</p>
Try what was suggested above.  (Luckily we aren&#39;t using Mobius for marks this semester.)"
"Due dates for assignments: On the Learn page, there are two different due dates for all of the written assignments, which one is the correct one?",Assignments are due on Mondays at 5:30 pm.  Where are these two different dates?
Today’s Tutorial Timeslot: I recall last term for our calculus tutorial timeslot we had an optional presentation on how to study effectively. Will something similar be happening for today’s tutorial or is there nothing happening? ,"There is nothing scheduled during tutorials during non-quiz weeks. 
There is nothing happening in today&#39;s tutorial time slot. (or any of the tutorial time slots when there isn&#39;t a quiz)"
"Can&#39;t see Mobius Feedback: <p>After submitting a practice, I click on the gradebook and it&#39;s empty. I&#39;ve tried clicking the load button, reloading and emptying cache and hard reloading, but to no avail.</p>
<p></p>
<p>Has anyone else encountered this problem? How can I access the gradebook marks?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdh2s3927me%2Fa88bd6aa64bdc47571b25032953b69a2bb9c1e037dd6d1936f660b8ea8033f57%2Fimage.png"" alt=""image.pngNaN"" /></p>","I was able to check my answer after submitting it and clicking &#34;view details&#34; on the bottom corner of the submition confirmation page
<p>It shouldn&#39;t be the case that you have to click through that many things to see solutions/feedback on a practice assessment.</p>
<p></p>
<p>You should now see a &#34;How did I do?&#34; button on the left side of the page to help speed things up!</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjv2q5bsakc6y9%2F78df7f0bc0213fe4849c3a4a3ad3f41568a32b8bc879b2be4f56b3c588bdaf7c%2Fimage.png"" alt=""image.pngNaN"" width=""597"" height=""322"" /></p>
<p></p>
<p></p>
<p>Clicking that button reveals whether or not you got it right, along with the feedback</p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjv2q5bsakc6y9%2F381238d2d10c60fee687c79b801d93cc9c31301ccfaa8348a19c8f405d4da68f%2Fimage.png"" alt=""image.pngNaN"" width=""571"" height=""520"" /></p>
<p></p>
<p>In what I can only describe as a surprise, $$\pi$$ was not the correct answer.</p>
<p>It shouldn&#39;t be the case that you have to click through that many things to see solutions/feedback on a practice assessment.</p>
<p></p>
<p>You should now see a &#34;How did I do?&#34; button on the left side of the page to help speed things up!</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjv2q5bsakc6y9%2F78df7f0bc0213fe4849c3a4a3ad3f41568a32b8bc879b2be4f56b3c588bdaf7c%2Fimage.png"" alt=""image.pngNaN"" width=""597"" height=""322"" /></p>
<p></p>
<p></p>
<p>Clicking that button reveals whether or not you got it right, along with the feedback</p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjv2q5bsakc6y9%2F381238d2d10c60fee687c79b801d93cc9c31301ccfaa8348a19c8f405d4da68f%2Fimage.png"" alt=""image.pngNaN"" width=""571"" height=""520"" /></p>"
"Question about cross product: <p>Our course notes only defined the cross product in R3, but I wonder can the cross product be performed in other dimensions such as R2 or R4?</p>
<p>Thank you</p>","<p>Recall that for $$\vec x, \vec y \in \mathbb R^3$$, $$\vec z := \vec x \times \vec y$$ satisfies:<br /><br /></p>
<p>a) $$\vec z \cdot \vec x = 0$$ and $$\vec z \cdot \vec y = 0$$<br />b) $$\vec y \times \vec x = -\vec z = -\vec x \times \vec y$$<br />b) if $$ \vec x, \vec y$$ are nonzero, then $$\|\vec z\| = \|\vec x \| \|\vec y\| \sin \theta$$ where $$\theta$$ is the angle between $$\vec x$$ and $$\vec y$$</p>
<p></p>
<p>Suppose we want to extend the cross product to other dimensions. In $$\mathbb R^2$$, suppose you have nonzero $$\vec x, \vec y \in \mathbb R^2$$ where $$\vec x$$, $$\vec y$$ are not collinear with the origin (imagine $$\vec x, \vec y$$ do not have the same direction up to positive or negative). It turns out that the only vector which is orthogonal to both $$\vec x$$ and $$\vec y$$ is the zero vector (I challenge you to think about why this might be the case and why it changes if $$\vec x, \vec y$$ are collinear with the origin). </p>
<p></p>
<p>In 3 dimensions, for nonzero vectors $$\vec x, \vec y$$ which are again not collinear with the origin, it turns out there is always exactly a line which is orthogonal to both $$\vec x, \vec y$$.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2Feecfc644bf8621cb6fe1e61277dbfcafd61fe47703f21cfaeb68fa25c843c31f%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>Here, I&#39;ve drawn $$\vec x$$ and $$\vec y$$ as blue and red, on the axis to make it clear where they point. The pink line represents all vectors which are orthogonal to both $$\vec x$$ and $$\vec y$$.</p>
<p></p>
<p>If we want to fix the magnitude of this vector, you would be able to identify exactly two vectors on this line which have the right magnitude (one going in each direction of this line). Then comes in the right-hand rule, where we need to uniquely identify which of these two vectors we give to the cross product. This unique choice always satisfies the property of skew-symmetry (think about what $$\vec x \times \vec y$$ and $$-\vec x \times \vec y$$ each represent).</p>
<p></p>
<p>However, in more than 3 dimensions, there is no longer a line which is orthogonal to both $$\vec x$$ and $$\vec y$$ but rather a higher dimensional space: for instance, in 4 dimensions, there is a plane of vectors which are all orthogonal to both $$\vec x$$ and $$\vec y$$. By specifying a specific magnitude $$(\|\vec x\| \| \vec y\| \sin \theta$$), you could identify a &#34;disc&#34; of vectors on the plane with this magnitude. At this point, you would need a much more sophisticated rule to identify which vector on this disc you want to uniquely assign to the cross product, as there are now an infinite number of vectors on this disc, as opposed to the two options in 3 dimensions.</p>
<p></p>
<p>So, in general, the cross product tends to only be used in 3 dimensions.</p>
<p></p>
<p></p>
The short answer is that the cross product is only defined in $$\mathbb{R}^3$$.   If you want more thoughts on this you can read the student answer. "
"Mobius Practice: <p>Is this the Mobius practice everyone is referring to?</p>
<p></p>
<p><a href=""https://open.math.uwaterloo.ca/1"" target=""_blank"" rel=""noopener noreferrer"">https://open.math.uwaterloo.ca/1</a></p>
<p></p>
<p>Are we supposed to be doing this?</p>","No, this is something else. Reading these notes should be considered optional and not a part of our course. "
"Standard Basis for Rn: Just a question about Standard Basis, isn&#39;t it same as scalar multiplication and diagonalization?",Could you elaborate on your question? What do you mean by being the same?
"question about vector equality: in the lecture we learnt 2 method of justifying whether 2 vectors are equal. One of them is that if their direction and magnitude are the same, the vectors are the same. So I wondering whether [1 2]^T and [1 2 0]^T are equal to each other. ","<p>These two vectors are in different spaces. One is in $$\mathbb{R}^2$$ and one is in $$\mathbb{R}^3$$. A comparing law not defined unless more (embedding or equivalence) rules are introduced. For example, one cannot compare $$2\in \mathbb{R}$$ and $$\begin{bmatrix}2\\ 1 \end{bmatrix} \in \mathbb{R}^2$$.</p>
<p></p>
<p> </p>
<p>These two vectors are in different spaces. One is in $$\mathbb{R}^2$$ and one is in $$\mathbb{R}^3$$. A comparing law not defined unless more (embedding) rules are introduced. For example, one cannot compare $$2\in \mathbb{R}$$ and $$\begin{bmatrix}2\\ 1 \end{bmatrix} \in \mathbb{R}^2$$.</p>
<p></p>
<p> </p>
<p>These two vectors are in different spaces. One is in $$\mathbb{R}^2$$ and one is in $$\mathbb{R}^3$$. It is not defined unless more (embedding) rules are introduced. For example, one cannot compare $$2\in \mathbb{R}$$ and $$\begin{bmatrix}2\\ 1 \end{bmatrix} \in \mathbb{R}^2$$.</p>
<p></p>
<p> </p>"
"Quiz Practice Resources: <p>Hello!</p>
<p></p>
<p>I&#39;m just wondering if, in addition to the weekly Mobius and written practice problems, we will have something similar to a &#34;practice quiz&#34; in this course?</p>","No, there won&#39;t be practice quizzes."
"Vector Notation: When we are given the vector notation &lt;a,b....&gt;^T. Does T refer to the number of elements in column arranged in rows?","Im pretty sure the T is just for saying that its a vector but you are writing it in that way
<p>When we write</p>
<p></p>
<p>$$\begin{bmatrix} a & b & c \end{bmatrix}^T$$</p>
<p></p>
<p>or</p>
<p></p>
<p>$$\begin{bmatrix} a & b & c \end{bmatrix}^\top$$</p>
<p></p>
<p>we mean the same thing as</p>
<p></p>
<p>$$\begin{bmatrix} a \\ b \\ c \end{bmatrix}$$.</p>
<p></p>
<p>The ^T denotes the transpose of a matrix (imagine drawing a line through the diagonal and flipping all elements across it): e.g.<br /><br />$$\begin{bmatrix} 1 & 2 & a \\ b & c & 4\end{bmatrix} ^ \top = \begin{bmatrix} 1 & b \\ 2 & c \\ a & 4 \end{bmatrix}$$</p>
<p></p>
<p>We tend to write $$\begin{bmatrix} a & b & c \end{bmatrix}^\top$$ as an inline vector so that it doesn&#39;t take up too much vertical space.</p>
<p>When we write</p>
<p></p>
<p>$$\begin{bmatrix} a & b & c \end{bmatrix}^T$$</p>
<p></p>
<p>or</p>
<p></p>
<p>$$\begin{bmatrix} a & b & c \end{bmatrix}^\top$$</p>
<p></p>
<p>we mean the same thing as</p>
<p></p>
<p>$$\begin{bmatrix} a \\ b \\ c \end{bmatrix}$$.</p>
<p></p>
<p>The ^T denotes the transpose of a matrix (imagine drawing a line through the diagonal and flipping all elements across it): e.g.<br /><br />$$\begin{bmatrix} 1 & 2 & a \\ b & c & 4\end{bmatrix} ^ \top = \begin{bmatrix} 1 & b \\ 2 & c \\ a & 4 \end{bmatrix}$$</p>
<p></p>
<p>We tend to write $$\begin{bmatrix} a & b & c \end{bmatrix}^\top$$ as an inline vector so that it doesn&#39;t take up too much vertical space.</p>"
"Unit vector questions: <p>The square of a unit vector will still be the unit vector itself right?</p>
<p></p>",What do you mean by the &#34;square of a unit vector&#34;?
Lost Hat from MC 4059: Hi<div>Has anybody seen a light brown winter hat from MC 4059 (or in general)</div><div>Its a furry light brown hat with two puffballs attached to a string with like bear ears on top</div><div>If so please email me</div><div>dy2kim@uwaterloo.ca </div><div>Thank you</div>,"It has been located, thank you"
"Geometric Interpretation.: Let -»x, -»y ∈ R^n . Prove that ||-»x &#43; -»y ||^2 &#43; ||-»x − -»y ||^2 = 2(||-»x||2 &#43; ||-»y ||2 ). Can you come up with a geometric interpretation of this identity? I&#39;m not understanding what geometric interpretation means?","Hint: think about the diagonals of the parallelogram formed by the two vectors x and y. 
« Geometric interpretation » means to show this proof as a graph or shape or some other visual. <div><br /></div><div>Most likely in $$R^2$$ because it’s the easiest, but not always / doesn’t have to be. Instructor answer gives a hint on what you should look for. <div><br /></div><div>Hope that helps!</div></div>"
"Is section 1.7 covered?: <p>In the course schedule page on learn, it says the coverage for week 1 is 1.1 to 1.9 from the course notes. However, in the practice problems coverage for week 1, it excludes section 1.7 (i.e. 1.1-1.6, 1.8-1.9). Does this mean section 1.7 is not tested/covered? More specifically, is standard inner product a testable topic?</p>
<p></p>","My professor said we would not be covering 1.7 
We will not be covering 1.7."
When will the answer of w1pp be released?: When will the answer of practice problems 1 be released?,Solutions to the practice problems will be posted on the Thursday following the week they cover.  So the solutions to week 1 practice problems will be posted next Thursday. 
"Reference Sheet for Assessments: <p>Hello!</p>
<p></p>
<p>I&#39;m wondering if we will be provided with reference sheets for quizzes and/or exams in this course similar to those in MATH 135?</p>
<p></p>
<p>Thanks!</p>",There are no reference sheets. 
"Vector subtraction: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwatgzzdt48x%2F99d143f23c4e4bf585f4a707c88d0bc196cde211fbcb1924f6991397555d4253%2FScreen_Shot_2024-01-12_at_11.48.51_AM.png"" width=""342"" height=""154"" alt="""" /> </p>
<p></p>
<p>Could someone explain the step on how to do (vector v) - projection of v onto w</p>","You multiply the scalar 2/5 to the vector &lt;7,1&gt; and then subtract it from &lt;4,-8&gt;.<div><br /></div><div><br /></div>"
"Geometric interpretation of the dot product: <p>Could someone show me a geometric representation of the dot product?<br />I learned from lecture that</p>
<p>U dot V = ||U|| ||V|| cos θ </p>
<p>My first thought would be the component of U on V but that is the definition of projection so it is simply not true. The projection is a vector whereas a dot product is a scalar. Since there is a scalar there is no way of representing it geometrically so I guess I just answered my own question here, but I am still not sure what the dot product is besides the two definition that was provided in class.<br />1. U dot V = the sum of the multiples of the components of u and v <br /><br /></p>
<p>2. U dot V = ||U|| ||V|| cos θ <br />I am also not sure how the two formula are related.<br /><br /></p>
<p>Please give me another way I could think about the dot product other than the definitions themselves as well as how the two definitions are related to one another.</p>
<p></p>","It’s basically the product of magnitude of the component of u along v into magnitude of v.<div><br /></div><div>Geometrically you could make two vectors u and v. Get a projection of u over v. The product of magnitude of projection and vector v represents dot product.</div>
<p>The dot product is a function that encodes a bit of information about the magnitudes and directions of the two vectors.</p>
<p></p>
<p>If $$\theta$$ is the angle between $$\vec u$$ and $$\vec v$$, then $$\cos \theta$$ tells you the general direction the two vectors point. If they point in opposite directions, then $$\theta = \pi/2$$ so $$\cos \theta = -1$$ so the dot product will be negative. If they point in the same direction, then $$\theta = 0$$ so that $$\cos \theta = 1$$. You can check for yourself that when the dot product is positive, $$\vec u$$ and $$\vec v$$ point in the general same direction (i.e. $$0 < \theta < \pi/4$$) and if the dot product is negative, then $$\vec u$$ and $$\vec v$$ point in general opposite directions.<br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2Fd53ced3493fc00d90fcd706981794adcc0a47bb4d900b1c06922e5351847f979%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Here is one way to think of the dot product: consider the projection of one of the vectors onto the other (it doesn&#39;t matter $$\vec u$$ onto $$\vec v$$ or $$\vec v$$ onto $$\vec u$$, but here I&#39;ve illustrated $$\vec u$$ onto $$\vec v$$).</p>
<p></p>
<p>By some trigonometry, we can find that the length of the projection is $$\cos \theta \| \vec u\|$$, and the dot product now is the simple (real-valued) product of $$\cos(\theta) \|\vec u\| \cdot \|\vec v\| = \|\vec u \| \|\vec v\| \cos \theta$$.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2Ff87015937752aa07a432358ea00ab98e2f1bbd5528541ae293e94b4358796682%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>To see why the two formulae are equivalent, I&#39;ll leave you a hint: the above is a drawing of a triangle with sides $$\vec u$$, $$\vec v$$, and $$\vec v - u$$. Apply the law of cosines and see what happens.</p>"
prof Fernandez office hour: It was mentioned in class that prof Fernandez&#39;s office hour might change from the original 5-7 on Wednesday. I haven&#39;t seen anything on learn as to when the office hours had been changed to. Could this be confirmed. Many thanks.,"Thank you for reminding me about this; I forgot to mention in the morning lecture, but I will in the afternoon lecture. The office hours will be changed to Tuesdays 4:30-6:30. I will reflect this in the first lecture notes and this will be also added to the learn page."
"[w1pp - Q1]: <p>I&#39;m a little confused about my solution, I got that $$\underset{u}{\rightarrow}$$ = $$\underset{0}{\rightarrow}$$ or a = 0 but then wasn&#39;t sure how to calculate the rest.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8rpo1vu6yt%2F4f08900f0be1e7a7199187107ff3ce38a37e02985cfa68ee5165aafcfb3caf5a%2FScreenshot_2024-01-12_at_3.51.48_PM.png"" alt=""Screenshot_2024-01-12_at_3.51.48_PM.pngNaN"" width=""880"" height=""33"" /></p>","You have the correct solution. When you determined that $$a = 0$$ satisfies the equation, it does not depend on the choices of $$\vec u, \vec v$$, so they can take on any value. In situations like these, we call $$\vec u$$ and $$\vec v$$ free variables.<div><br /></div><div>Similarly, in the case when $$\vec u = 0$$, the equation holds independent of the choices of $$a$$ and $$\vec v$$, so they may take on any values.</div>
Vector v belongs to R^5 and<div>If a = 0 then u belongs to R^5 and if u is a zero vector then a belongs to R.</div><div><br /></div><div>I believe this is how you would express all u, v and a.</div>
Vector v belongs to R^5 and<div>If a = 0 then u belongs to R^5 and if u is a zero vector then a belongs to R.</div><div><br /></div><div>I believe this is how you would express the whole all u, v and a.</div>"
"parallelogram area in R2: When given vectors $$\vec{u},\vec{v}$$ In $$\mathbb{R}^2$$ can we calculate area using $$A=||\vec{u}||||\vec{v}||\sin\theta$$ or is this only in  $$\mathbb{R}^3$$? Is using $$A=(||\vec{u}||)(||perp_\vec{u}(\vec{v})||)$$ a correct approach?","I guess it should be $$\mathbb{R}^{3}$$ because if you are defining area (which is a vector quantity defined in the axis perpendicular to both $$ \overrightarrow{u}$$ and $$\overrightarrow{v}$$) then it isn&#39;t possible for it to exist in $$\mathbb{R}^{2}$$ as the only vector which will be orthogonal to $$\overrightarrow{u}, \overrightarrow{v}$$ vector in $$\mathbb{R}^{2}$$ is $$\overrightarrow{0}$$ thus implying area to be 0.<br /><br /><br />
I guess it should be $$\mathbb{R}^{3}$$ because if you are defining area (which is a vector quantity) then it isn&#39;t possible for it to exist in $$\mathbb{R}^{2}$$ as the only vector which will be orthogonal to $$\overrightarrow{u}, \overrightarrow{v}$$ vector in $$\mathbb{R}^{2}$$ is $$\overrightarrow{0}$$ thus implying area to be 0.<br /><br /><br />
<p>Yes, in $$\mathbb{R}^2$$, area is well-defined as long as you have a norm. In $$\mathbb{R}^2$$, you don&#39;t have cross product.</p>
<p></p>
<p>If your $$\theta$$ is the angle between $$\vec{u}$$ and $$\vec{v}$$, then $$\|\vec{u}\| \|\vec{v}\| \sin \theta$$ and $$\|\vec{u}\| \| \mathrm{proj}_{\vec{u}}(\vec{v})\|$$ are the same. I would recommand you prove it before using it.</p>
<p></p>
<p>Hints:</p>
<ul><li>Using basic trig idenity, you can deduce $$\|\vec{v}\|^2\sin(\theta)^2 = \|\vec{v}\|^2 (1-\cos(\theta)^2)= \|\vec{v}\|^2 - \|\vec{v}\cos(\theta)\|^2 = \|\vec{v}\|^2- \|\mathrm{proj}_{\vec{u}}(\vec{v})\|^2$$.</li></ul>
<p></p>
<ul><li>On the other hand, using the techniques learned in class, </li></ul>
<p>              $$\|\mathrm{perp}_{\vec{u}}(\vec{v})\|^2 = (\vec{v} - \mathrm{proj}_{\vec{u}}(\vec{v}))\cdot (\vec{v} - \mathrm{proj}_{\vec{u}}(\vec{v}))$$</p>
<p>               $$=  \|\vec{v}\|^2 {\color{red}- 2(\vec{v}\cdot \mathrm{proj}_{\vec{u}}(\vec{v})) + \|\mathrm{proj}_{\vec{u}}(\vec{v})\|^2} = \|\vec{v}\|^2 {\color{red}-\|\mathrm{proj}_{\vec{u}}(\vec{v})\|^2}$$.</p>
<p>Yes, in $$\mathbb{R}^2$$, area is well-defined as long as you have a norm. In $$\mathbb{R}^2$$, you don&#39;t have cross product.</p>
<p></p>
<p>If your $$\theta$$ is the angle between $$\vec{u}$$ and $$\vec{v}$$, then $$\|\vec{u}\| \|\vec{v}\| \sin \theta$$ and $$\|\vec{u}\| \| \mathrm{proj}_{\vec{u}}(\vec{v})\|$$ are the same. I would recommand you prove it yourself.</p>
<p></p>
<p>Hints:</p>
<ul><li>Using basic trig idenity, you can deduce $$\|\vec{v}\|^2\sin(\theta)^2 = \|\vec{v}\|^2 (1-\cos(\theta)^2)= \|\vec{v}\|^2 - \|\vec{v}\cos(\theta)\|^2 = \|\vec{v}\|^2- \|\mathrm{proj}_{\vec{u}}(\vec{v})\|^2$$.</li></ul>
<p></p>
<ul><li>On the other hand, using the techniques learned in class, </li></ul>
<p>              $$\|\mathrm{perp}_{\vec{u}}(\vec{v})\|^2 = (\vec{v} - \mathrm{proj}_{\vec{u}}(\vec{v}))\cdot (\vec{v} - \mathrm{proj}_{\vec{u}}(\vec{v}))$$</p>
<p>               $$=  \|\vec{v}\|^2 {\color{red}- 2(\vec{v}\cdot \mathrm{proj}_{\vec{u}}(\vec{v})) + \|\mathrm{proj}_{\vec{u}}(\vec{v})\|^2} = \|\vec{v}\|^2 {\color{red}-\|\mathrm{proj}_{\vec{u}}(\vec{v})\|^2}$$.</p>"
"dot product and proj visualization: <p>I&#39;m a little confused about the difference between the visualization of the dot product and the proj of one vector on the other. </p>
<p></p>
<p>Also, I&#39;m slightly confused as to what the scalar value the dot product produces represents. </p>","<p>The geometric interpretation of the dot product of vectors $$\vec{u}$$ and $$\vec{v}$$ is that it calculates the product of the length of the two vectors and the cosine of the angle between them.  $$\vec{u} \cdot \vec{v} = ||\vec{u}|| ||\vec{v}|| \cos \theta$$.</p>
<p></p>
<p>The projection of $$\vec{v}$$ onto $$\vec{u}$$ can be thought of the shadow that $$\vec{v}$$ would make on $$\vec{v}$$ if a light was shone directly above.  We can also think of it the as the component of $$\vec{v}$$ the goes in the direction (or possibly opposite direction) of $$\vec{u}$$. </p>
<p>just to add, if you have a little bit of familiarity with highschool physics, I like to think of the projection of  u  onto  v  as the component of u that is in the direction of v.</p>
<p></p>
<p>hopefully that helps with some intuition :)</p>"
Parallel vectors: Is every vector in R3 parallel with the zero vector?,"<p>Two vectors are parallel if they are scalar multiples.  The zero vector is a scalar multiple of all vectors in $$\mathbb{R}^3$$ and so by convention we can say that the zero vector is parallel to every vectors in $$\mathbb{R}^3$$.  However, the dot product of the zero vector with every vector in $$\mathbb{R}^3$$ is 0 and so we can also say that the zero vector is orthogonal (or perpendicular) with every vector in $$\mathbb{R}^3$$. </p>
<p></p>
<p>Essentially the zero vector can be thought of having no direction or all directions and that&#39;s why we can interpret it as being parallel to all vectors and orthogonal to all vectors. </p>"
cross product in R3: Why does the cross-product only work in R3. Can vectors not be perpendicular in higher dimensions?,"<p>In all dimensions you can find vectors which are orthogonal.  The cross product just gives us a convenient way to find a vector which is orthogonal to two vectors in $$\mathbb{R}^3$$. </p>
<p></p>
<p></p>"
Quiz 1 content: How far does quiz 1 cover? I can&#39;t seem to find it on LEARN,"Quiz 1 covers the contents of Chapter 1 that were covered in class, that is, 1.1-1.6 and 1.8-1.9. Inner Products are not included."
"Möbius Login: <p>Hi,</p>
<p></p>
<p>When I try to log into Möbius, it says &#34;Login unsuccessful.&#34; Is anyone else having this error?</p>
<p></p>
<p>Thanks!</p>",You can&#39;t log into Mobius you need to access it through the links in LEARN. 
"Q2 pratice: How can you guys do the last one?<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2Fe8ae96bcffd5184c99baef63171cf09c5cf58eca29fd2a726910fbba71c0a5fa%2Fimage.png"" alt=""image.png"" />","<p>here’s what I have <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc0mgoaj4v5%2F95e6d4ee4329f8cf84076177e4a19977a74e4b4d4e37f2143350354bf6c491b8%2Fimage.png"" alt=""image.pngNaN"" width=""424"" height=""345"" /><br /><br /></p>
<p>edit: I made a mistake on the last step (misread 4 as 9), it should be $$\vec{u}=(\frac{1}{1+i})\begin{bmatrix}2-3i\\-9+5i\end{bmatrix}$$</p>
<div></div>
<div></div>
<div></div>
<p>here’s what I have <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc0mgoaj4v5%2F95e6d4ee4329f8cf84076177e4a19977a74e4b4d4e37f2143350354bf6c491b8%2Fimage.png"" alt=""image.pngNaN"" width=""424"" height=""345"" /></p>
<div></div>
<div></div>
<div></div>
here’s what I have <div></div><div></div><div><br /></div>
here’s what I have <div></div>
here’s what I have<div></div>
I didn&#39;t check all the arithmetic of the student answer but the general approach looks good. "
"Q5 from practice: <p>How do you do the backward proof after getting the dot product of vector x and vector y is 0 from knowing they are orthogonal?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2F010170107e76f40d65e336052a707fe70e1d3211950e4fc821d499e146626a75%2Fimage.png"" alt=""image.pngNaN"" /></p>",Hint: Express $$\|\vec{x} + \vec{y}\|^2$$ in terms of dot product and expand the dot product using linearity.
"Question 8 from the practice: <p>Could someone give me a hint to solve this?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc43j3qa5i2%2Fd0391f0ebf5f277dd536109e89d2181e6deb2780acd04c09cf63ea89d333039f%2Fimage.png"" alt=""image.png"" /></p>",Write $$\vec{u}=\begin{bmatrix}u_1\\ u_2\\ u_3\end{bmatrix}$$ and $$\vec{v}=\begin{bmatrix}v_1 \\v_2\\ v_3\end{bmatrix}$$. Work out $$(\vec{u}+c\vec{v})\times \vec{v}$$ and $$\vec{u}\times \vec{v}$$ explicitly.
"PP1 Q3 (recommended): <p>I was wondering if what I did here was correct:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F3fb72ac0d2620cd6265f0d55b18a3a761b2bde2a91fe6ecb34371a56cfb9edac%2Fimage.png"" alt=""image.png"" /></p>","<p>This might be a commonly made mistake. One cannot &#34;cancel&#34; $$\vec{w}$$ in $$\frac{\vec{v}\cdot \vec{w}}{\vec{w}\cdot \vec{w}}$$. $$\frac{\vec{v}}{\vec{w}}$$ is not well-defined.</p>
<p></p>
<p>Moreover, $$\frac{\vec{v}\cdot \vec{w}}{\vec{w}}$$ is undefined.</p>
This might be a commonly made mistake. One cannot &#34;cancel&#34; $$\vec{w}$$ in $$\frac{\vec{v}\cdot \vec{w}}{\vec{w}\cdot \vec{w}}$$. $$\frac{\vec{v}}{\vec{w}}$$ is not well-defined.
This might be a commonly made mistake. One cannot &#34;cancel&#34; $$\vec{w}$$ in $$\frac{\vec{v}\cdot \vec{w}}{\vec{w}\cdot \vec{w}}$$. $$\frac{\vec{v}{\vec{w}}$$ is undefined. "
"Pp1 q4: <a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwea27oxg6a4%2Fb75e677d5caeed611b39400b017ba7a6f225acda081e269a01247600f713a3f2%2FIMG_0044.png"" target=""_blank"" rel=""noopener noreferrer"">IMG_0044.png</a> I am wondering if what I did here is correct.","No.  Sorry.  You are mixing up the dot product with scalar multiplication.  You also don&#39;t have the definition of projection correct.  The denominator of the scalar at the front should be the length of $$\vec{w}$$ squared. 
I believe there&#39;s a mistake with $$\frac{(\vec{v}+\vec{u})\cdot \vec{w}}{||\vec{w}||^2}\vec{w}=(\vec{v}+\vec{u})\cdot\hat{w}^2$$ as we haven&#39;t covered squaring vectors.<br /><br />try expanding the dot product $$(\vec{v}+\vec{u})\cdot \vec{w} = (\vec{v} \cdot \vec{w}) + (\vec{u} \cdot \vec{w})$$"
"Can someone help explain the projection formula?: For example, I don&#39;t get why normalizing is part of the formula. I also just don&#39;t get in general why each variable is there and why it is important","We know that the projection of $$\vec{v}$$ onto $$\vec{w}$$ is a scalar multiple of $$\vec{w}$$.  How do we find the scalar involved?  Check out this slide from my lecture.  Let me know if you have any questions.  <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fidt8cfyuv993c4%2Fee3fb94c2a98be36360b4d6ab3a5ceda90f2da36eb50b5e88870a61a1d84a848%2FScreen_Shot_2024-01-14_at_10.32.57_PM.png"" alt=""Screen_Shot_2024-01-14_at_10.32.57_PM.pngNaN"" />"
"When are the weekly practice problems posted each week?: <p>On what day and at what time? Thanks</p>
<p></p>
<p>Would be great if they were available immediately after the earliest 136 classes end on Monday</p>
<p></p>
<p></p>",I am the one posting the weekly practice problems.  I&#39;ll try to get them posted earlier. 
"Do we have tutorial today?: Hi, just want to know if today&#39;s tutorial is required to show up",Tutorial time slots are only used for quizzes.  If there isn&#39;t a quiz then you don&#39;t need to show up. 
"Q7 practice : check proof: <p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb00ovad5em%2F15aa6b5462abcc6ecbc1905081746c85f3e67f7fdff284d4c781228a8d143c52%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>can someone please check if this is a valid solution? thank you so much.</p>
<p>btw I didn&#39;t write that c = v2/u2</p>","It looks good, but what if any of the entries of the vector u is 0? You have to consider that since you are deriving by these values."
Can we use defination without proving it?: I just wondering can we use the aXb = ||a|| ||b|| sin(theta) without proing it in the test?,"I assume you mean $$||\vec{a} \times \vec{b}||=||\vec{a}|| ||\vec{b}||\sin \theta$$ because what you wrote is not correct.  And yes, you can use this fact without proving it. "
"Final -- Calculators Allowed?: <p>I was reviewing the course outline again and noticed that the midterm specifies <strong>no calculators</strong> but the quizzes and the final <strong>do not explicitly address </strong>whether calculators are allowed. I&#39;m assuming they are not allowed since 135, 137, and 138 do not permit them. However, I just want to confirm so I&#39;m not taken by surprise or anything.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2Fed2cd6398aa48d72480773a8895fa4d9bcbf045b2cba7c23f7adae6fc662d49b%2Fimage.png"" alt=""image.pngNaN"" /></p>",Calculators are not permitted for any of the proctored assessments. 
"Plan through the origin: <p>Hi! I was a little bit confused on the third point of the following picture.  I do not really understand why that would be a condition in order for the plane to go through the origin. </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdjis2njc7%2F946a6df5aa984288ab4ae20e6d375eccb679870be703b1f11b7f92e2a069c06d%2Fimage.png"" alt=""image.pngNaN"" width=""728"" height=""277"" /></p>
<p></p>
<p>Thanks! </p>","The general vector equation for a plane is of the form $$\vec{p}=\vec{u}+a\vec{v}+b\vec{w}$$, where $$\vec{u}$$ is a point on the plane. The third point says that this plane goes through the origin if the single choice $$\vec{u}=\vec{0}$$ also gives the equation of the plane.
The general vector equation for a plane is of the form $$\vec{p}=\vec{u}+a\vec{v}+b\vec{w}$$, where $$\vec{u}$$ is a point on the plane. The third point says that this plane goes through the origin if the single choice $$\vec{u}$$ also gives the equation of the plane. "
"projection of vector: I&#39;m a bit confused as to why it&#39;s wrong.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2Ff38869290ac5a943c1d5b8741b6272d23b1ce557468dce066ffdba74dd0d2517%2FWechatIMG1193.jpg"" width=""1881"" height=""1386"" alt="""" />","Remember that if $$\alpha$$ is a scalar and $$\vec{v}$$ is a vector, $$\alpha\cdot\vec{v}$$ means scalar multiplication. If both $$\vec{v}$$ and $$\vec{w}$$ are vectors then $$\vec{v}\cdot\vec{w}$$ means dot product. Context is everything. 
Remember that if $$\alpha$$ is a scalar and $$\vec{v}$$ is a vector, $$\alpha\cdot\vec{v}$$ means scalar multiplication. If both $$\vec{v}$$ and $$\vec{w}$$ are vectors then $$\vec{v}\cdot\vec{w}$$ means dot product. 
Yah I don’t see why it’s wrong"
Parallel to the plane: Can someone try and explain what it means for a vector to be parallel to the plane? ,Essentially it means that the vector can be written as a linear combination of the direction vectors of the plane. 
"Linear combination: Can I determine whether vector a is a linear combination of vector b and vector c by solving system of equations? If there&#39;s a solution, then yes, vice versa.","You sure can! That will be the topic of chapter 3.
You sure can! that will be the topic of chapter 3. "
"Doubt Regarding Crowdwmark: Hi, I just wanted to confirm that the crowd mark is not updated yet for the MATH 136 course, right?<br /><br />As  it is not showing me the course yet.<br /><br />And one more thing is that where can i find the latex assignment and where to submit it.","<p>You shouldn&#39;t see anything on Crowdmark yet, this is ok.</p>
<p></p>
<p>Written Assignment 1 has not been released yet either. You can expect to see that online early next week.</p>"
"Example 2.4.6: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl3a6u2rficd4fp%2F2bb556b394fde272d702e09a1093a600eaf45b64649477697b8caa6236d88183%2FScreenshot_2024-01-17_at_5.49.47_PM.png"" alt=""Screenshot_2024-01-17_at_5.49.47_PM.pngNaN"" width=""445"" height=""95"" /></p>
<p></p>
<p>I am a little confused about how span works in this example. In this example, would the span of M and P be the same? If not, how would the span of M be represented as? </p>","<p>The span of a set of vectors always includes the origin (consider simply taking all of the scalars to be $$0$$).</p>
<p></p>
<p>You can clearly see that</p>
<p></p>
<p>$$\mathcal P = \operatorname{span}\left\{\begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}, \begin{bmatrix} -1 \\ 2 \\ -3 \end{bmatrix}\right\}$$</p>
<p></p>
<p>hence</p>
<p></p>
<p>$$\operatorname{span} \{\mathcal P\} = \mathcal P$$.</p>
<p></p>
<p>However, $$\mathcal M$$ is not the span of any set of vectors (I challenge you to think about what the span of $$\mathcal M$$ is).</p>
<p></p>
<p></p>"
"Cross product: Are we forced to use the formal formula for cross product or can we do it any way we want and still get full marks,",You can use whatever correct memory tool you have to calculate the cross product and you will get full marks. 
How do you find the distance between two parallel planes in R3: also how do you find the distance between 2 parallel lines :(,"I explore this exact question in a problem that I gave my students.  You can check out the discussion in this youtube video:  <a href=""https://youtu.be/A0UIJNdSz68?si=QOvYqGeLTLBVDA0U"" target=""_blank"" rel=""noopener noreferrer"">https://youtu.be/A0UIJNdSz68?si=QOvYqGeLTLBVDA0U</a>"
"Q8b: Can some one help me with this question? Tks!<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F8f4e7727a043bf47e3605f23e2d3f1f7b905216494349050ca71ccb8afe921ef%2FScreenshot_2024-01-18_at_1.03.49_AM.png"" alt=""Screenshot_2024-01-18_at_1.03.49_AM.pngNaN"" />","For this question, most unfortunately, your best bet is to write out each vector in terms of its components and simply show the left and right sides are equal. 
<p>I will explain what I did.</p>
<p></p>
<p>Step 1: </p>
<p>First of all, I did cross multiplication of vector v and vector w</p>
<p>I did:</p>
<p>vector u dot &lt;(v2w3-v3w2) , -(v1w3-v3w1) , (v1w2-w2v1)&gt;^T</p>
<p></p>
<p>Step 2:</p>
<p>Then I applied dot product on vector u. I got:</p>
<p>u1v2w3-u1v3w2-u2v1w3&#43;u2v3w1&#43;u3v1w2-u3v2w1</p>
<p></p>
<p>Step 3</p>
<p>&lt;-v1(u2w3-u3w2) v2(u1v3-u3w1) -v3(u1w2 - u2w1)&gt;^T = -&lt;v1(u2w3-u3w2) - v2(u1v3-u3w1) v3(u1w2-u2w1)&gt;^T because of scalar multiplication. Think of -1 as the scalar of the vector that is distributed inside each row of the vector. </p>
<p></p>
<p>Step 4</p>
<p>Bring v1 v2 and v3 out</p>
<p>this simplifies to v dot -(u x w)</p>
<p>I hope this helps.</p>"
What will quiz 1 cover: What chapters will we be tested during quiz 1?,Check out the learn announcement. 
"Span of 2 Vectors: <p>Hey, I&#39;m having a hard time visualizing the span of 2 vectors - and how it creates a plane?</p>
<p></p>
<p>Currently, I&#39;m trying to think of it as adding a scaled vector (v) and a scaled vector (w).</p>
<p></p>
<p>So is it like the first director vector can &#34;take care of creating a line&#34; (&#34;one edge of the paper&#34;). While the second direction vector takes of &#34;creating the surface area of the paper&#34;?</p>","I think your way is perfectly valid, but the way I like to imagine a plane is to get my notebook and imagine two vectors coming out of that notebook. Then I think about how another notebook can be place in a way such that it is on top both of those vectors. I hope that makes sense
I like your way of thinking about it. It&#39;s similar to the way I think about it. "
"Q5 missing: <p>I think the solution is missing the way around right ? It is iff</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F759d91e6fdd8daaf6889c3d165bdba2487fa0ec8b962bb7e0461d7fb0bb9bed1%2FScreenshot_2024-01-18_at_4.15.21_PM.png"" alt=""Screenshot_2024-01-18_at_4.15.21_PM.pngNaN"" /></p>",Notice the arrows used are the biconditional arrows.
"Week 2 Practice, Q7: How to solve a system of 3 equations with 4 unknowns: <p>I am working on the question attached below. I have substituted the three points into the general cubic equation form $$y = ax^3 + bx^2 + cx + d$$ and I have obtained 3 equations with 4 unknowns. I am not sure how to proceed, so I was wondering if I could get a hint.</p>
<p></p>
<p>Thank you.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwenjljntyg%2F7b3c2b3bed5c04b57db2f305efc2853e4ceef6d3dfe0a5425172844477af3018%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>","I think you will find next week’s material extremely relevant here. Stay tuned! 
I think you will find next week’s material extremely relevant here. Stay tuned! "
"Question about cross product: Hi, I&#39;m wondering if the question only tells us that it&#39;s two vectors in R^3 multiplied together, how do we know if we are asking for a cross product or a dot product?","Notationally, these questions appear differently. The cross product appears as $$\overrightarrow{x} \times \overrightarrow{y}$$ and the dot product appears as $$\overrightarrow{x} \cdot \overrightarrow{y}$$. You can also see an example of this in the week 1 written practice problems, question 5. Let me know if you need any more clarification! <div><br /></div><div>Edit: if you’re specifically referring to a question that says « multiply $$\overrightarrow{x}$$ and $$\overrightarrow{y}$$ in $$\mathbb{R}^3$$ », that’s pretty vague and I don’t think they would do that. If you have an example with that exact wording please send it because I’d be confused too. </div>
Notationally, these questions appear differently. The cross product appears as $$\overrightarrow{x} \times \overrightarrow{y}$$ and the dot product appears as $$\overrightarrow{x} \cdot \overrightarrow{y}$$. You can also see an example of this in the week 1 written practice problems, question 5. Let me know if you need any more clarification! 
The question will never say &#34;multiply these two vectors&#34; because that is not clear what is being asked.
The question will never say &#34;multiply these two vectors&#34; because that is not clear what is being asked. "
"Can I please be added to Crowdmark?: Hi, I enrolled in this course late and I&#39;m not in the Crowdmark. My student number is 21013490 and my email is c29yang@uwaterloo.ca","None of us have been added to crowdmark yet either since we haven’t had any assessments yet. You can also refer to @62 since they posted a similar question. The first quiz is on Monday so make sure you have a seat on odyssey for that, otherwise reach out to math136@uwaterloo.ca
As mentioned above, you don&#39;t have to worry about Crowdmark. But the roster has been synced and your name is on the roster there."
"Weekly problem: for question c, can I leave my answer as [-4, 7.5, -8.5]? or in fractions instead of leaving 1/2 outside.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F2d4329f0597e81088a2c0fa9183edf26296db0a4c6b238fc7b4b053059c28584%2FScreenshot_2024-01-20_at_11.11.20.png"" width=""1760"" height=""460"" alt="""" />",Both answers should be accepted.
"WP1 Q8a: <p>Here are the question and solution posted for Q8a:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2F772abbd73a5259c1def867940529829ffaacb88e22188faddae9992ec0310beb%2Fimage.png"" alt=""image.png"" width=""392"" height=""91"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2F0f18f94637534779f88f2c14cb2e912463adb838bb9c988390ac42ca84137033%2Fimage.png"" alt=""image.png"" width=""336"" height=""158"" /></p>
<p>Do we need to prove it like this, or can we just use the linearity of the cross product to &#34;distribute&#34; it and then use the fact that v x v = 0?</p>","Linearity of the cross product in the first argument is not proven in the course notes (although it is proven for the second argument and follows rather straightforwardly from skew-symmetry). You should still aim to prove every fact before using it, though."
"WP1 Q2c: Does this problem cover the material of 1.7? The inner product, which I thought is not covered in our course.",The first weekly practice problem set does not cover the content of 1.7.
Are there any practice problems for Quiz 1 on Monday?: ,There are some practice problems on the learn (see weekly practice problems).
"Are we required to cite properties on the quiz: Such as the vector addition, scalar multiplication, dot product and cross product properties?","Some basic operations such as vector addition and multiplication don’t require citation, but if you use a bigger theorem or lemma such as linearity of cross product, please reference them."
"Q1c: <p>Hey, I was wondering if someone could explain how they distributed this:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F63c4793c8441ba4242a1aeb9b5f740895dd93689eda0945837c6df6481094e13%2Fimage.png"" alt=""image.png"" /></p>","They have written each component in standard form, as per math 135. 
<md>Just multiply by the conjugate of 1+i, which is 1-i.</md>"
"quiz format: <md>I'm wondering what is the quiz format? What kinds of questions will it contain (MCQ, T/F, SA, LA)?</md>","You can possibly expect True/False, short answer, full solution, short proofs.   We won&#39;t have multiple choice questions. "
Where is the &#34;AL&#34; building?: Hi. I am going to have my quiz somewhere called &#34;AL&#34;. What does it stand for?,"Arts Lecture hall next to dp
You can find campus buildings here: https://uwaterloo.ca/map/."
"Formatting of Vectors: <p>I&#39;m wondering if there is any convention for how much we should simplify answers for the quiz / assignments. What should we factor out as a scalar and what should be left within the vector notation? I tried this example on Mobius and it was marked correctly, however the answer provided was fully simplified. Is there a preferred way to represent this? Thanks!</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2Feb16edfbe789167454fbce3a27cd058f8a2f7573b4ba0b67120d99e89c1716ee%2Fimage.png"" alt=""image.png"" /></p>",Both answers will be accepted.
"WP1 Q7: I understand the forward proof where it ends up being the zero vectors. However, don&#39;t we have to state anything for the backward proof so that they are parallel? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwawbisf44qe%2F6f0a7fff19e4bece8b188a5077d48b68e7008e007b04efa62a55c5ca9c4f5476%2Fimage.png"" alt=""image.png"" />",I believe the rest of the solution is on the next page. This is not the end of the proof.
"Scalar to Vector: <p><a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweg63r8w7db%2F8ee3147179ba282f7cb9f01550dc340e4f2c1f5da04fe7e40a9e5eee588c30e2%2FScreen_Shot_2024-01-21_at_2.37.46_PM.png"" target=""_blank"" rel=""noopener noreferrer"">Screen_Shot_2024-01-21_at_2.37.46_PM.png</a></p>
<p>Here, I&#39;m not understanding why you make 2 variables to be 0? I also struggle to convert from scalar equation to vector equation. What are some guidelines?</p>","<p>If you mean in the step when we are trying to find points on the plane, taking $$x = y = 0$$ and solving for $$z$$ is a relatively simple way to get a point on the plane.</p>
<p></p>
<p>How are you attempting this question? I&#39;m not sure your $$\vec p$$ point is on the plane since<br /><br />$$-2(-2) - 4(-4) - (-1) = 21 \neq 0$$</p>"
Do we have reference sheet for quiz 1 ?: ,"No
That is correct. There will be no reference sheets for this course. "
"WP2 Q4: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwda3897x6e4%2F46ed2a9387859adc80e331a44b00b1398f95c4996d50c0660c76dc071cdfd6f8%2FScreenshot_2024-01-21_at_2.16.40_PM.png"" alt=""Screenshot_2024-01-21_at_2.16.40_PM.png"" width=""458"" height=""171"" /></p>
<p>I&#39;ve found a direction vector for my final equation by taking the norm of both planes and calculating the cross-product but I&#39;m confused how I would find a point on the intersection of the two planes. I&#39;m assuming the s and t in the first equation are not neccasarily the same as the s and t in the second. I&#39;ve tried taking the parametric equations and solving, but they don&#39;t really provide much insight because there is 4 variables. </p>
<p></p>","Good job using the cross product of the normals. You are also correct that s and t do not have to be the same values. You should try working with the scalar equation for the planes to find a point of intersection. The later we will have a more efficient method, and just using the scalar equations. "
"Area of a Parallelogram: <p>Given two vectors, v and w, the area of the parallelogram they create is apparently the length of the matrix v x w (the cross product between the two vectors).</p>
<p></p>
<p>I think I may have missed this fact from a lecture, could someone explain the proof behind this, and direct me to where in the textbook/lecture notes this was mentioned?</p>","<p>Most likely this wasn&#39;t proved in class, but you should think of it backwards. The cross product was defined as the vector that:</p>
<ul><li>Is perpendicular to u and v</li><li>Has a length equal to the area of the parallelogram create by u and v</li><li>Respects the right hand rule</li></ul>
<p>All three conditions are necessary to uniquely define the cross product. </p>
<p>Refer to the instructor&#39;s answer, but I also found this on pg. 29 of the course notes</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F2c05ba560842c9286e1fdf1072ff36ec3fde43ecbac6a60465973ac2f2062f56%2Fimage.png"" alt=""image.png"" /></p>"
"WP1 Q7: <p>Can anyone explain to me how can we get v1/u1 u1 =v1?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwe7qdk3e5qr%2F80a30c85f078c41f2515594b10e1f5c10e920ac16a870a0a5a1ac3d744ab60da%2F__2024-01-21_15.44.20.png"" width=""2254"" height=""738"" alt="""" /></p>","<p>$$v_{1} = v_{1}$$</p>
<p>$$v_{1} \ \cdot\ \frac{u_{1}}{u_{1}} = v_{1}$$</p>
<p>$$\frac{v_{1}}{u_{1}} \ \cdot \ u_{1}= v_{1}$$</p>
<p></p>
<p></p>
Just multiply the right-hand side and simplify. "
"WP1 Q5: I just wondering when can we prove the statement if and only if using  biconditional arrow?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwe7qdk3e5qr%2Fc6021bd73db3ebd39fd9a6d76dee2a9cb0566a8c48e0b0a758774481ebec59cf%2F__2024-01-21_15.54.05.png"" alt=""__2024-01-21_15.54.05.pngNaN"" />","Well, I whenever the arguments that explain each step are indeed if and only if. "
||u x v|| = ||u||||v||sin(theta): Can we use the identity stated above without proof?,Yes
hi class: ,"hi
Everything ok?"
"WP1 Q7: I was wondering if this proof is good enough. It is much much shorter compared to the posted solution, and I would like to know what is needed to make it more robust if necessary.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwan4htmc35k%2F3f7412ce182ed6f579dbc9f676b0a03fb6fd663af85eab220088130229503936%2FD048573A-8149-440C-9F4D-14ADF7712403.png"" alt=""D048573A-8149-440C-9F4D-14ADF7712403.pngNaN"" />","<p>Can you prove that</p>
<p></p>
<p>$$\vec u \times \vec v = c\vec v \times \vec v \implies \vec u = c \vec v$$ ?</p>
<p></p>
<p>What if there is some other vector which gives the same cross product with $$\vec v$$?</p>"
"Type of question on the quiz: What form of questions will appear on the quiz? Multiple choice? Short answer, long answer... etc.",See @83
collinear and Parallel: Does two vectors being Parallel imply they are collinear?,"<p>The term &#34;collinear&#34; usually requires at least 3 points. Collinear describes when a bunch of points fall on the same line. If you have two points (or vectors), then you can always draw a line that passes through both of them.</p>
<p></p>
<p>Perhaps you mean collinear with the origin (i.e. you can draw a line passing through the origin which also passes through both vectors). If this is the case, then yes, two parallel vectors are also collinear with the origin.</p>
Not necessarily as two parallel vectors can have a different starting point and thus are non collinear"
"MP1 Q1: <p>Hi, just wondering why this question is false? </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwegn0dxq7gb%2Ff2a66220822b8127e7179089294fe4bf9a8021b88ddfe205f46668019d47c86e%2F183231705893975_.pic.jpg"" alt=""183231705893975_.pic.jpgNaN"" /></p>",That&#39;s a mistake.  I&#39;ll report it. 
"WP1 Q4b: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd8exhah635%2Ffe1877e4912a9c618b090fc1ebce935ef0155b9286ba332d260eb33a3cf4e2c2%2FScreenshot_2024-01-21_at_10.40.05_PM.png"" alt=""Screenshot_2024-01-21_at_10.40.05_PM.png"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd8exhah635%2Fa0c01f178f72a9be35f1c8fa45768a806d8eb403e301c9c9bacf59bbabbd1edf%2FScreenshot_2024-01-21_at_10.40.20_PM.png"" alt=""Screenshot_2024-01-21_at_10.40.20_PM.pngNaN"" /></p>
<p></p>
<p>Is the answer suppose to be a typo? Isn&#39;t vector u suppose to be -1 1 2 instead of -1 1 3? I got my answer to be 3/14 [-3 -2 1] instead.</p>
<p></p>","<p>You are correct in that the $$\vec{u}$$ is a typo in the preliminary step, however the final answer should be correct still. First, make sure that you calculated the projection to be what the answer shows. Let me know if you got something different.</p>
<p></p>
<p>Then, doing component-wise subtraction, notice that $$-1 - \frac{6}{14} = \frac{-20}{14}$$, $$1 - \frac{9}{14} = \frac{5}{14}$$, and $$2 - \frac{3}{14} = \frac{25}{14}$$. Factor out $$\frac{5}{14}$$ from everything to get $$\frac{5}{14} \cdot\begin{bmatrix}    -4 \\ 1 \\ 5 \end{bmatrix}$$.<br /><br />If you&#39;re still stuck you could post your steps and I&#39;d be happy to help you out! </p>"
"WP1 Q8b: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd8exhah635%2F20142ef76449fc3b9741914a570eca1012932233ebe794d6b3e40ae21b2ce6c9%2FScreenshot_2024-01-22_at_10.30.05_AM.png"" alt=""Screenshot_2024-01-22_at_10.30.05_AM.pngNaN"" /></p>
<p>Can someone explain how you get the second last step? Why is the cross product reversed? I can&#39;t really see it in the third-last step, and don&#39;t we need the expanded cross-product to have negative components so that it can be reversed?</p>","<p>Do you mean</p>
<p></p>
<p>$$\quad (u_3 w_2 - u_2w_3) + (u_1w_3 - u_3w_1) + (u_2w_1 - u_1w_2)$$</p>
<p>$$= (w_2u_3 - w_3u_2) - (w_1u_3 - w_3u_1) + (w_1u_2 - w_2u_1)$$</p>
<p>$$= \vec w \times \vec u$$?</p>
<p></p>
<p>If not, could you please clarify your question?</p>"
"WP1 Q1c: <p>If 2 $$\vec{u}$$ = $$\begin{bmatrix} -8 \\ 15 \\-17 \end{bmatrix}$$ , then is </p>
<p>$$\vec{u}$$ = $$\begin{bmatrix} -4 \\ 15/2 \\-17/2 \end{bmatrix}$$</p>
<p>or should I consider scalar and vector as two different things</p>","<p>The answer to your first question is yes, $$\vec u = \begin{bmatrix} -4 \\ 15/2 \\ -17/2\end{bmatrix}$$.</p>
<p></p>
<p>I&#39;m not sure what you mean by &#34;should I consider scalar and vector as two different things&#34;. In a vector space such as $$\mathbb R^2$$, we have vectors which are elements of $$\mathbb R^2$$. A property of vector spaces is that it is closed under scalar multiplication, which means that if you take a scalar such as $$2$$ or $$5/2$$ or $$e$$ and multiply a vector such as $$\begin{bmatrix} 1 \\ 0\end{bmatrix}$$ by it, the resulting vectors are all also elements of $$\mathbb R^2$$:</p>
<p></p>
<p>$$2\begin{bmatrix}1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0\end{bmatrix} \in \mathbb R^2$$, $$\frac52\begin{bmatrix}1 \\ 0 \end{bmatrix} = \begin{bmatrix} 5/2 \\ 0\end{bmatrix} \in \mathbb R^2$$, $$e\begin{bmatrix}1 \\ 0 \end{bmatrix} = \begin{bmatrix} e \\ 0\end{bmatrix} \in \mathbb R^2$$, etc.</p>"
"Math Tutoring Center: <p>Hi,</p>
<p></p>
<p>I logged into the math tutoring center channel and can&#39;t find the MATH 136 office hours. Does anyone know where to find that?</p>
<p></p>
<p>Thanks!</p>","The office hours for each professor are listed on learn: <a href=""https://learn.uwaterloo.ca/d2l/le/content/982130/viewContent/5342327/View"" target=""_blank"" rel=""noopener noreferrer"">https://learn.uwaterloo.ca/d2l/le/content/982130/viewContent/5342327/View</a>."
"Norm: When it asks to find norm, should I only tell value of norm or value with vector. For example one of the Mobius question asked to find norm of [-1, 2] to which the answer was sqrt(5). But shouldn&#39;t we multiply this with the vector because the formula for norm is (1 / || v||) * v? So shouldn&#39;t the answer be srqt(5) * [-1, 2]?","<p>The norm describes the function with the two bars: $$\| \vec v\|$$ gives the norm of a vector $$\vec v$$. The norm is always real-valued (also known as a scalar) and nonnegative.</p>
<p></p>
<p>The formula you described $$\vec v / \|\vec v \|$$ gives you a unit vector $$\hat v$$ which points in the direction of $$\vec v$$. By definition, unit vectors have a norm of $$1$$, and this is easy to verify here:</p>
<p></p>
<p>$$\left\|\frac{\vec v}{\|\vec v\|}\right\| = \frac{1}{\|\vec v\|}\|\vec v\| = 1$$</p>"
Weekly practice problem &amp; Quiz &amp; Assignments: I am wondering that will the content for each weekly problem also be matched with the quiz or assignment due next week in the future right? (Like quiz1 - weekly practice problem 1),WA1 will cover weeks 1 and 2.  Quiz 2 will be on weeks 2 and 3.  WA2 will cover weeks 3 and 4.  And so on....  Each week has its own set of weekly practice problems. 
geometric to algebraic: Do we need to know how to convert between the two for the quiz?,What specific topic are you referring to that has both geometric and algebraic representations?
"prependicular vectors: Since there infinitely many perpendicular vectors to any given vector, is there any relationship between those vectors?","<p>We&#39;ll learn more about this topic in the coming weeks, but the short answer is that the most relevant relationships between these vectors are that:</p>
<p>1. they are closed under addition (i.e. if $$\vec v$$ and $$\vec w$$ are both orthogonal to $$\vec u$$, then so is $$\vec v + \vec w$$</p>
<p>2. they are closed under scalar multiplication (i.e. if $$\vec v$$ is orthogonal to $$\vec u$$, then so is $$c \vec v$$ for all $$c \in \mathbb R$$</p>
<p>3. $$\vec 0$$ is orthogonal to everything</p>
<p></p>
<p>These 3 key properties form the definition of subspaces (topic which will be covered in the coming weeks).</p>"
"right hand rule: <p style=""text-align:left"">Are we to remember the right-hand rule for the cross-product?</p>","<p>This is a good thing to know how to use.</p>
<p></p>
<p>If you forget it but remember the component formula for the cross product, then you can check which way it works by applying the formula to $$\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$$ and $$\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$$.</p>"
"Explicitly stating part of properties of cross/dot products: For proofs, do we state explicitly, for example, that this follows from the linearity of dot products or do we just state from the properties of the dot product","Some properties are true by definition (e.g. as you mentioned, linearity), while other properties are proven. You should mention if it&#39;s a property that we proved in class."
"clarification of mobius q11: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8btlx5d4as%2Fb7b98f9d46b6a3600101d47c4bdd932cb6b90c0582685827db15d3c98aeea018%2Fimage.png"" alt=""image.png"" /></p>
<p>What exactly is this question asking for? is it asking for a vector perpendicular to both vectors?</p>","<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2Fd03e11b30ddd79c8a8cd0c3c306712ab58710bbb4a0e8da8859e8b6cad77999a%2Fimage.png"" alt=""image.pngNaN"" />"
"Cross production properties: Can we state the fact without proof that $$\vec{v} \in \mathbb{F}^n, \vec{v} \times \vec{v} = \vec{0}$$?","The proof is straightforward so I’m sure you can.
Yes, but remember that the cross product is only defined for $$\mathbb R^3$$, not $$\mathbb F^n$$."
"WP1 Q7: Using || u x v || = ||u|| ||v|| sin(theta) instead?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdh2s3927me%2Ffc16f0aa34c9a65296ced969a10fea4c3e820ccbe03f7e5b8fed4a0fd0fb2c62%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>For this question, couldn&#39;t we use the fact that $$ ||u \times v|| =||u|| \cdot ||v|| sin \theta$$?</p>
<p></p>
<p>We would end up with $$sin\theta=0$$, which means that the angle between the two vectors is $$n\pi$$, with $$n \in$$ Real Numbers, so they&#39;re parallel?</p>","<p>I think you mean to write the angle is $$n\pi$$, where $$n \in \mathbb Z$$.</p>
<p></p>
<p>In particular, we would know that the angle would work out to either $$0$$ or $$\pi$$ radians, but this doesn&#39;t exactly align with the definition of parallel given. This approach would work, but you would still need to show that the angle between two vectors is $$0$$ or $$\pi$$ if and only if they are scalar multiples of each other (this is not hard to do, but don&#39;t forget to match your answer with the given definitions).</p>"
"Could someone clarify what they got for the true or false questions b and d: <p>b) is the one with d=0 and</p>
<p>d) is the one asking what the perp of the perp is equal to</p>",b) was false and d) was true. 
WA1 LaTex file: When will the Source and solution template files for WA1 be posted? Thanks.,I will add then later tonight. 
Latexing: Yo can I just use the math 135 latex template solution,Yes. The idea of having a template is just to help with the basic packages and commands that one may need for typing their solutions. 
"latex: Hi, i wonder how to type the order such as 1.1.1, 1.1.2 for theorems in overleaf","<md>Not sure if this is what you are referring to, but there exists a `theorem` (and corresponding `proof`) environment in latex. [See here](https://www.overleaf.com/learn/latex/Theorems_and_proofs)</md>"
Quiz Results: Hi I am just wondering if the quiz answers are typically going to be posted (similar to 138). Thanks!,"Yes, solutions will eventually be posted. "
"WA1 question2 (b): For question 2 (b), Do we have to show to progress by using matrix or etc, or can we just skip the progress show its a span. ",Just show the spans are the same. You can represent it as a matrix if you&#39;d like.
WA1 Q2: I am confused about how to show that two spans are subsets of each other. What do we need to have to prove it?,"See part a). 
maybe think of it in a way that the element of one span is also an element of the other span "
"WA1 Question 2 (a): From question. 2(a), Is, i equal to m? So is i the number of vector v?",$$i$$ is referring to each $$i$$ with $$1 \le i \le k$$.
"Span: If a vector is not a span of two direction vectors does, it means that it is orthogonal to two direction vectors? ","Not necessarily. Since the span of two direction vectors is a plane, then if a third vector is not in the span, it simply means it is not in the plane, it doesn&#39;t necessarily have to be orthogonal to the direction vectors. "
"WA1 Q3: <p>It&#39;s a bit desperate I know, but could I perhaps possibly get a small hint for Q3.</p>
<p></p>
<p>Thank you and have a splendid day &lt;3</p>","Hint: $$perp_{\vec{v}}(\vec{u})$$ is orthogonal to $$\vec{v}$$.
Hint: $$perp_{\vec{v}}(\vec{v})$$ is orthogonal to $$\vec{v}$$. "
"WP2 Answers?: Hello, I was wondering by when will solutions to Weekly Practice 2 be posted? ",They will be posted tomorrow morning. 
"Unique RREF and Rank: <p>Hello,</p>
<p>I&#39;m struggling with understanding unique RREF and rank, can you illustrate these concepts?</p>","<p>What do you find confusing? </p>
<p></p>
<p></p>"
"Rewriting Spans: <md>If I say $$\vec{x} \in Span\{\vec{v_1}, \vec{v_2}, ..., \vec{v_k}\}$$, is it valid to then continue and say ""then there exists $$c_1,c_2,...,c_k$$ such that $$\vec{x} = c_1\vec{v_1}, c_2\vec{v_2}, ..., c_k\vec{v_k}$$"". 

I know in lecture it was said to make the clear distinction between sets and vectors but would this still be valid?</md>","<p>Suppose your scalar field is $$\mathbb{R}$$. Recall the definition of span: $$\mathrm{Span}\{\vec{v_1}, \dots, \vec{v_k}\} = \{c_1\vec{v_1} + \cdots c_k\vec{v_k} : c_1, \dots, c_k \in \mathbb{R}\}$$. So, yes, if you take a vector $$\vec{x}$$ in the span, there exsit coefficients $$c_i$$ that express $$\vec{x}$$ as <strong><em>a linear combination in the form of  $$\vec{x}=c_1\vec{v_1} + \cdots + c_k\vec{v_k}$$</em></strong>. </p>
<p></p>
<p></p>
<p>Recall the definition of span: $$\mathrm{Span}\{vec{v_1}, \dots, \vec{v_k}\} = \{c_1\vec{v_1} + \cdots c_k\vec{v_k} : c_1, \dots, c_k \in \mathbb{R}\}$$. So, yes, if you take a vector $$\vec{x}$$ in the span, there exsit coefficients $$c_i$$ that express $$\vec{x}$$ as <strong><em>a linear combination in the form of  $$\vec{x}=c_1\vec{v_1} + \cdots + c_k\vec{v_k}$$</em></strong>. </p>
<p></p>
<p></p>
<p>Recall the definition of span: $$\mathrm{Span}\{vec{v_1}, \dots, \vec{v_k}\} = \{c_1\vec{v_1} + \cdots c_k\vec{v_k} : c_1, \dots, c_k \in \mathbb{R}\}$$. So, yes, if you take a vector $$\vec{x}$$ in the span, there exsit coefficients $$c_i$$ that express $$\vec{x}$$ as <strong><em>a linear combination in the form of  $$\vec{x}=c_1\vec{v_1} + \cdots c_k\vec{v_k}$$</em></strong>. </p>
<p></p>
<p></p>"
"Latex Submission: For the latex file, I will be submitting the <strong>.tex</strong> file or the <strong>compiled PDF</strong> file for what I have written in Overleaf.<br /><br />Can anyone help?","<p>Submit the compiled PDF to crowdmark.</p>
<p></p>
<p>On the right side of Overleaf, you should see a download arrow to download the pdf and you can upload it.</p>
<p></p>
<p>If you are compiling your latex locally, the pdf file will appear in the same folder as you compile.</p>"
"WA1 question3 (a): Do we have prove the fact that u and z are orthogonal, or o we have to use that to prove span{u,v}=span{u,z}","<p>You need to show both: that you can choose $$\vec z \in \mathbb R^n$$ so that</p>
<p></p>
<p>a) $$\vec u \cdot \vec z = 0$$, AND</p>
<p>b) $$\text{Span}\{\vec u, \vec v\} = \text{Span}\{\vec u, \vec z\}$$</p>"
Latex question: I wonder how to align the equal symbols for math equation ,"<p>You can use </p>
<p></p>
<pre>
\begin{align*}</pre>
<p>and use <kbd>&amp;</kbd> signs to align your equations.<br /><br />Make sure to end your <kbd>align*</kbd> environment with an</p>
<p></p>
<pre>
\end{align*}</pre>
<p>You can use </p>
<p></p>
<pre>
\begin{align*}</pre>
<p></p>"
"WA1 Question 2 (a): Is i the corresponding number of u or the number of span? For example when i=2 then does it mean it is span{u1, u2} or is it just u2","The second statement is asking you to show that for all $$1 \le i \le k$$, $$\vec u_i \in \text{Span}\{\vec v_1, \ldots, \vec v_m\}$$."
"WA1 Q3 b): <p>When I declare $$\vec{u}$$ and $$\vec{v}$$, can I put a constraint on them? for example: each component for both vectors have to be 1. Or do I have treat them as for all statements, and cannot have any constrains?</p>
<p><br />Thanks!</p>","<p>Orthogonal just means their dot products are $$0$$, so you cannot impose additional constraints on them.</p>
<p></p>"
"WA1 Question 2 (a): <p>If it is a subset, we need to show an example that makes it not equal, right?</p>
<p></p>","<p>I&#39;m not sure what you mean.</p>
<p></p>
<p>If you are trying to show the contrapositive for one direction, and you wish to demonstrate $$A \subsetneq B$$, then it would suffice to find $$a \in A$$ so that $$a \notin B$$, if that is what you are asking.</p>
I think you&#39;re thinking of a strict subset."
"Referencing other Q&#39;s Proofs: <p>Hello!</p>
<p></p>
<p>Just to confirm - if we have proved something in the 1st question of a WA (for example), can we reference that result within the 2nd question of the same WA to prove another result?</p>
<p></p>
<p>Thanks!</p>","Yes, that is fine."
"Vectors in Rn: Can any vector in Rn be rewritten as a linear combination of two orthogonal vectors? That is that suppose u and v are orthogonal, so Span(u,v) = Rn","<p>No. Consider:</p>
<p></p>
<p>$$\vec u = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$$ and $$\vec v = \begin{bmatrix} 0 \\ 1 \\ 0\end{bmatrix}$$.</p>
<p></p>
<p>I&#39;ll leave it to you to check that they are orthogonal and that $$\text{Span}\{\vec u, \vec v\} \neq \mathbb R^n$$.</p>"
"Solving REF / RREF: Hi I just wanted to ask when dealing with REF and RREF, is it ok to do manipulations such as R1 -&gt; 0.5*R1 - R2, will marks be deducted since this step includes multiple EROs?",I would recommend that you do not do combine steps like this because it will affect things that we do later on when working with determinants. 
"perp and projection: Hi, I just wanted to ask if we need to prove that perpendicular and projection vectors are orthogonal when we are doing the written assignment, or can we just use it directly?","<p>You need not prove it.</p>
<p></p>
<p>You can find a proof of it as proposition 1.6.9 in the course notes.</p>"
"WA1 question3 (b): Can we just say vector w is a span{u,v,z}?, and how can we use the fact u and v are orthogonal?","<p>I&#39;m not sure what you mean.</p>
<p></p>
<p>$$\text{Span}\{\vec u, \vec v, \vec z\}$$ is a set of vectors, whereas $$\vec w$$ is a vector, so you can&#39;t say </p>
<p></p>
<p>$$\vec w = \text{Span}\{\vec u, \vec v, \vec z\}$$.</p>
<p></p>
<p>As a hint, if you solved (a), how can you use (a) to solve (b)?</p>"
REF and RREF: Hi. Is it the fact that for any given matrix the REF for which is not unique and we are allowed to get any REF matrix that satisfies the REF condition? But the RREF is unique?,"The REF for a given matrix is not unique but the RREF is unique.  Therefore, if you are asked for an REF of a matrix there will be an infinite number of correct solutions.  You can give any REF which satisfies the REF conditions. "
"WA1 Q4: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwemr3flutc%2F4fe8ae25d54eb4bed1033896e1e7e6ffa9c07716d6a474f315a4b9d600a0c241%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>Just to clarify,  what does vector a not being on the plane mean algebraically. I can visualize it but do not understand how to use it</p>","<p>We can represent the plane as $$\mathcal P = \{\vec b + r \vec v + s \vec w : r, s \in \mathbb R\}$$, so if $$\vec a \notin \mathcal P$$, this means that there does not exist $$r, s \in \mathbb R$$ so that</p>
<p></p>
<p>$$\vec a = \vec b + r \vec v + s \vec w$$, or alternatively, for all $$r, s \in \mathbb R$$,</p>
<p></p>
<p>$$\vec a \neq \vec b + r \vec v + s \vec w$$.</p>"
"WA1 Q3: For Q3 b), I&#39;m confused as to how the statement is true in R2 (since it&#39;s supposed to be true for Rn). If u and v are already orthogonal in R2, how can there be a third vector that is orthogonal to both of them?",What about $$\vec 0$$?
WA1 Q1: What are the rules for proving an if and only if if I am trying to prove both ways at the same time. As long as the math makes sense backwards that is allowed right?,Yes.<div><br /></div><div>An if and only if means that both directions are true.</div><div><br /></div><div>e.g. A iff B means A implies B and B implies A</div>
"LATEX: <p>Hi,</p>
<p>Since I took a screenshot of my handwriting onto ChatGPT to convert my solution into LATEX code, how do I acknowledge that I&#39;m using it?</p>","You could simply say that you used ChatGPT to create your LaTeX code.  However, the whole point of the bonus on the assignments is that you learn to typeset your own work.  It is a very useful skill. "
"WP2 Q3 (Recommended Problems): <p>This was the solution to Q3:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2F7848dfe9fd8fe752cde8474339dd5593b69db55f59f1814331ecdb3f6bcd8da2%2Fimage.png"" alt=""image.pngNaN"" width=""428"" height=""241"" /></p>
<p>The only step I don&#39;t understand is this one:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2Fce15bb098ae5a79ed76643bed5837e3124ef93ef1d5a2ac56a111667dadf3c07%2Fimage.png"" alt=""image.png"" width=""323"" height=""31"" /></p>
<p>How do we know that?</p>
<p></p>
<p></p>",I believe that is the assumption (if you reread the question again)!
"Weekly practice2 q6 warm up: In class I think the instructor said elementary row operation<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2Fcf2886b2985b59301b0209ffb2f314248ac980c96dcfff35badf82840d5cd794%2FScreenshot_2024-01-27_at_11.55.07.png"" width=""2094"" height=""768"" alt="""" /> only has addition and multiplication (no subtraction and division), while in the solution for q6 I see that it has e2-2e1. So can we include subtraction/division in elementary row operation?","Yes. Since $$\mathbb{R}$$ is a field, adding a negative number works the same as subtracting a positive number. Same philosophy works for division.
Yes. Since $$\mathbb{R}$$ is a field, adding a negative number is same as subtracting a positive number. Same philosophy works for division."
"Take home practifce problem: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2F68b3d478b4764260ecbf8a35615c43e8538abf69d988c2b7e70f821c07fad5f9%2Fimage.png"" alt=""image.png"" /></p>
<p>I&#39;ve solved it down to</p>
<p>$$\left[\begin{array}{@{}cccc|c@{}}
1 & 2 & -3 & 0 &4 \\
0 & 1 & 0 & 1 & 3\\
0 & 0 & 0 & 0 & 0\end{array}\right]$$</p>
<p>where a particular soln is</p>
<p>$$\begin{bmatrix}
4 \\ 0 \\ 0 \\ 3
\end{bmatrix}$$, how would I find the full set of solutions?</p>","parametrize the free variable $$x_2$$ and $$x_3$$, express the pivots as a linear combination of the free variables e.g.$$x_1=-2x_2 + 3x_3$$. Edit: Get the matrix to RREF first
parametrize the free variable $$x_3$$ and $$x_4$$, express the pivots as a linear combination of the free variables. Edit: Get the matrix to RREF first
parametrize the free variable $$x_2$$ and $$x_3$$, express the pivots as a linear combination of the free variables, e.g. $$x_1=4-2x_2 + 3x_3$$
parametrize the free variable $$x_2$$ and $$x_3$$, express the pivots as a linear combination of the free variables, e.g. $$x_1=-2x_2 + 3x_3$$"
"Week 3 Practice Problem Q5 (g): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa8mrv3il6%2Fa3d39c1886351649e586c33652da3208025c66b3a04cacbe3a0a65e3a22f73ab%2Fimage.png"" alt=""image.png"" /></p>
<p>When using variables m and n, does this statement mean&#xff1a; “for all m and n, there exists...&#34; or does it mean &#34;there exists... for some m and n...&#34;?</p>","When we state a theorem like this, we mean that it is true for all $$m$$ and $$n$$.<div><br /></div><div>If the validity of the statement depends on $$m$$ or $$n$$, then we would say it is false.</div>"
"WA1 Q3 a): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehgz4527lv%2F76abcd42292196e00dec12057a483224c46cc1b9385955573203ab40004af58e%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>How is this ever true if the vector v is parallel to the vector u in R^2?</p>",Consider the zero vector.
WA1 Q2a: Can someone please explain what exactly this question is asking? It seems like there&#39;s almost nothing to solve if we just use the definition of spans and linear combinations.,"The questions are not meant to be extremely difficult or long, if that’s what you are asking.<div><br /></div><div>I think the question statement is quite clear, but if you have a specific clarification, please elaborate.</div><div><br /></div><div><br /></div>"
"Span of the next test?: For the upcoming test next Monday, what is the coverage?","You can see the expected coverage for all assessments on Learn under the practice problem schedule: <a href=""https://learn.uwaterloo.ca/d2l/le/content/982130/viewContent/5306181/View"">https://learn.uwaterloo.ca/d2l/le/content/982130/viewContent/5306181/View</a>.<div><br /></div><div>Quiz 2 will cover 3.3-3.8.</div>"
"WP Q4: Can anyone explain what happened here with the u vectors? <img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwatfhhwv48n%2Ffnlkhmmnvnac%2Fpublic_2024_01_27.jpeg"" /><p></p>","<p>If you call $$\vec{x}=
\begin{bmatrix}
1\\ -\frac{3}{5}\\0
\end{bmatrix}$$ and $$\vec{y}=
\begin{bmatrix}
1\\-\frac{1}{5}\\1
\end{bmatrix}$$, then</p>
<p></p>
<p>$$\{\vec{x} + t\vec{y}:t\in \mathbb{R}\}= \{\vec{x}+2\vec{y} + (t-2)\vec{y}: t \in\mathbb{R}\} = \{\vec{x} + 2\vec{y} + \frac{t-2}{5}(5\vec{y}): \in \mathbb{R}\}$$, where you do a variable change for $$t\in \mathbb{R}$$.</p>"
"About solving Matrix: When solving a matrix, does it matter which row we should subtract or add? Or can we just subtract and add any row while making rref.","RREF is unique, so it does not matter which way you row reduce."
"WA1, q2, part b: How to prove whether 1 span is subset of other, If I succesfully show that every vector of span 1 is present in span 2, will that work??(my friend gave me this hint) and if this works why does it work??","I’m think it has to do with the definition of a span and your ability to switch up scalar coefficients.
Yes, it follows from the fact that spans are sets and how we show that two sets are subsets of each other. "
"Free Variable: Does a free variable come from a 00..00|0 or can it be 00..00|x where x is any non-zero number? are we only allowed to assign that row to be free if all the values in the row are 0? Also, if it does have the form 00..00|x where x is non-zero, are there no solutions at all?","Remember that in an augmented matrix representation of a system of linear equations, each row corresponds to an equation, so if you have a row of the form $$\begin{bmatrix} 0 & 0 & \cdot & 0 & \mid & x\end{bmatrix}$$ where $$x$$ is nonzero, then it is clear to see that there is no solution."
WA1 Q4: How long is the proof supposed to be? The question seemed self-explanatory and there&#39;s really nothing to prove. ,"Although the answer to the proof is quite intuitive, an instructor told me that your proof must still require some algebraic work given the equations of the line and the equation of the plane. Consider proving it by contradiction, what can you say about the line and the plane when they intersect at a point.
You can&#39;t just explain the intuition here.  You need to give a rigorous proof using the algebraic definitions of lines and planes. "
"Overleaf Download: I finished my assignment on Overleaf, and I wish to submit the file to Crowdmark. However, Overleaf just gives me the entire document rather than one for each question. Do I just submit the whole thing on every question on crowd mark?","When you upload the file on Crowdmark it’ll split every page up. Then Crowdmark allows you to drag and drop each page to the question it represents. So upload it and drag the pages.<div><br /></div><div>@163</div>
When you upload the file on Crowdmark it’ll split every page up. Then Crowdmark allows you to drag and drop each page to the question it represents. So upload it and drag the pages.
See @163, too."
"Conceptual Question - 3.10 Textbook: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2F92e8f2033ab9b13ce295e6ffdbd731c1abb086fdab2e0b2c14d1bb2eeed22f40%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>I don&#39;t understand how they went from $$c_1A\vec{x}_1$$ to $$c_1\vec{e}_1$$. What exactly is $$\vec{x}_1$$?</p>","<p>By hypothesis, we assumed that the systems $$A\vec x = \vec e_i$$ is consistent for each $$1 \le i \le m$$. This means that for each $$1 \le i \le m$$, there is $$\vec x_i \in \mathbb R^n$$ so that</p>
<p></p>
<p>$$A \vec x_i = \vec e_i$$.</p>"
"Leading Coefficient: I know that pivot can be leading one, can it also be leading coefficient?","Could you clarify your question? What is it a leading coefficient of?
Leading coefficient is any other leading coefficient in the row other than 1."
"Showing an element of the spanning set is in a span: Can we state that $$\vec{x} \in \operatorname{Span} \{ \vec{x}, \vec{y}, \vec{z}\}$$ without explicitly writing the linear combination that shows that it is?","Yes, this is clear enough to see."
"WA1 Question 3 (a): I’m really unsure on how to even start the proof for this question even with the hint, can someone maybe help me just understand how to go about it?","<p>Try drawing out a diagram.</p>
<p></p>
<p>Given two arbitrary vectors $$\vec u$$, $$\vec v \in \mathbb R^n$$, what would it look like to have a vector $$\vec z$$ which is orthogonal to $$\vec u$$? What would it look like if that vector $$\vec z$$ had to also maintain the same span (i.e. $$\text{Span}\{\vec u, \vec v\} = \text{Span}\{\vec u, \vec z\}$$)?</p>"
"WA1 Q3(a): Can anyone help me to start the proof of Q3(a)?<div>I am thinking about using proof in Q2, like if I can express v in u, z and z in u, v, then two span will equal to each other<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwe793kim5nl%2Fkwsakhdlgsmv%2Fpublic_2024_01_28.jpg"" /><p></p></div>","Remember that when you project a vector $$\vec v$$ onto $$\vec u$$, it has direction $$\vec u$$. Knowing this, what is the directional vector of $$\text{perp}_{\vec u}(\vec v)$$?"
"WA1: Q2B: If we want to show that a vector can be represented as a linear combination of other vectors, do we have to show how we got the values for the constants (or is it sufficient to just provide them)? What if the method of obtaining them can kind of just be done by looking at it?","Do you mean Q2b? If so, then yes, for this question, you don&#39;t need to show work in how you solved a system of linear equations (although in general, you will need to show work if the question asks you to solve a system of linear equations)."
WA1 Q3b: Can I directly use part a) to prove part b)?,Yes
"WA1 Q3b: Hi,<div>For q3 b), do we have to explain how we find z?</div><div>Thank you!<br /><div><br /></div><div><br /></div></div>","No. 
No, but whatever z you find, you have to prove it&#39;s orthogonal to both u and v."
"Quiz 1 Q2: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2Fa66c85aa8594853407fadd458a7158f6add00af55c85fcb9c1dd3d9dc5c5ed6a%2Fimage.png"" alt=""image.png"" /></p>
<p>What is the correct answer of a?</p>",It should be $$-i$$ in the first step. 
"q1 d): Hi, can anyone explain to me why this is true?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwe7qdk3e5qr%2Ff57ccbbe52529893080bd68a69616d3bf2cb7822bd569fb65f31e61262f941a9%2F__2024-01-29_10.51.58.png"" alt=""__2024-01-29_10.51.58.pngNaN"" />",proj(v)(perp(v)(u)) = 0(vector)<div>(Expand it to get the result)<br /><div><div>So double perp(v)(u) = perp(v)(u) - 0 = perp(v)(u)</div></div></div>
"Wk4 Practice Not Found in LEARN: <p>Hello,</p>
<p>When I tried to search for practice problems on LEARN, I could not find the pdf. I was wondering when the practice problem 4 will be posted.</p>",I just posted it. 
"LaTeX alternative: Hello, I read the note at the bottom of the attached image, and I just want to make sure I will still get the bonus if I would like to typeset using unicode in MS word rather than LaTeX. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwejjv1bq8t%2Fa25f55b1d417a0c59c01d98cb7768fd627d95f8becd4f62aed98c4184e5b3f4f%2Fimage.png"" alt=""image.pngNaN"" />","<p>If you are planning to use the insert equation/symbol feature in MS word, I believe MS word may only let you input typsetted characters and a few basic mathematical symbols such as sums (Capital sigma), products (Capital pi), etc.. </p>
<p></p>
<p>In general, any software that is based on TeX such as Latex or even KaTeX would be much preferred and far easier to implement (I can confidently LaTeX notes faster than I can handwrite after having used it since my first year (2 years ago)).</p>
<p></p>
<p>We provide you the .tex file for the questions and you need only type up your solutions, which is very akin to just type it in MS word or alike, except you put $ signs around your math symbols. Overleaf is a free online software that lets you LaTeX.</p>
<p></p>
<p>-----</p>
<p></p>
<p>As long as the output is clearly understood and easy to read, then you will be awarded the LaTeX bonus.</p>
<p>If you are planning to use the insert equation/symbol feature in MS word, I believe MS word may only let you input typsetted characters and a few basic mathematical symbols such as sums (Capital sigma), products (Capital pi), etc.. </p>
<p></p>
<p>In general, any software that is based on TeX such as Latex or even KaTeX would be much preferred and far easier to implement (I can confidently LaTeX notes faster than I can handwrite after having used it since my first year (2 years ago)).</p>
<p></p>
<p>We provide you the .tex file for the questions and you need only type up your solutions, which is very akin to just type it in MS word or alike, except you put $ signs around your math symbols. Overleaf is a free online software that lets you LaTeX.</p>
With respect to the instructor&#39;s answer, from personal experience, MS Word can satisfy all your LaTeX needs! Once exported to a PDF it is indistinguishable from a document that was typed using LaTeX. (I am sure there is metadata which can show how the document was created). However, from a functional standpoint, MS Word is equipped with the tools to typeset these assignment questions.
With respect to the instructor&#39;s answer, from personal experience, I can tell you MS Word can satisfy all your LaTeX needs! Once exported to a PDF it is indistinguishable from a document that was typed using LaTeX. (I am sure there is metadata which can show how the document was created). However, from a functional standpoint, MS Word is equipped with the tools to typeset these assignment questions."
Parallel-ness of proj and vectors: Can we state without proof that a vector u is parallel to the projection of another vector onto u? e.g.<br /><br />\Vec{u} is parallel to proj_{\Vec{u}}(\Vec{w}) for some w,"<p>Yes.</p>
<p></p>"
"Haven&#39;t received the crowdmark mailer: <p>Hello!</p>
<p>I haven&#39;t received the crowdmark mailer for assignment 1. How can I do to solve it?</p>
<p>Thank you!</p>",I just did a new sync. Please let me know if you can see the assignment on Crowdmark now.
"Question about RREF: When asking about whether an augmented matrix is in RREF, only coefficient matrix has to be in RREF and don’t have to consider b right?<div><br /></div>","<p>If you are looking for the RREF of $$[A\mid \vec{b}]$$, then you need to consider $$\vec{b}$$. Matrix like</p>
<p>$$ \left[ \begin{array}{cc|c} 1& 1 & 2\\ 0 & 0 & 1 \end{array} \right] $$</p>
<p>isn&#39;t in RREF, though it suffices to tell the solutions (no solution).</p>
<p></p>
<p>If you are looking for the RREF of $$[A\mid \vec{b}]$$, then you need to consider $$\vec{b}$$. Matrix like</p>
<p>$$
\left[
\begin{array}{cc|c}
1& 1 & 2\\
0 & 0 & 1
\end{array}
\right]
$$</p>
<p>isn&#39;t in RREF, though it suffices to solve the equations.</p>
<p></p>"
"WA1 Q2B: Using a system of equations: I was wondering if it was valid to include scalar coefficients of one span in the equation for scalar coefficients of the other span using a system of equations? <br /><br />For example; let &#34;a&#34; be the scalar coefficient of Span 1, and let &#34;b&#34; be the scalar coefficient of Span 2. As an example, can I say that a=2b&#43;3.<br /><br />I am using this to show that when we let the scalar &#34;a&#34; be a certain function of the scalar &#34;b&#34;, then every element in Span 2 is an element of Span 1.<br /><br />Sorry if the wording is confusing, I&#39;m not quite sure how to articulate my thoughts properly.",I&#39;m not sure what you are asking.<br /><br />Can you give a more specific example?
Coverage for quiz2: What would be the coverage of quiz 2?,"<p><span style=""text-decoration:line-through"">The quiz will cover all covered content up until 3.8.</span><br /><br />----</p>
<p></p>
<p><span style=""text-decoration:line-through"">Edit: the quiz will cover up until 3.6 not 3.8.</span></p>
<p></p>
<p>Edit: The announcement was wrong.  It now says the correct coverage which is up to Section 3.10.  Sorry for all of the confusion.  </p>
<p><span style=""text-decoration:line-through"">The quiz will cover all covered content up until 3.8.</span><br /><br />----</p>
<p></p>
<p><span style=""text-decoration:line-through"">Edit: the quiz will cover up until 3.6 not 3.8.</span></p>
<p></p>
<p>Edit: The announcement was wrong.  It now says the correct coverage which is up to Section 3.10</p>
<p>The quiz will cover all covered content up until 3.8.<br /><br />----</p>
<p></p>
<p>Edit: the quiz will cover up until 3.6 not 3.8.</p>
<p></p>
<p>Edit: The announcement was wrong.  It now says the correct coverage which is up to Section 3.10</p>
<p>The quiz will cover all covered content up until 3.8.<br /><br />----</p>
<p></p>
<p>Edit: the quiz will cover up until 3.6 not 3.8.</p>
<p></p>
<p>Edit:  I think the announcement was wrong.  It now says 3.10 but that might change Tuesday morning. </p>
<p>The quiz will cover all covered content up until 3.8.<br /><br />----</p>
<p></p>
<p>Edit: the quiz will cover up until 3.6 not 3.8.</p>
The quiz will cover all covered content up until 3.8."
"Hi, when are the practice problems released?: When are the mobius practice problems and written practice problems made available? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcdk94xt77a%2Fbadf4d32278f01134b414629beccff1aa25679d88deb9a8ca2647110e4a38c59%2FScreenshot_2024-01-29_at_7.06.25_PM.png"" alt=""Screenshot_2024-01-29_at_7.06.25_PM.pngNaN"" width=""404"" height=""281"" />",Both are available as of now.
"Are quizzes cumulative?: Will each quiz cover all content <em>up until</em> a certain chapter, or only that specific chapter?","The focus of each quiz will be to cover content that has not yet been covered by any quiz or written assignment, but it is inevitable that some content from previous weeks is required prerequisite knowledge for the upcoming quiz.<br /><br />For instance, the usage of vectors (week 1) will be seen throughout the rest of the course."
Midterm coverage: What is the coverage of chapters and sections for the midterm?,The midterm will cover up until 4.4.
"wp2 q7: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Feb590d7e7dd0784bdb43a5df80b19f35de7e5f62f764c33ee3f0bf5bdaa597b6%2FScreenshot_2024-01-31_at_20.35.09_2.jpeg"" alt=""Screenshot_2024-01-31_at_20.35.09_2.jpegNaN"" />I find this problem hard to calculate. Can anyone give me any suggestions to make the calculation easier? I just use back substitution. ",Can you post your calculation? It is subjective to say how easier is easier.
"3.8 covered?: I know that the quiz covers up to section 3.10, but considering we didn&#39;t go over 3.8 in class I was wondering if it would be assessed. Thanks.","Yes, 3.8 is covered. "
"wp2 q5 (recommended): <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fc5889789a41465b1da1d0e3cb70f5e9092402ca37b2b96f5be7cbae38b2a0277%2F__2024-02-01_09.29.35.png"" alt=""__2024-02-01_09.29.35.pngNaN"" />I don&#39;t understand why we have to prove any vector orthogonal to u must be a scalar multiple of the vector [−u2 u1] T, such that v and perp u (x) must be scalar multiples of each other. Can&#39;t we directly say two vectors with the same direction are scalar multiples with each other because it is obvious that v an perp u (x) are on the same line?","<p>Given that $$\vec{v}$$ and $$\mathrm{perp}_{\vec{u}}(\vec{x})$$ are orthogonal to $$\vec{u}$$, these two vectors may <em>not </em>be on the same line. It would be true if</p>
<ul><li>$$\vec{u}$$ is nonzero, and</li><li>the problem is in $$\mathbb{R}^2$$ (or $$\mathbb{F}^2$$).</li></ul>
<p>As you can see in the proof, it uses the conditions $$\vec{u}\neq \vec{0}$$ and $$\mathbb{R}^2$$ to work out the components explicitily.</p>
<p>Given that $$\vec{v}$$ and $$\mathrm{perp}_{\vec{u}}(\vec{x})$$ are orthogonal to $$\vec{u}$$, these two vectors may <em>not </em>be on the same line. It would be true if</p>
<ul><li>$$\vec{u}$$ is nonzero, and</li><li>the problem is in $$\mathbb{R}^2$$ (or $$\mathbb{F}^2$$).</li></ul>
<p>As you can see in the proof, it uses the fact of $$\vec{u}\neq \vec{0}$$ and $$\mathbb{R}^2$$ to work out the components explicitily.</p>"
"wp3 q1(recommended): <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2F4624f02a2fb79e14f0691dd3186be106fd9d6e9197c77b6ddb90200452d8b5d3%2F6801706816668_.pic.jpg"" alt=""6801706816668_.pic.jpgNaN"" /> I wonder if we can use vector equations to solve this problem. I&#39;m stuck and don&#39;t know what should be the next step.","<p>The way you have written things, you are assuming that the planes are parallel since they have the same direction vectors.  That is not necessarily true.</p>
<p></p>
<p>This approach would have to involve a lot of case work as you consider different options for the vectors involved.   I think would you get there eventually but it wouldn&#39;t be very pretty.  </p>
<p>The way you have written things, you are assuming that the planes are parallel since they have the same direction vectors.  That is not necessarily true.</p>
<p></p>
<p>This approach would have to involve a lot of case work as you consider different options for the vectors involved. </p>"
"Format for a Transposed Vector: <p>Hi,</p>
<p></p>
<p>I have a quick question. When I&#39;m writing a vector in its transposed form, do I add commas between each values? </p>
<p></p>
<p>For example, is it valid to write $$[5, 6, 7]^T$$?</p>
<p></p>
<p>Thank you for your time</p>","No, they should be delimited by spaces (i.e. $$\begin{bmatrix} 5 & 6 & 7 \end{bmatrix}^T$$) but sometimes this may be a bit hard to see:<br /><br />$$\begin{bmatrix} 1 & 2 + i & 34 - 12\end{bmatrix}^T$$<br /><br />so if it may be ambiguous, it is better to write it as a column vector.<br /><br />Note for LaTeX: in the \begin{bmatrix} environment, you can use &amp; characters to make spaces.
No, they should be delimited by spaces (i.e. $$\begin{bmatrix} 5 & 6 & 7 \end{bmatrix}^T$$) but sometimes this may be a bit hard to see:<br /><br />$$\begin{bmatrix} 1 & 2 + i & 34 - 12\end{bmatrix}^T$$<br /><br />so if it may be ambiguous, it is better to write it as a column vector."
Chapter coverage: Does quiz 2 include chapter 2.1-2.5 and 3.1-3.10 or is there something in those chapters that is not included,All of those sections is included. 
"WP Warm Up Q1: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwei8tg2jp%2F3825cb4e7b201b3069f77d74f6250ac6c786e7cfa65b30752a49cae9a2d4e25d%2F__2024-02-01___4.47.33.png"" alt=""__2024-02-01___4.47.33.pngNaN"" width=""662"" height=""124"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwei8tg2jp%2Fbfd5e4ad66d6b83a9e83f1e32910e393ea0e6d75d87b695a2bf2294cd2923e29%2F__2024-02-01___4.47.54.png"" alt=""__2024-02-01___4.47.54.pngNaN"" width=""709"" height=""105"" /></p>
<p>I got stuck to get it in RREF, I got $$\begin{bmatrix} i & 0 & 1 \\ 0& 1& 1\\ 1& 0 & 0 \end{bmatrix}$$ for coefficient part and $$\begin{bmatrix} -i\\ -1-i \\ 1+i \end{bmatrix}$$ for augmented part</p>","Hint: from where you got to, try swapping rows 1 and 3."
"Is 3.7 on the quiz, since we only covered it this week?: ^^^",The quiz covers up to 3.10. 
"Simplifying solution set: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2F810675706210144404227127da324f099391e4becbfee99c4f25517b9ed6239f%2Fimage.png"" alt=""image.png"" /></p>
<p>How did they get s = (t-2)/3? And do we have to simplify it or can we leave it as fractions?</p>",You don’t need to simplify your solution past the first step.
"Weekly practice  recommend problem  Q5)d: From  recommend problem  Q5)d, When we change the Matrix to RREF Doesn&#39;t it have the row of zeros?, because if we trying to find the rank we need to change the matrix to RREF.  So using rank(A)&lt;n isn&#39;t there a row of zeros?","You are right. Whether or not $$A$$ has a row of zeroes has almost nothing to do with the rank of $$A$$, whereas determining the rank of $$A$$ has to do with finding how many rows of zeroes are in the RREF of $$A$$."
"about solving a system: When solving a system matrix, do I have to write just the unique solution or the solution in a line? So example, do I have to solve the matrix in a null space and add the particular solution of ax=b? In the exam, are the details provided in the question about how to solve the system equation?","<p>I&#39;m not sure what you mean. If the solution is unique, then you can write the solution as a singleton (e.g. $$\{\vec x\}$$, where $$\vec x$$ satisfies $$A\vec x = \vec b$$).</p>
<p></p>
<p>In general, questions will be very specific about how we want you to represent your solution, and you can always ask for clarification if needed.</p>"
"WP3 Q1b - Moving from REF to RREF: <p>Hi,</p>
<p></p>
<p>For 1b, I am slightly unsure on how the solution had 1, 2, 0, -1 | 2 in the first row. </p>
<p></p>
<p>My other rows are correct, according to the solution. So, I am confused on where I went wrong. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdnjctl125%2F7559b00f23a1a480ce8e850927ea111486db574d7ecc92c3ebb52330338c620d%2F20240202_121658.jpg"" width=""599"" height=""799"" alt="""" /></p>
<p></p>
<p>I appreciate the help, </p>
<p></p>
<p>Thank you</p>","<p>Remember that in RREF, the only nonzero entry in a pivot column can be the pivot itself.</p>
<p></p>
<p>Here, your second row, third column is a pivot, so the entry above it needs to be 0.</p>
<p></p>
<p>You can perform the operation $$R_1 \to R_1 - R_2$$ to zero it out.</p>
<p>Remember that in RREF, the only nonzero entry in a pivot column can be the pivot itself.</p>
<p></p>
<p>Here, your second row, third column is a pivot, so the entry about it needs to be 0.</p>
<p></p>
<p>You can perform the operation $$R_1 \to R_1 - R_2$$ to zero it out.</p>
<p>Remember the in RREF, the only nonzero entry in a pivot column can be the pivot itself.</p>
<p></p>
<p>Here, your second row, third column is a pivot, so the entry about it needs to be 0.</p>
<p></p>
<p>You can perform the operation $$R_1 \to R_1 - R_2$$ to zero it out.</p>"
"Week 4 practice problems Q4: How to apply the hint?: <p>Hi, </p>
<p>I am working on Q4 of practice problems week 4. When proving that if $$\vec{x} \in Null(A^TA)$$, then $$\vec{x} \in Null(A)$$, I am not sure how to proceed with the hint. I have attached my work below. I noticed in the solutions that they make reference to the dot product, but I am not sure why that works and how matrix multiplication can be associated with dot product (which produces a scalar). I appreciate the help.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwenjljntyg%2F1e6f597a1c6cf28b04a500bec1f5efdaf0f54a1e57b799cdb7caa2bdd3d8ddf8%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwenjljntyg%2F1d231040e02e15c8bf065d9ef39787204024ddbbd49b3b2f8d46362f1e172bcc%2FScreenshot_2024-02-02_at_6.03.57_PM.png"" alt=""Screenshot_2024-02-02_at_6.03.57_PM.pngNaN"" /></p>","<p>I&#39;m not sure if you intend what you are writing. <br /><br />Think about the type or the size of the vectors. At first, you have $$(A^\top A) \vec x = \vec 0$$.</p>
<p></p>
<p>The matrix $$A^\top A$$ is an $$n \times n$$ matrix, and $$\vec x \in \mathbb R^n$$, so $$(A^\top A) \vec x \in \mathbb R^n$$.</p>
<p></p>
<p>Then, when you &#34;multiply by $$\vec x^\top$$&#34; on the left, you have $$(A^\top A)\vec x \vec x^\top \in M_{n \times n}(\mathbb R)$$</p>
<p></p>
<p>which means that when you denote $$\vec 0$$ on the RHS, that $$\vec 0$$ is actually an $$n \times n$$ matrix of $$0$$s.</p>
<p></p>
<p>Perhaps what you meant is to take the vector $$A^\top A \vec x$$ and to take its dot product with $$\vec x$$:</p>
<p></p>
<p>$$\vec x \cdot (A^\top A \vec x) = \vec x \cdot \vec 0 = 0$$.</p>
<p></p>
<p>As a hint for continuing from here, remember that a different way to write the dot product $$\vec a \cdot \vec b$$ as a matrix-vector product is $$\vec a ^\top \vec b$$.</p>"
"WP2 Q2.b): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcbyt1x76wg%2F0ff13ddb741845197029be872dcd2f8c8952dd38abf3523d6da12bb66a72054d%2FScreen_Shot_2024-02-03_at_10.57.54_AM.png"" alt=""Screen_Shot_2024-02-03_at_10.57.54_AM.pngNaN"" width=""458"" height=""51"" /></p>
<p>Is it possible to prove this by finding the standard basis for $$\mathbb{R}$$$$^3$$?</p>
<p>ie. finding variables for the linear combinations from the set B such that you find answers for $$\begin{pmatrix}1 \\ 0 \\ 0 \end{pmatrix}$$,$$\begin{pmatrix}0 \\ 1 \\ 0 \end{pmatrix}$$,$$\begin{pmatrix}0\\ 0 \\ 1 \end{pmatrix}$$?</p>","I&#39;m not sure what you are asking.  Are you going to try to express every standard basis vector as a linear combination of the vectors in $$B$$ and then also show that every vector in $$B$$ is a linear combination of the standard basis vectors?  (The second part is very easy)  This would lead to a valid proof.  Similar to the proofs of Q2 in WA1
You are then only proving its true for those vectors, and not all of r^3. Unless you mean the span of the standard basis vectors then I think it&#39;s a valid proof
You are then only proving its true for those vectors, and not all of r^3. Unless you mean the span of e1, e2, e3, then I think it&#39;s a valid proof
You are then only proving its true for those vectors, and not all of r^3"
"Practice problems 2 Q3: <p><a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2Feb9eada293ff4575cf3947b4e6ee04e26e08b702028f8f1035233728557cbd10%2Fimage.jpg"" target=""_blank"" rel=""noopener noreferrer"">image.jpg</a></p>
<p></p>
<p>is my method acceptable for this question?</p>",I didn&#39;t check all your calculations but your approach looks good. 
Just to clarify: So the number of pivots in REF(A) = RREF(A)?,Yes.  To change a matrix from REF to RREF you don&#39;t change any of the pivot positions.  You just change the pivots to 1s and make sure that all the non-pivots are 0.  So the number of pivots won&#39;t change. 
Quiz 2 Readiness: Hi I am wondering what is the best way to prepare for quiz 2. Would you say doing the practice problems posted is enough to prepare us fully?,"If you have been keeping up with the lectures and have done all of the practice problems, then you should be well prepared."
"WP2 Q1 b: <p>Hello,</p>
<p></p>
<p>According to the posted solutions, the given example for an x not in the Span B is [2 2 2]. However, I was wondering if other solutions were possible, namely one in which [x y z] = [2 7 2]?</p>
<p></p>
<p>I thought that I could make y = 7 because the equation for the Span sets it up so y = 2t, thus if y is an odd number, the vector x cannot be in the Span. Would this be adequate logic for this problem?</p>","Yes, there are many different answers. Your reasoning is not sound, however: just because $$y = 2t$$ does not make $$y$$ an even number because $$t$$ can take on any real value, not just integer."
"question 5: I dont understand how x could be expressed in proj and perp like that. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekwghjfhu%2Fea50e9089847085aadc4b7e9f4379212d108128b90faf7e194f3174c98bc281e%2Fimage.png"" alt=""image.pngNaN"" />","<p>By definition,</p>
<p></p>
<p>$$\text{perp}_{\vec u}(\vec x) = \vec x - \text{proj}_{\vec u}(\vec x)$$</p>"
Midterm Coverage: May I ask that which chapter will be covered in the midterm? Thanks,Up until 4.4.
"Week 2 - PP Q7: <p>my solution differed from the posted solutions, so to clarify the process of solving a system - it&#39;s just about:</p>
<p>1) eliminating as many variables as possible</p>
<p>2) setting paramater(s) if needed?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F93034bfa009a43c6c205fb684baf6f27e22a9bab6066e9efbec41b73a4845353%2Fimage.png"" alt=""image.png"" /></p>
<p></p>","<p>Your method seems fine, though unconventionally represented. I didn&#39;t check every line, but if your answer doesn&#39;t line up, then you must have made a simple arithmetic mistake somewhere.</p>
<p></p>
<p>You are right about the method to solving a linear system. The point of performing EROs and reducing a system or augmented matrix to REF or RREF is so you can very easily see which variables are free or fixed.</p>"
"Week 3 Practice, 3b): <p>I&#39;m confused how we know that rank([A|b]) is 2. I understand rank(A) is 2, but how do they know rank([A|b]) is also 2?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8ul9ciu7gn%2Fede2db67726e19750be5e95f41ecdeb1615209c1eb4d2b1e82438605251cdd9e%2FScreenshot_%28168%29.png"" width=""500"" height=""139"" alt="""" /></p>","<p>Check proposition 3.6.4 in the course notes.</p>
<p></p>
<p>The idea is that if $$\text{rank}(A) < \text{rank}( [A \mid \vec b])$$, then augmenting $$A$$ with $$\vec b$$ must yield 1 new pivot in $$\vec b$$&#39;s column. This clearly shows that the system is inconsistent, and you can check that the backward direction also holds.</p>"
"Week 3 PP 6: I am confused about how do we get from the first step to the next.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2F9e9afe2f09bf77669d8f5da67d8c8d3989b7232f60c462674f5ccf2122003bd0%2FWechatIMG74593.jpg"" width=""1378"" height=""190"" alt="""" />","<p>Recall that for a complex number $$a + bi \in \mathbb C$$, $$\text{Re}(a + bi) = a$$. For each $$x_1, \ldots, x_n$$, you can write them as $$x_j = \alpha_jj+ i\beta _j$$, $$1 \le j \le n$$ and check that the two are the same.</p>
<p></p>
<p></p>"
"null space: To be clear, is finding a nullspace, is it finding the solution that makes the Matrix homogeneous?","<p>The nullspace of $$A$$ refers to the set of <em>all</em> solutions $$\vec x$$ so that $$A\vec x = \vec 0$$. That is:</p>
<p></p>
<p>$$\vec x \in \text{Null}(A) \iff A \vec x = \vec 0$$</p>"
Week 4 PP  for quiz2: May I ask which of the problems in the week 4 practice problem is related to quiz 2?,"<p>I did a quick scan and here&#39;s what I think.</p>
<p></p>
<p>From the warm-up exercises:  Q1-Q3.</p>
<p></p>
<p>From the Recommended Problems Q1a,b, Q3a,c,</p>"
"w2pp q3: why is a point on the plane not included in the final solution for the vector equation? Isn&#39;t it supposed to be p=[x x x] &#43; s[x x x] &#43; t[x x x] for a plane?  <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8mampgg62v%2Fab7475d7680f2b399af8c4ea66af1cb5b21b4416fe3e0f18e6c9fea40fa4b538%2FScreen_Shot_2024-02-03_at_9.40.38_PM.png"" alt=""Screen_Shot_2024-02-03_at_9.40.38_PM.pngNaN"" width=""1005"" height=""691"" />","In this case, it means the origin is also on the plane (the first vector is the zero vector)."
Quiz 2 Content: Will quiz 2 cover content from chapter 2 (contents from WA01) or will it only cover content from 3.1 to 3.10?,"I believe the coverage is something like 2.1-2.5, 3.3-3.10. But as always, &#34;content builds on previous content so its technically cumulative.&#34;"
"w4pp Q3c: <p>For (c) in this question am I trippin? It seems like the system is inconsistent.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F3da1db2deeb4e49d2e8f7c6b6f57d91fd43d2e321eeeeacf3ed6148b402a8410%2FScreenshot_2024-02-03_at_10.51.53_PM.png"" alt=""Screenshot_2024-02-03_at_10.51.53_PM.pngNaN"" /></p>
<p>However, the solution seems to suggest there exists a vector x that makes Ax = b true... can somebody help me clarify why and what possible solution this could be?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F0ee995054a05fd5fc99239890cd5c08f906d4ee6f394eb1e4264a8cbe6e201a1%2FScreenshot_2024-02-03_at_10.53.51_PM.png"" alt=""Screenshot_2024-02-03_at_10.53.51_PM.pngNaN"" /></p>","The question just says to look for A b and x that represent the system, it doesn’t require that the system have a solution.
The question just says to look for A b and x that represent the system, it doesn’t require that the system have a solution."
"Mobius MP3 #9 - Is there an error with the solution?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd0mncx94q3%2F5fd237b2e331bb4586421afbaf90f45b0e9a53288d3bdeb63815faf37b82a56b%2FScreenshot_2024-02-03_231919.png"" width=""451"" height=""304"" alt="""" />",I&#39;ll pass this along. 
"Mobius Problems 3 -- Question 9: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkesz5s9tccz3qk%2F2f4a0dbbdb6314b06509e6a92cea9d6f055a76944e11a77730087c8b6cc8433b%2Fimage.png"" alt=""image.pngNaN"" width=""844"" height=""583"" /></p>
<p></p>
<p>I&#39;m not sure why this system is inconsistent? I thought given the matrix above we would have 0x_1 &#43; 0x_2 &#43; 0x_3 = 0...</p>","Yeah, I got this too. I think it&#39;s an error on Mobius&#39; end.
I&#39;ve reported this. "
"Conceptual Understanding: Just to clarify basic variables always the pivot points? Are pivots always 1? Moreover, is the system of linear equation somehow related to matrix multiplication?","<p>I think the convention is that the first column in a matrix corresponds to $$x_1$$, the second column to $$x_2$$, and so on, $$x_n$$ is a basic variable if its column has a pivot point in it. </p>
<p></p>
<p>Pivots are not always 1, you turn them into 1 when you are turning the matrix into the reduced row echelon form.</p>
<p></p>
<p>I don&#39;t know if there is some other relation, but I think systems of linear equations are related to &#34;matrix&#34; multiplication in this way:</p>
<p></p>
<p>$$\begin{bmatrix} a&b&c \\ d&e&f \\ g&h&i\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3\end{bmatrix} = \begin{bmatrix} ax_1+bx_2+cx_3 \\dx_1+ex_2+fx_3 \\gx_1+hx_2+ix_3 \end{bmatrix}$$</p>
<p></p>
<p>and then you can set the resulting matrix equal to the vector with the constant values for each equation to represent the system of linear equations</p>
<p>I think the convention is that the first column in a matrix corresponds to $$x_1$$, the second column to $$x_2$$, and so on, $$x_n$$ is a basic variable if its column has a pivot point in it. </p>
<p></p>
<p>Pivots are not always 1, you turn them into 1 when you are turning the matrix into the reduced row echelon form.</p>
<p></p>
<p>I don&#39;t know if there is some other relation, but I think systems of linear equations are related to &#34;matrix&#34; multiplication in this way:</p>
<p></p>
<p>$$\begin{bmatrix} a&b&c \\ d&e&f \\ g&h&i\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3\end{bmatrix} = \begin{bmatrix} ax_1+bx_2+cx_3 \\dx_1+ex_2+fx_3 \\gx_1+hx_2+ix_3 \end{bmatrix}$$</p>
<p></p>
<p>and then you can set the resulting matrix equal to the constant values for each equation to represent the system of linear equations</p>
<p>I think the convention is that the first column in a matrix corresponds to $$x_1$$, the second column to $$x_2$$, and so on, $$x_n$$ is a basic variable if its column has a pivot point in it. </p>
<p></p>
<p>Pivots are not always 1, you turn them into 1 when you are turning the matrix into the reduced row echelon form.</p>
<p></p>
<p>I don&#39;t know if there is some other relation, but I think systems of linear equations are related to &#34;matrix&#34; multiplication in this way:</p>
<p></p>
<p>$$\begin{bmatrix} a&b&c \\ d&e&f \\ g&h&i\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3\end{bmatrix} = \begin{bmatrix} ax_1+bx_2+cx_3 \\dx_1+ex_2+fx_3 \\gx_1+hx_2+ix_3 \end{bmatrix}$$</p>
<p>I think the convention is that the first column in a matrix is $$x_1$$, second column is $$x_2$$ and so on, $$x_n$$ is a basic variable if its column has a pivot point in it. </p>
<p></p>
<p>Pivots are not always 1, you turn them into 1 when you are turning the matrix into the reduced row echelon form.</p>
<p></p>
<p>I don&#39;t know if there is some other relation, but I think systems of linear equations are related to &#34;matrix&#34; multiplication in this way:</p>
<p></p>
<p>$$\begin{bmatrix} a&b&c \\ d&e&f \\ g&h&i\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3\end{bmatrix} = \begin{bmatrix} ax_1+bx_2+cx_3 \\dx_1+ex_2+fx_3 \\gx_1+hx_2+ix_3 \end{bmatrix}$$</p>"
"Complex Number for RREF: <p>Just to clarify,</p>
<p>When you simplify with complex numbers in RREF, do you always have to multiply its conjugate to simplify into pivot and eliminate any variables below pivot?</p>","I think so, if you have $$a+bi$$ and you want to turn it into 1, you multiply by $$\frac{1}{a+bi}$$ which turns into $$\frac{a-bi}{a^2+b^2}$$
I think so, if you have $$a+bi$$ and you want to turn it into 1, you multiply by $$\frac{1}{a+bi}$$ which turns into $$\frac{a-bi}{a^2-b^2}$$"
"Mobius 4 Q2 how to get solution set: <p>I am confused how they got this answer:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fda0ee03cdc965c0f4f412b2c3084930015ad41a2af4a2a176052caff77927eea%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Why do they let $$x_2,x_3$$ and not $$x_2,x_4$$?, also I don&#39;t know how to get these answers, also why is there $$\frac{t}{2}$$ instead of just $$t$$?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F08254b1e779d6e539b62bf8552bfed690dc10b50e9386923a43fcb2d3610442d%2Fimage.png"" alt=""image.png"" width=""287"" height=""397"" /></p>
<p>I got these systems:</p>
<p></p>
<p>$$\begin{bmatrix} -\frac12+\frac{t}{2}+2s\\s\\ \frac12+\frac{t}{2}\\t\end{bmatrix}$$ for system 2</p>
<p>$$\begin{bmatrix} -\frac52+\frac{t}{2}+2s\\s\\ \frac32+\frac{t}{2}\\t\end{bmatrix}$$ for system 3</p>","Yes, this is odd.  We conventionally set the free variables to be $$x_2$$ and $$x_4$$.  I&#39;ll pass this along. "
Basic Variable: Are basic variables always pivot point?,"If the 𝑖th column of this matrix contains a pivot, then we call 𝑥𝑖 a basic variable.
Basic variables correspond to pivot columns. "
"Practice Problem: On 2b) Prove or disprove: Span B = R 3. When you see these similar types of questions and you want to prove the reverse direction, do you always have to use RREF and find each variables (rough calculation) and then isolate for the coefficient (matrix) or constants (span) ","Could you provide context as to what B is? Usually, you would have to solve the system of equations (most likely in REF/RREF form) to find variables such that you can write the standard basis of R^3 as a linear combination of the vectors in the set B. 
You need to show that every vector in $$\mathbb{R}^3$$ can be written as a linear combination of the vectors in $$B$$.  This is usually done by solving a system of equations.  Sometimes you can see the solution by inspection. "
"Q5: <p><a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweg63r8w7db%2Fb3a2d0554637816067a93ef51223b1b7f4a3697b91ee18fc0e64ad03aa0f857f%2FScreen_Shot_2024-02-04_at_3.50.42_PM.png"" target=""_blank"" rel=""noopener noreferrer"">Screen_Shot_2024-02-04_at_3.50.42_PM.png</a></p>
<p>I was not understanding the part where they add x vector and manipulate v1 variable. Can somebody explain?</p>","<p>Are you referring to the middle paragraph? It is proving that for any $$\vec{w}\in \mathbb{R}^2$$ and $$\vec{w}$$ is orthogonal to $$\vec{u}\neq 0$$, then $$\vec{w}$$ is a multiple of $$\begin{bmatrix} -u_2\\ u_1\end{bmatrix}$$.</p>
<p></p>
<p>Putting it into use, since both $$\mathrm{perp}_{\vec{u}}(\vec{x})$$ and $$\vec{v}$$ are orthogonal to $$\vec{u}\neq 0$$ in $$\mathbb{R}^2$$, $$\mathrm{perp}_{\vec{u}}(\vec{x})$$ and $$\vec{v}$$ are multiples of $$\begin{bmatrix} -u_2\\ u_1\end{bmatrix}$$ and consequently multiples of each other. </p>"
"If a question asks for a span do we have to remove redundant vectors?: Like is the answer span{[1,0]^t, [2, 0]^t} acceptable even though [2, 0]^t is obviously unnecessary?","What is the question?
<p>I don&#39;t think so unless the question explicitly says this like in some questions on mobius 4:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F050b35c4801cb2e27831d33a6e623128136f2bd84e43b7f6285fe8454c920288%2Fimage.png"" alt=""image.png"" /></p>"
"Writing EROs: <p>When writing the EROs that we&#39;re performing while working with a matrix:</p>
<p></p>
<p>Say we&#39;re multiplying R1 by -2 and adding it to R2, is it okay if we write it in the form &#34;R2 - 2R1&#34; instead of &#34;R2 -&gt; -2R1 &#43; R2&#34;. I know the latter it only a little longer, but it is much easier for me to think about it in the first form while doing the ERO.</p>","I don’t understand how “R2 - 2R1” could correspond to multiplying R2 by -2 and adding it to R1.<div><br /></div><div>I think you should stick to conventions to make it easier for other people to read your work. If needed, you can write it however you’d like, and change it at the end.</div>"
is this not a 2x3 RREF: $$\begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & * \\ \end{pmatrix} $$,"Zero rows must be at the bottom in RREF
It has a zero row at the top instead of the bottom."
"w2pp Q4: <p>How are these equivalent? Did we multiply the direction by 5? If so, how did the vector [1 -3/5 0] become [3 -1 2]?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F4413a85d504677717875d96aaef811698062a1cde50b81c5ca8514ae7b5d11a0%2FScreenshot_2024-02-04_at_4.51.46_PM.png"" alt=""Screenshot_2024-02-04_at_4.51.46_PM.pngNaN"" /></p>","You extract $$2\begin{bmatrix}1\\-1/5\\1\end{bmatrix}$$ from $$t\begin{bmatrix}1\\-1/5\\1\end{bmatrix}$$. As $$t$$ runs over all real numbers, so does $$(t-2)$$. Same for $$(t-2)/5$$."
"Möbius Answers: <p>Hi,</p>
<p></p>
<p>I&#39;m just wondering if there are answers to the Möbius questions posted on Learn.</p>
<p></p>
<p>Thanks!</p>","I believe that most Mobius questions have some feedback with them.  You need to submit an answer to see the feedback. 
For every question, on the left, there is a button that says “how did I do”. Click on that and it will tell you if u got it right or not, sometimes with steps to get to the answer."
"Question about consistent system test: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd2nl6yx52t%2F442abbc570df8c0546a507697a812ef8b3a9e97bb2956b3c021a8d1f43acc074%2Fimage.png"" alt=""image.png"" /></p>
<p>The above states that the system is consistent if $$rank(A) = rank(\begin{bmatrix} A | \vec{b} \end{bmatrix})$$.</p>
<p></p>
<p>However, when is the above equality not true? Wouldn&#39;t it always be true since the coefficient parts of both matrices are the same?</p>",See Example 3.6.5 (right below Consistent System Test) in the course notes.
"mobius practice question 8: I&#39;m wondering if anyone has been able to solve this? I&#39;ve been trying but it&#39;s just not giving the right answer<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbucg77l3q2%2Ffd6b8c49ad8ded452f7ec3e22adaf13ef407c1ab69bdb1d993a29e350002eb86%2FScreen_Shot_2024-02-04_at_6.09.19_PM.png"" alt=""Screen_Shot_2024-02-04_at_6.09.19_PM.pngNaN"" />","Try R1 -&gt; (-1/4)R1, R2-&gt; R2&#43;3R_1, R2-&gt; (-1/(8i))R2, swapping R1 and R2, ..."
"Referencing assignment proofs in quiz: <p>Hi, I am wondering whether we can reference properties/proofs that we have already proved in assignments or the practice problems on the quizzes. For example, can I reference the proof on the assignment that states if all vectors in a span belong to a second span, the first span is a subset of the second?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw85bjeyl318%2Ff60b0899667159416880c322a2709ee48aac7569e7380319db3de5e6f73ee2fa%2Fimage.png"" alt=""image.pngNaN"" width=""568"" height=""99"" /></p>","Yes, that is fine."
"What&#39;s the difference between these two definitions?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd3322255l%2F2f89125d99d33d87faf27b3b80dfa997774b6e81bd690831bdde9a062b7f2850%2F90DB5ECB-3B72-4E0D-A98C-1D8A0F997173_1_105_c.jpeg"" alt=""90DB5ECB-3B72-4E0D-A98C-1D8A0F997173_1_105_c.jpegNaN"" />","<p>The <em>first one</em> tells you a way of <em>expressing vectors</em> with terminal points on the plane.</p>
<p></p>
<p>The second one is the <em>set of vectors </em>whose terminal points are on the plane.</p>
<p>--------------</p>
<p>For example, </p>
<ul><li>All even numbers can be written as $$n=2k$$ for some integer $$k$$. </li><li>The set of all even numbers is $$\{2k: k\in \mathbb{Z}\}$$.</li></ul>"
homogenous and trivial solutions: Can a homogenous system ever have a trivial solution?,"Yes, $$\vec 0$$ is always a solution to a homogenous system."
"System Rank: <p>in a) what does the n represent? how did they get the solution set parameters?  </p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flm71gfh8zr81kh%2F5b17a32d52a957f4e08dc885bc443377f72242e8c292b02bdbc2cefb40c30414%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p></p>
<p>Similarly, what does the n here represent? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flm71gfh8zr81kh%2Fe7269881fe2bf907b428f9422ee6993a89684c501297f7585242e1430125fecb%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p></p>",$$n$$ denotes the number of columns in $$A$$ (you can see this in the first declaration $$A \in M_{m \times n}(\mathbb F)$$.
W3PP Q4: Can someone explain how if a doesn’t equal one the system has a unique solution?,"a-1 is the coefficient, it&#39;s not the variable x3. 
See below. "
"W3PP - 5d: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2Ff06786766f5dbecb1e86cbc8882ad54c90618ae200d612f136cc8aa9f1ae261d%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>According to textbook definition, rank must be calculated in RREF form. So the example for 5d given would contain a row of zeroes. Then 5e would be true since a matrix with r &lt; m, must have 1 or more rows of zeroes. </p>
<p></p>
<p>So what would be the solution here? Thanks!</p>","A matrix with less than full rank doesn&#39;t have to have a row of zeroes, but its RREF must have a row of zeroes."
Planes: Did I miss a lecture or something (I didn’t actually miss one) or did we not really go over planes in lecture? Are they an important thing that i should be really comfortable with because the first time i knew they were a thing was on WA1,"lmao yeah they are pretty important,<br />2.4-2.5 in the course notes covers the vector and scalar equations of planes but there are also some Chapter 3 questions that need an understanding of planes (for example WP3 Q1)
Yes, planes are something you should know. "
"WP2 Q4 (Warm Up): <p>I completed this question by finding the scalar equations of both planes, then solving for $$x$$ and $$y$$ in terms of $$z$$ to form a vector equation of a line, however I&#39;m confused how the answers have this last step:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F31f4b599c9a903d1f6f75eca585f226033195bcacf89edd83ecb794326d71a27%2Fimage.png"" alt=""image.png"" /></p>
<p>I stopped at the equation with the fractions inside. I understand that the second vector was multiplied by 5 but I&#39;m not sure what operation was performed on the first vector. Would be grateful if someone could elaborate on this, thanks! </p>","<p>Think of $$\{1+t7: t\in \mathbb{Z}\}$$. The parameter is over $$\mathbb{Z}$$. Adding any <em>integral multiples</em> of 7 to 1 does not change the set, say $$\{15+t7: t\in \mathbb{Z}\}$$.</p>
<p></p>
<p>Now you have $$\{\vec{x} + t\vec{v}: t\in \mathbb{R}\}$$, where the parameter is over $$\mathbb{R}$$. Adding any <em>real multiples</em> of $$\vec{v}$$ to $$\vec{x}$$ does not change the set.</p>
<p>Think of $$\{1+t7: t\in \mathbb{Z}\}$$. The parameter is over $$\mathbb{Z}$$. Adding any <em>integral multiples</em> of 7 to 1 does not change the set, say $$\{15+t7: t\in \mathbb{Z}\}$$.</p>
<p></p>
<p>Now you have $$\{\vec{x} + t\vec{v}: t\in \mathbb{R}\}$$, where the parameter is over $$\mathbb{R}$$. Adding any real multiples of $$\vec{v}$$ to $$\vec{x}$$ does not change the set.</p>"
"WP2 Q5 warm-up -- Assume domain in the Reals?: <p>I was writing the solution set for Q5, and I noticed that nowhere does the question say I am working exclusively over the reals. All of the constants are real numbers so I assume we are working in $$\mathbb{R}$$-- is it okay to assume this if the domain isn&#39;t specified? Or should I use $$\mathbb{C}$$ as the domain to be safe? </p>
<p></p>
<p>I also assume that the quiz will be more specific but I figured I would ask in case I&#39;m missing something. Thanks! </p>","When not mentioned, and all constants are real, it is safe to assume the domain to the reals. This would be different only if the operations involved in the problem would lead a complex solution, e.g. $$x^2 = -1$$. For such cases, the problem needs to be specific so we can decide if there is or not a solution."
"can every system be put into REF/RREF?: I can think of a counterexample for RREF, but it seems like every system can be put into REF? Any ideas?","Correct me if I&#39;m wrong, but I&#39;m pretty sure every system can be put into REF. The caveat is that some systems can be put into REFs using only integers, whilst others will have the leading 1 with the rest of the numbers as a mix of integers and/or fractions.
Correct me if I&#39;m wrong, but I&#39;m pretty sure every system can be put into REF. The caveat is that some systems can be put into REFs using only integers, whilst others will have the leading 1 with the rest of the numbers as a mix of integers and fractions."
"Wpp2: why these two are equivalent?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fgftfmigmcuyr%2FIMG_0543_2024_02_05.jpeg"" /><p></p>",See @252 and @291.
"Quiz 2 coverage: I was wondering whether emphasis on quiz 2 will be given to vectors or matrices, since the majority of the vector content was already covered in the previous written assignment?",Prepare for it all.  
Nullspace question.: does A$$\vec{x} = \vec{0}$$ $$\Rightarrow \vec{x} \in Null(A)$$?,@267
ranks of matrixes: Does the rank of a matrix change after ERO&#39;s have been performed or is the rank of a matrix only referred to from the REF or RREF of the matrix.,The rank of a matrix does not change if you do row operations to it. 
How to calculate rank in REF: I am a bit confused about how to calculate rank in REF.,The rank is the number of pivots.  Do you know what a pivot is?
"spanning and matrices: To show that a given matrix spans a given dimension, would you show that each of the column vectors of the matrix belong to the spanning set of the dimension (the basis vectors)?","Matrices don&#39;t span anything. The column space of a matrix $$A$$, denoted $$\text{col}(A)$$ is, by definition, the span of the columns of $$A$$.<br /><br />I don&#39;t believe we have talked about dimension yet (chapter 8), so could you clarify what you mean by &#34;spans a given dimension&#34;?"
"By the System Rank Theorem,: If rank of A is r= m, then is that a spanning set for $$\mathbb{R}^m$$","Yes!<br /><br />By system rank theorem, if $$\text{rank}(A) = m$$, then the system $$A \vec x = \vec b$$ is consistent for every $$\vec b \in \mathbb R^m$$, which means that $$\mathbb R^m \subseteq \text{Col}(A)$$ (4.1.2), hence the columns of $$A$$ must be a spanning set for $$\mathbb R^m$$."
"mobius03: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweiycmgr50%2Fdaf7fd58f6c298b027e7dae77bc33ca225575910a4214bddaf5f0488e8345dfc%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>I am wondering what is the correct step of calculating the RREF of A</p>",It looks like you may have made an arithmetic mistake somewhere.
"w3pp q5e): <p>How does this counterexample apply to (e)? doesn&#39;t the matrix further simplify to have a row of zeroes? plus, I don&#39;t get how the rank of the 2x2 matrix filled by 1&#39;s is 1. </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Fafd01e9ddb8e7ad4c0bb48e5c71e2153b7c71eb019de7d1e73cdb4eb1204b296%2FScreenshot_2024-02-05_at_12.45.39_PM.png"" alt=""Screenshot_2024-02-05_at_12.45.39_PM.pngNaN"" /></p>","<p>To determine the rank of a matrix, we must look at how many pivots it has in its RREF. It doesn&#39;t matter what the current matrix looks like, we have to row reduce it first.<br /><br /></p>
<p>$$\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \overset{R_2 \to R_2 - R_1}\longrightarrow \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$$</p>
<p></p>
<p>which is in RREF (you should check this yourself). Then, we can see that there is only 1 pivot in the RREF, hence the rank of $$\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$$ is 1.<br /><br />Perhaps where this is confusing you is that the answer would be different if the question asked for the number of zero rows <em>in the RREF</em>, as opposed to in its current form.</p>"
"Identity Matrix: For identity matrix, does it matter which of the diagonals are all 1s? Can a matrix with two such diagnals be an identity matrix?","<p>The identity matrix $$I_{n, m}$$ has</p>
<p></p>
<p>$$I_{n, m}(i, j) = \begin{cases} 1, & i = j \\ 0, & i \neq j\end{cases}$$</p>
<p></p>
<p>where $$I_{n, m}(i, j)$$ denotes the entry at the $$i$$-th row and $$j$$-column.</p>
<p></p>
<p>This is to say that if you start at the top left-corner, there is a $$1$$ there, and you keep moving down-right. That diagonal is full of $$1$$s, and every other entry is $$0$$.<br /><br />You should check that this satisfies the properties of the identity matrix.</p>"
"w2pp - Q5: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2Fb2a1f78502692e30c60592f8ff57f109a5b8a5ebf4c378466b95444bc495229f%2Fimage.png"" alt=""image.png"" /></p>
<p>Could this be a solution to question 5?</p>","<p>You are arbitrarily given two vectors $$\vec u, \vec v$$, of which you only know that they are orthogonal.</p>
<p></p>
<p>You can&#39;t change the value of $$\vec u$$ by setting it equal to $$\text{perp}_{\vec v}(\vec a)$$, but you are on the right track. As a hint, try picking an arbitrary vector $$\vec a$$ and look at:</p>
<p></p>
<p>$$\text{perp}_{\vec v}(\vec a) = \vec a - \text{proj}_{\vec v}(\vec a)$$</p>
<p></p>
<p>What does this equation tell you about how you can express $$\vec a$$?</p>"
"Terminal points V and W: <p>The terminal points, 𝑉 and 𝑊, associated with the vectors, #»𝑣 and #»𝑤, are not usually on the plane. In fact, 𝑉 is a point on the plane if and only if #»𝑢 ∈ Span{ #»𝑣 , #»𝑤}; that is, if and only if 𝑈 lies on the plane through the origin that contains 𝑉 and 𝑊.</p>
<p>Could someone explain this part?</p>","<p>The plane being described has directional vectors $$\vec v, \vec w$$ i.e. has the form</p>
<p></p>
<p>$$P = \{ \vec u + s \vec v + t \vec w : s, t \in \mathbb R\}$$.</p>
<p></p>
<p>the set $$\{s \vec v + t \vec w : s, t \in \mathbb R\}$$ already defines a plane which passes through the origin (by taking $$s, t = 0$$), so if you add $$\vec u$$ to every element in this plane, it shifts the plane by that vector $$\vec u$$. Then, the terminal points $$V, W$$ corresponding to $$\vec v, \vec w$$ can only be on $$P$$ if $$\vec u \in \text{Span}\{\vec v, \vec w)\}$$, as otherwise they would have been shifted away.</p>"
"wp4Q1: I don&#39;t know why this is equal to [7 5], in my own idea i calculate this to [8 3]<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvahtlzux65ur%2F2f5433a94fbe337e807f44018fed715897811c28c9f2e2fdea48c33184b349ba%2F___2024-02-05_13.45.48.jpeg"" alt=""___2024-02-05_13.45.48.jpegNaN"" />","It’s 2x3 and 1x1 and then 2x2 plus 1x1.
To get the first entry you used the first row of the first matrix and $$\vec{b}$$.  So you get $$3(2)+1(1)$$.  The second entry is $$2(2)+1(1)$$."
quiz 2: do we have questions about nullspace and proof questions?,"<p>Null space was introduced in section 3.7 which is in the scope of the quiz. </p>
<p></p>
<p>Proof questions are possible on the quizzes. </p>"
"determine inconsistent or consistent: <p>Is this matrix consistent or inconsistent&#xff1f;</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2F3f2c46136a3abca4618d724e6da63ca2e0edb13edfe81332b76201c720ff396d%2FIMG_6028.jpeg"" width=""446"" height=""331"" alt="""" /></p>","Yes, by the consistent system test. I would believe it has nullity of 2, and hence 2 parameters.
This augmented matrix corresponds to a consistent system. "
were the wa1 solutions released?: ,"They haven&#39;t been posted yet.  They will be released later today. 
I checked and I don&#39;t think they were posted yet.  Usually they get posted when the assignment is returned. "
"Can we solve b by showing rank of the matrix corresponding to the span is 3?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2F94b4fce1aba3099c4394fb36ac526bb8f601b23f16f82d367930aed5eeb58af5%2FScreenshot_2024-02-05_at_3.09.29_PM.png"" width=""940"" height=""346"" alt="""" />",That could be part of the solution. 
WA1 solution: Will WA1&#39;s solution be posted?,They will be posted later today. 
"WA1 Regrade Request: <p>When will we be able to start submmitting regrade request for wa1? and since I have never submit one before, how am I supposed to submit it? Is it through crowdmark or on learn?</p>
<p></p>
<p>Thanks!</p>",Please wait a full day (24hrs) until you submit regrade requests. You can find the page for submitting requests on learn.
when WA2 come out?: ,Not until after the midterm. 
Could we not have to write quizzes in RCH 101/201 in the future please. Thanks: ^title. <br />The desks were very small and made writing quite a bit more difficult. ,"Add AL 116 to this list lol
We don&#39;t have a lot of control over which rooms we write the quizzes in.  We need very large rooms all at the same time and there are limited options. "
"understanding class note: Hi, I am a bit of confuse how do we get e1 e2…here by having the eros. Could someone explain it?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2F4636549a6a0f78d7b90d52d22716459358660c4a9895fd0693f8d10e5dcf7dee%2F9757c5d103ad8fe69fbcc3cd0bb0ae25.jpeg"" width=""1043"" height=""486"" alt="""" />","<p>I think there is a typo. Between the second and third matrix in the line of 5 matrices, the row operation performed should be $$R_2 \to R_2 - 3R_1$$.</p>
<p></p>
<p>To find a matrix which corresponds to a row operation, you can apply that row operation to the identity matrix as a matrix on the left (you should really try this out with a few and check that it makes sense).</p>
<p></p>
<p>For instance, in the example above, the first step is to swap $$R_1 \leftrightarrow R_2$$. If we apply this operation to the identity matrix, we get $$\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$$, so</p>
<p></p>
<p>$$\begin{bmatrix} 3 & 0 \\ 1 & 2 \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 3 & 0 \end{bmatrix}$$</p>
<p></p>
<p>Then, we apply the row operation of $$R_2 \to R_2 - 3R_1$$, which, if we apply this to the identity matrix, we get $$\begin{bmatrix} 1 & 0 \\ -3 & 1 \end{bmatrix}$$, hence</p>
<p></p>
<p>$$\begin{bmatrix} 1 & 2 \\ 0 & -6 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ -3 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 3 & 0 \end{bmatrix}$$</p>
<p></p>
<p>To see why this is true, we can try by picking any ERO. Write out the identity as a bunch of row vectors:</p>
<p></p>
<p>$$I = \begin{bmatrix} \vec v_1 ^\top \\ \hline \vdots \\ \hline \vec v_m^\top \end{bmatrix}$$, $$\vec v_i \in \mathbb R^m$$.</p>
<p></p>
<p>Then, for each row operation, write out how that would change $$I$$ (e.g. if we wanted to do $$\vec v_i^\top \to \vec v_i^\top + \vec v_j^\top$$, then we can write our new matrix as<br /><br />$$J := \begin{bmatrix} \vec v_1^\top \\ \hline \vdots \\ \hline \vec v_i ^\top+ \vec v_j^\top \\ \hline \vdots \\ \vec v_m^\top \end{bmatrix}$$</p>
<p></p>
<p>and then check that $$JA$$ gives the desired row operation on $$A$$ (you should convince yourself of this part).</p>"
"If nullity of a matrix A is 0, then is the RREF(A) = I_{n}: Stumbled on this practise problem online. I believe the answer is false as you could have an inconsistent matrix that still has a nullity of 0. Then I_{n} \neq RREF(A)","If $$\text{nullity}(A) = 0$$, then $$n = \text{rank}(A)$$, so the number of columns is equal to the number of pivots in the RREF, so there must be a pivot in every column in the RREF. This certainly reduces to the identity, so the answer is true.<br /><br />I don&#39;t follow your logic, but you are confusing matrices with augmented matrices. Matrices aren&#39;t consistent or inconsistent because they don&#39;t represent systems of linear equations. You need to augment a matric with a specific vector $$\vec b$$ so to &#34;set&#34; each equation equal to something. 
If $$\text{nullity}(A) = 0$$, then"
"Quiz 2 marks and solution release: <p>I wonder if quiz 2 marks will be released before our midterm and also the solution as well.</p>
<p></p>
<p>Thanks</p>","<p>We are already grading it. The expectation is that we can release the grades Monday morning. The solutions will be posted today.</p>
<p></p>
<p>---</p>
<p></p>
<p>The solutions to Quiz 2 have been posted on Learn.</p>
We are already grading it. The expectation is that we can release the grades Monday morning. The solutions will be posted today.
We are already grading it. The expectation is that we can release the grades Monday morning. The solutions will be posted today.
We are already grading it. The expectation is that we can release the grades Monday morning."
"Practice problem 4: How can this change like this? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2Ffbb7f7f7e6397d708b26f4f786f319460d3e56b19ce16ea2a65edbedc89a3e06%2Fimage.png"" alt=""image.png"" />","This may happen when within the list of vectors that you are spanning there is one or more vectors that can be written as a combination of the other. So, if $$\begin{bmatrix} 0 \\ 0 \\ -1\end{bmatrix}$$ could be spanned from the other two, including it on the spanning list will not make a difference.<br /><br />Another example is Span{$$\vec{e}_1, \vec{e}_2, \vec{0}$$} = Span{$$\vec{e}_1, \vec{e}_2$$}. The zero vector being included in the spanning set will clearly not generate any different vectors via liner combinations, since it is a combination of the other two, i.e., $$0\cdot\vec{e}_1+0\cdot\vec{e}_2 = \vec{0}$$."
"week 4 practice problem: <p>what does this mean? </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2Fe105183ee7264a0d015ddd56727de71bb44b35dbb6ba7e2fa2a9a8de2a788cf0%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>The $$\vphantom{}^\top$$ or $$\vphantom{}^T$$ gives the transpose of a matrix, which flips it along the main diagonal (@24), so the left $$\vec u^\top \vec v$$ represents matrix-vector multiplication (where $$\vec u^\top$$ is a $$1 \times n$$ matrix). Writing out <br />$$\vec u = \begin{bmatrix} u_1 \\ \vdots \\ u_n\end{bmatrix},\vec v = \begin{bmatrix} v_1 \\ \vdots \\ v_n\end{bmatrix}$$</p>
<p></p>
<p>you should check that the matrix multiplication is the same as the dot product.</p>"
"Question 1d) Practice Problem: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F26e8e50492d1669a96573d5c535917f21bacc23c44b0b448d20c08601c560ed3%2Fimage.png"" alt=""image.png"" width=""539"" height=""334"" /></p>
<p></p>
<p>For the solution to the recommended question 1d), what is the significance of b = [5 1 0]^T  / how did they find it?</p>","<p>$$\vec b = \begin{bmatrix} 5 \\ 0 \\ 1\end{bmatrix}$$ is just a vector in the solution set.</p>
<p></p>
<p>They found that $$x_3 = 1, x_2 = t, x_1 = 5 - 4t$$ parameterizes the solution, so taking $$t = 0$$ yields a point, which happens to be the easiest point to find.</p>"
"Would q3 (of Quiz 2, T/F) be true if the same row operations were applied to the vector b?: ",q3 of what?
"Diagonals: <p>is the following matrix considered diagonal?</p>
<p></p>
<p>0 1</p>
<p>1 0</p>
<p></p>
<p>Or is it only when the non-zeros are going down from left to right.</p>","It is not a diagonal matrix.
It is not a diagonal matrix.  "
"Unrecognized theorem: <p>What theorem is this from? I noticed this was used for an extra practice question</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2F307c884a70dcc072d9d634fbbfc6920bf1c34faf52a693444bda36d7d6288162%2FScreen_Shot_2024-02-08_at_3.05.23_PM.png"" alt=""Screen_Shot_2024-02-08_at_3.05.23_PM.png"" /></p>","This is not true. We have $$\left(\vec{u}^T\vec{v}\right)_{1,1}=\vec{u}\cdot\vec{v}$$. This is just matrix multiplication. Try it!
<p>The student answer is correct, but the matrix multiplication $$\vec u ^\top \vec v$$ gives a $$1 \times 1$$ matrix, which we often canonically associate with the value inside of that $$1 \times 1$$ matrix.<br /><br />i.e. if we ever write $$\vec u^\top \vec v \in \mathbb R$$ or in any other way implicitly assuming that $$u^\top v$$ is a real-value or complex-value, then we actually mean to write $$(\vec u^\top \vec v)_{1, 1}$$.</p>
<p></p>
<p>We choose to write $$\vec u^\top \vec v$$ because writing $$(\vec u^\top \vec v)_{1, 1}$$ is very pedantic and conveys almost no extra meaning.</p>"
is any question from wp5 relevant for the midterm: ,"The midterm covers until 4.4, and the coverage for this week starts at 4.5, so no."
"This is from the textbook, not sure what it means: ","If you meant to upload an image, it didn&#39;t load."
"Not sure what this means: <img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwedhx99h6vo%2Fniblyoojulsi%2FIMG_0518_2024_02_09.jpeg"" /><p></p>This is from the textbook","<p>Is there supposed to be a photo attached? I don’t see one</p>
<p></p>
<p>----</p>
<p></p>
<p>Edit:</p>
<p></p>
<p>The theorem states that if you have a solution set to a homogenous system, and you wish to derive a solution set to a non-homogenous system, then if $$\vec x_p$$ satisfies $$A \vec x_p = \vec b$$, then you know that $$\vec x_p + \vec x$$, $$\vec x \in S$$ is also a solution to the non-homogenous system, because</p>
<p></p>
<p>$$A(\vec x_p + \vec x) = \underbrace{A \vec x_p}_{= \vec b} + \underbrace{A \vec x}_{= \vec 0} = \vec b$$</p>
Is there supposed to be a photo attached? I don’t see one"
Referencing Theorems on the Midterm: Are we allowed to use theorems proven in the written assignments on the midterm without proof?<div><br /></div><div>Thank you!</div>,"Also, if we can’t remember the specific name of a proposition or theorem, would we be able to just write “by theorem/proposition taught in class”?
Yes, that is fine."
"Q4PP Q8b): In this question, you are asked to do a matrix multiplication of 2x3 matrix and 3x2 matrix. Shouldn&#39;t the answer be a 2x2 matrix? The solution has a 3x3 matrix as the answer.","Yeah I think it might be wrong was about to put up a post about this. Also I think the same thing applies to Q9d if I&#39;m not mistaken
Yeah I think it might be wrong was about to put up a post about this.
<p>I believe you have your dimensions mixed up.</p>
<p></p>
<p>When we say a matrix is of size $$m \times n$$, it has $$m$$ rows and $$n$$ columns. In general, when we perform matrix multiplication, for $$A \in M_{m \times n}(\mathbb F), B \in M_{n \times p}(\mathbb F)$$, the product $$AB$$ has size $$m \times p$$.</p>
<p></p>
<p>Note that the second dimension of $$A$$ and the first dimension of $$B$$ must be the same (otherwise you are taking the dot product of two things of different dimension).</p>
<p></p>
<p>Lastly, recall that in matrix multiplication (say $$AB$$), we are taking the dot product of rows of $$A$$ with columns of $$B$$.</p>
<p></p>
<p>The solution given is correct (I just verified it, too).</p>"
"Week 3 practice problem 1f: <p>what am i doing wrong?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2Fd35cd82160d63c92ebace13cd99fba8b11d0bf3cb440ea6b66ca738733f455bd%2Fimage.png"" alt=""image.png"" /></p>",The third row of your 4th matrix isn&#39;t clear to me. You should get $$i-2 - (i+1)(i-1)= i-2 - (-2)=i$$ instead of $$3i-1$$.
"Struggling a lot with turning a matrix into REF, RREF: <p>I seem to be struggling a lot with getting the correct elementary row operations for me to turn a matrix/system of equations into REF or RREF quickly. I would often have to do SO many operations (and often stray the wrong way while I&#39;m doing all of them), so what I am doing is probably inefficient. I don&#39;t think I have a good intuitive grasp of what I need to do when I see a matrix like this with so many coefficients and terms:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkesz5s9tccz3qk%2Ffc60b2ae8f6b54dd5f6b7e8679714438d35c5d0299f31d7f0a75d0f5e99dadbc%2Fimage.png"" alt=""image.png"" width=""473"" height=""190"" /></p>
<p></p>
<p>How can I get better at doing these quickly?</p>","<p>my strategy is to try to get each pivot to equal 1 (starting with the one in position 1,1) one column at a time so it&#39;s easier to use ERO to eliminate all other coeffs in that column. i like to start off with making the coefficient in position 1,1 equal to 1 as i can then get rid of the rest of the coefficients in column 1 using elementary row operations. i would then simplify, and repeat the process with the next column that can have a pivot.</p>
<p>for a question like this, if i want the coeff at 1,1 to equal 1, i could swap r4 and r1. i&#39;d then use ERO: r2 -&gt; r2-4r1, r3 -&gt; r3&#43;3r1, r4 -&gt; r4-2r1. after simplifying, r2=r3=r4, so using ERO: r3 -&gt; r3-r2, r4 -&gt; r4-r2, so we end up with two zero rows. after this step, the next possible column with a pivot is column 3, so you could repeat this process there. i hope this makes a little bit of sense :,)))) </p>
<p></p>"
"practice midterm q7: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedhx99h6vo%2F7e7ef0c9601778254ba534b56240723c1bdbd82ff7c2213a14933e7e8aaf2570%2FScreen_Shot_2024-02-10_at_8.03.08_AM.png"" width=""1180"" height=""363"" alt="""" /><br /><br /><br />i was just wondering how they derive these two assumptions from those facts, liek what is the connection? and are they always true?","These follow from definitions of column space (a span of columns of A), and nullspace. See page 23 and 82 in the course notes.
These follow from definitions of column space (a span of columns of A), and null space. See page 23 and 82 in the course notes.
These follow from definitions of column space (a span or columns of A), and null space. See page 23 and 82 in the course notes."
"How do u convert parametric equations into a matrix?: do we need to know how to do this? if so, how?","You can take the coefficients of the variables in the parametric equations and that will be the coefficient matrix, and the constants become the vector to the right of the augment. This will create an augmented matrix"
"Week 4 Extra Problem Q2: <p>How does this imply that A is the zero matrix? I&#39;m confused how this differs from just assuming that any system has a homogenous solution to find nullspace</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8rpo1vu6yt%2F8c5c202424db8a544a201e27a5d98d98790e921beeb45d2912aad8944ac3560c%2FScreenshot_2024-02-10_at_11.21.05_AM.png"" alt=""Screenshot_2024-02-10_at_11.21.05_AM.pngNaN"" width=""727"" height=""84"" /></p>","<p>&gt;&gt;How does this imply that A is the zero matrix?</p>
<p>This follows from Lemma Column Extraction (p100) in the course notes. </p>
<p></p>
<p>&gt;&gt;I&#39;m confused how this differs from just assuming that any system has a homogenous solution to find nullspace.</p>
<p>Quanfications are different! As you said, every homogenous system has at least one solution, but <em>not</em> evrey system has the solution set being the whole space. This question is saying that fix a system of linear equation, if nullspace is the whole space then the coefficient matrix is zero matrix.</p>
<p></p>
<p></p>
<p>&gt;&gt;How does this imply that A is the zero matrix?</p>
<p>This follows from Lemma Column Extraction (p100) in the course notes. </p>
<p></p>
<p>&gt;&gt;I&#39;m confused how this differs from just assuming that any system has a homogenous solution to find nullspace.</p>
<p>Quanfications are different! This question is saying that fix a system of linear equation, if nullspace is the whole space then the coefficient matrix is zero matrix.</p>
<p></p>
<p>As you said, every homogenous system has at least one solution, but <em>not</em> evrey system has the solution set being the whole space.</p>"
"finding the inverse of a 2 by 2 matrix: When you are finding the inverse of a 2 by 2 matrix, is it possible to solve it using the algorithm rather than the formula? When I used the formula I got a different answer than if I were to use the algorithm","Yes, you can use the algorithm. Both should give you the same answer. In fact, the formula comes from doing the algorithm on a general 2x2 matrix, $$\begin{bmatrix} a \ b \\ c \ d \end{bmatrix}$$."
"Span: . In general, how to find a vector that is orthogonal to a plane Span{<br />#»v ,<br />#»w} in R<br />n with<br />n &gt; 3?","Dot product i believe
Dor product i believe
<p>For a vector to be orthogonal to $$\text{span}\{\vec v , \vec w\}$$, it needs only to be orthogonal to both $$\vec v, \vec w$$, so if we want to find such a $$\vec u$$, set</p>
<p></p>
<p>$$\vec u \cdot \vec v = 0, \vec u \cdot \vec w = 0$$.</p>
<p></p>
<p>You can write out both systems and solve for the $$\vec u$$ (in general, there should be many, many free variables, assuming $$n > 0$$).</p>
<p></p>
<p>Alternatively, you can take an arbitrary vector $$\vec w$$ and take $$\text{perp}_{\vec v}(\text{perp}_{\vec u}(\vec w))$$ (you should take a look at the solutions to WA1 q3b if you haven&#39;t, as they discuss the orthogonality of this vector to $$\vec u, \vec v$$.</p>"
"Mobius Week 4: Hello I was wondering for this question what it meant by giving the smallest possible size for Col(A). As well in the solution, why do we keep the columns of A with basic variables? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2ercx056z%2F611794fc602c29244719ce6308a13e710d5301b5c83d743167f75e2e14bf745f%2Fimage.png"" alt=""image.png"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2ercx056z%2Fadeea5e67a82fc1a2417d3f14ab6ac6ff6aa84ff2404e020644def279ed86107%2Fimage.png"" alt=""image.png"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2ercx056z%2Fadeea5e67a82fc1a2417d3f14ab6ac6ff6aa84ff2404e020644def279ed86107%2Fimage.png"" alt=""image.png"" />","<p>This is asking for a smallest spanning set that can span a given space. </p>
<p></p>
<p>For instance, to span $$\mathbb{R}^2$$ one needs at least <em>two</em> vectors. One vector isn&#39;t sufficient and three or more vectors may be redundant. Not arbitrarily two vectors could span $$\mathbb{R}^2$$. You need to take them smartly.</p>
<p></p>
<p>In this problem, you want to find such a set that spans $$\mathrm{Col}(A)$$. The algorithm tells you that (1) reduce $$A$$ to $$\mathrm{REF}(A)$$; (2) Take the original columns of $$A$$ cooresponding to the pivot positions. </p>
<p></p>
<p>&gt;&gt;why do we keep the columns of A with basic variables?</p>
<p>This is a great question! We will learn the proof very soon. If you are interested in it, you can read Prop 8.3.6 (p201) in the course notes.</p>
<p></p>
<p></p>
<p>This is asking for a smallest spanning set that can span a given space. </p>
<p></p>
<p>For instance, to span $$\mathbb{R}^2$$ one needs at least <em>two</em> vectors. One vector isn&#39;t sufficient and three or more vectors may be redundant. Not arbitrarily two vectors could span $$\mathbb{R}^2$$. You need to take them smartly.</p>
<p></p>
<p>In this problem, you want to find such a set that spans $$\mathrm{Col}(A)$$. The algorithm tells you that (1) reduce $$A$$ to $$\mathrm{REF}(A)$$; (2) Take the original columns of $$A$$ cooresponding to the pivot positions. </p>
<p></p>
<p>&gt;&gt;why do we keep the columns of A with basic variables?</p>
<p>This is a great question! We will learn the proof very soon. If you are interested in it, you read Prop 8.3.6 (p201) in the course notes.</p>
<p></p>
<p></p>"
"Quiz 2 Solutions Q2) b: <p>Can somone explain why the answer is 2 for Q2) b? i was thinking 1</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F1553a7ed46c70767bace7353df598b660f1253e2326a7b401934bb6b8c35762f%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F1f51ed4d908637d357f21cdd68288012f3acab444268b89aff816244296ed86f%2Fimage.png"" alt=""image.pngNaN"" /></p>","There are 6 linear equations, so 6 rows and at most 6 pivots. Among 8 variables, at least 2 doesn&#39;t correspond to pivots, so at least 2 free parameters. "
"Difference between 0 and vector 0: <p>What&#39;s the difference between 0 and $$\vec{0}$$</p>
<p></p>
<p>Is it wrong if I say $$\vec{u}$$ and $$\vec{v}$$ is orthogonal if $$\vec{u}$$ . $$\vec{v}$$ = $$\vec{0}$$, not 0</p>","Yes, as 0 is a scaler while the zero &#34;vector&#34; is a vector, the basic physics definition between a scaler and a vector is that a scaler has only magnitude &#34;size&#34; while a vector has both magnitude and direction. <div>So a dot product of 2 numbers should return a scaler hence 0, and suppose a vector - itself will return the 0 Vector. </div><div>Hope that helps </div>
<p>Our usage of the vector arrows over letters is merely a convention to allow the reader to easily infer which variables are vectors and others are scalars.</p>
<p></p>
<p>Writing $$\vec u \cdot \vec v = \vec 0$$ is technically not incorrect, because $$\vec 0$$ could be the vector in the vector space $$\mathbb R$$, but in general we want to reserve the vector arrow for elements of $$\mathbb R^n$$ (such as $$\vec u, \vec v$$, and we don&#39;t write the vector arrow for elements of $$\mathbb R$$ (such as $$\vec u \cdot \vec v$$).</p>"
"PP Week 4 Q1d: Hi in question 1 d below:  this is the solution. My question is how do we not that in the augmented matrix, that b vector is equal to 5 1 0?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb4wzraz68t%2Feb0c8a9578132832486f5ed01cf0d95c3ddb51d58341004cae089dae1a6eeb84%2Fimage.png"" alt=""image.pngNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb4wzraz68t%2Fc1e44d15bc121e1e3ae42b6f42686ccd146a9d020ef84334058c2ccf5225b70f%2Fimage.png"" alt=""image.pngNaN"" />","youre supposed to reduce [8 13 -11]$$^T$$ along with the coefficient matrix A
youre supposed to reduce $$[8 13 -11]^T$$ along with the coefficient matrix A"
"Q5 f): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2F00552852dd6f4cb8cf5bccc83e03e64c41224d4fee4c455ac1a2c2d5e67d2da4%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>For this question why can&#39;t a 0 matrix be an example of a system that satisfies the condition?</p>","<p>Note that the system specifies that all solutions are exclusively integer points.</p>
<p></p>
<p>The set of all integer points $$\begin{bmatrix} n \\ m \end{bmatrix}, n, m \in \mathbb Z$$ might satisfy a system</p>
<p></p>
<p>$$\mathcal O_{2 \times 2} x = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$</p>
<p></p>
<p>but a lot of non-integer points such as $$\begin{bmatrix} 1.5 \\ 0.5 \end{bmatrix}$$ also satisfy this system.</p>
<p></p>
<p>Something to note is that for $$A \in M_{m \times n}(\mathbb R), \vec b \in \mathbb R^m$$, if $$\vec u, \vec v \in \mathbb R^n$$ satisfy $$A \vec u = \vec b, A \vec v = \vec b$$, then the <em>midpoint</em> of $$\vec u$$ and $$\vec v$$, given by</p>
<p></p>
<p>$$\vec z = \frac 12 \vec u + \frac 12 \vec v$$ also satisfies</p>
<p></p>
<p>$$A(\vec z) = A\left(\frac12 \vec u + \frac12 \vec v \right) = \frac 12 A\vec u + \frac 12 A\vec v = \frac 12 \vec b + \frac 12 \vec b = \vec b$$</p>
<p></p>
<p>hence $$\vec z$$ is also a solution for the linear system $$A \vec x = \vec b$$. From here, you can easily derive that no solution to any linear system can be exclusively the set of all integer points.</p>"
Dot product matrix and vector: Does the dot product apply to a matrix and a vector? If so how? ,"Matrix-vector multiplication is computed similar to the dot product, but it is not the dot product. The dot product is only for vectors I believe. "
"terminal points on the plane: <p>I don&#39;t understand why &#34;terminal points of $$\vec{v}$$ and $$\vec{w}$$ are on the plane iff plane goes through the origin&#34;</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8b86aib46q%2F60078aa8e217f1dd84317df93263074449848aa52c7b2c3061b5b2b1aeded61f%2FIMG_2653.jpeg"" alt=""IMG_2653.jpegNaN"" width=""636"" height=""481"" /></p>
<p></p>
<p>Isn&#39;t plane infinite so it could contain any points?</p>","<p>If you have a plane of the form $$\vec p = \vec u + \alpha \vec v + \beta \vec w$$, where $$\vec u \notin \text{span}\{\vec v, \vec w\}$$, then note that if $$\vec u = \vec 0$$, then the plane clearly passes through the origin because you can take $$\alpha, \beta = 0$$. Then, if we take $$\vec u \neq \vec 0$$ with $$\vec u \notin \text{span}\{\vec v, \vec w\}$$, adding $$\vec u$$ to every point on the plane &#34;shifts&#34; that plane by the vector $$\vec u$$. You can think of this as moving the plane (keeping it parallel) until it reaches the terminal point of $$\vec u$$. From here, you can clearly see that neither of the terminal points of $$\vec v, \vec w$$ still remain on the plane.</p>
<p></p>
<p>To show this algebraically, for the forward direction, you would show that $$\vec u \in \text{span}\{\vec v, \vec w\}$$, and for the backward direction, you would show that you must have $$\vec u \in \text{span}\{\vec v, \vec w\}$$ because you can write </p>
<p></p>
<p>$$\vec 0 = \vec u + \alpha \vec v + \beta \vec w$$ for some $$\alpha, \beta \in \mathbb R$$ since $$\vec 0$$ is on the plane.</p>"
"Practice Midterm Q9 c: <p>why is this the case? that x=uxv is a solution to the homogeneous system too?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F4bdfe8a8edf1735d82ab8733b88246853d7a97486c7032e14bbcf74ea459cbaf%2Fimage.png"" alt=""image.pngNaN"" /></p>","By construction of the linear system $$A\vec x = \vec 0$$, satisfying the system requires a vector which is orthogonal to both $$\vec u, \vec v$$, and if a vector is orthogonal to both $$\vec u, \vec v$$, it must also satisfy the system."
"Week4 Q2b: <p>For this question: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehgz4527lv%2Fd0338fe63c73731ea891fcc8e985b908c64885954cc2afa6932827703cdc8b53%2Fimage.png"" alt=""image.png"" />, is it good enough to explain that the ith row is multiplied by xi, and the ith column is multiplied by yi (and the reverse for the right side), and explaining how it implies that the rows and columns of the Matrix are the same?</p>
<p></p>
<p>I also don&#39;t understand how $$A\vec{y} = a_{j}$$: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehgz4527lv%2Ffad6fe9e1ce85278dcbd65620cba0ff065bd72a2cf300d21c7648839944c4e2a%2Fimage.png"" alt=""image.png"" />.</p>","<p>I don&#39;t follow your description of your idea: how does this imply that the &#34;rows and columns [...] are the same&#34;? (and what does it mean for a row and column to be the same?)</p>
<p></p>
<p>Recall that if $$\vec e_j$$ is the $$j$$-vector of the standard basis vector (i.e. has a $$1$$ at the $$j$$-th entry and $$0$$s everywhere else), then $$A \vec e_j$$ gives the $$j$$-th column of $$A$$.</p>"
Nullspace Q4b practice problems 4: Can I say that Null(A) = Span{s$$\begin{bmatrix} 1\\ 1\\ 0\\ \end{bmatrix}$$ : s $$\in$$ R} ?,"<p>Technically, that is the same set, but that is because</p>
<p></p>
<p>$$\text{span}\left \{ s \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} : s \in \mathbb R\right\} = \text{span}\left\{\text{span}\left\{\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}\right\}\right\}$$</p>"
Resolved: ,Marking this as resolved.
"Confusion about finding the scaler eq. of a plane: <p>Hi, I&#39;m confused about how we get to the second last step for finding a scaler equation for the plane. Why are we using the cross product vector with the (x - 1) etc...</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2Ffe48c035b6f393cc191be932a51cc5a9a4dc16cf263a3daca0109091d93f0c1f%2FScreenshot_2024-02-10_at_5.04.18_PM.png"" alt=""Screenshot_2024-02-10_at_5.04.18_PM.pngNaN"" width=""428"" height=""203"" /></p>
<p>Thanks!</p>","<p>For scalar equations of the form $$ax + by + cz = 0$$, it is clear to see that $$a, b, c$$ is a normal of the plane because</p>
<p></p>
<p>$$\begin{bmatrix} a \\ b \\ c \end{bmatrix} \cdot \begin{bmatrix} x \\ y \\ z \end{bmatrix} = a x + by + cz$$</p>
<p></p>
<p>hence any point $$(x, y, z)$$ on the plane satisfies that it is orthogonal to the normal vector. Moreover, you will note that $$(0, 0, 0)$$ is on a plane of the form $$ax + by + cz = 0$$. If instead we know of a different point such as $$(\alpha, \beta, \gamma)$$ which is on the plane, then we can &#34;shift&#34; the plane, following the same idea as shifting functions:</p>
<p></p>
<p>$$a(x - \alpha) + b(y - \beta) + c(z - \gamma) = 0$$ shifts this plane so that $$(\alpha, \beta, \gamma)$$ is on the plane. After this step, we can expand and simplify</p>"
"Practice Midterm - Q3: <p>can anyone explain how you know this, how do you determine the equation? thanks.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb4wzraz68t%2F43570a20e2f77df40e62e5fc9b529acf62d89ce57c2f0a78aa1b36163777024a%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>This follows from the equivalent definition of the dot product:</p>
<p></p>
<p>$$\vec u \cdot \vec v = \|\vec u\| \|\vec v\| \cos \theta$$ where $$\theta$$ measures the angle between $$\vec u, \vec v$$.</p>"
"Question regarding null space and span: <p>I had the following two questions: </p>
<p></p>
<p>(1) Is the null space of a matrix always a span of vectors? Is there a way to find of how many vectors it&#39;s a span of?</p>
<p>(2) If we are given a solution set to a system how do we find the general solution (i.e. the solution to the corresponding homogenous system)?</p>
<p></p>","<p>Good questions!</p>
<p></p>
<p>(1) Yes! We will cover more on this in later weeks, and we will provide a more rigorous meaning of &#34;how many vectors&#34; it is a span of. The short answer is that it is the nullity of the matrix, and the long answer has to do with linear independence of vectors (you can look this up yourself if you would like) and the fact that the nullspace of a matrix is a vector space itself, and has a <em>dimension</em>, which measures exactly the least number of vectors needed to span its entire set. It turns out the nullity of a matrix is exactly the dimension of the nullspace.</p>
<p></p>
<p>(2) You should take a look at 3.11.9 in the course notes.</p>
<p>Good questions!</p>
<p></p>
<p>(1) Yes! We will cover more on this in later terms, and we will provide a more rigorous meaning of &#34;how many vectors&#34; it is a span of. The short answer is that it is the nullity of the matrix, and the long answer has to do with linear independence of vectors (you can look this up yourself if you would like) and the fact that the nullspace of a matrix is a vector space itself, and has a <em>dimension</em>, which measures exactly the least number of vectors needed to span its entire set. It turns out the nullity of a matrix is exactly the dimension of the nullspace.</p>
<p></p>
<p>(2) You should take a look at 3.11.9 in the course notes.</p>"
"Trig Identities: I see in PP4 Warmup Q11, trig identities are used. Are we expected to memorize trig identities for the midterm?","it is unlikely, but good to know as general knowledge. if you want to be safe memorize it
You will not need to memorize any trig identities for this midterm."
"Proof of Equality of Matrices: <p>I don&#39;t see how this proof is sound because it only shows its true when x is equal to the 𝑖𝑡ℎ standard basis vector 𝑒𝑖...</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F6bf9ff83283437f31de368b01afd7cacd979e87a80efdd116692914a16d3d28c%2FScreenshot_2024-02-10_at_6.13.50_PM.png"" alt=""Screenshot_2024-02-10_at_6.13.50_PM.pngNaN"" /></p>","We assume that Ax=Bx for all x for the reverse implication. Since we assume Ax=Bx then that means x can be anything, so they choose the standard basis vectors."
"Midterm practice Q8 - Is this typo..?: If I understand something wrong, please let me know <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2F55a639b29706236bf9409bf3c99ee5bf2e343038c2d055c7fc409b060195609f%2Fimage.png"" alt=""image.pngNaN"" />","seems like it should be u&#43;v
Note that an easier way to solve this question is to use WA 1 2 a)"
Midterm practice is just one?: Are there more practice for midterm? ,"there is one posted sample midterm, but you can find more from previous years at https://services.mathsoc.uwaterloo.ca/university/exambank"
"Q1 T/F Question: My question was asked at @223, but I still can&#39;t wrap my head around this T/F question. Wouldn&#39;t the perpendicular of a perpendicular just be the projection?","<p>Suppose we have $$\vec u, \vec v$$ where $$\vec u \cdot \vec v = 0$$, i.e. $$\vec u, \vec v$$ are orthogonal.</p>
<p></p>
<p>Then, $$\text{perp}_{\vec v}(\vec u) = \vec u$$ (draw out a diagram and check that this is true).</p>
<p></p>
<p>So, when you try to take the perpendicular of a vector onto an orthogonal vector, you get the same vector back.</p>
<p></p>
<p>Since $$\text{perp}_{\vec v}(\vec u)$$ is always orthogonal with $$\vec v$$, we can conclude that taking the perpendicular of $$\text{perp}_{\vec v}(\vec u)$$ onto $$\vec v$$ should return itself, i.e. </p>
<p></p>
<p>$$\text{perp}_{\vec v}(\text{perp}_{\vec v}(\vec u)) = \text{perp}_{\vec v}(\vec u)$$</p>"
"Recommended Problem Q6: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw91fw8w2x8%2Fa759d0aad70520d0cfce8f3da4f71c156082f8e457ba4d3a97f0771394fa9e64%2Fimage.png"" alt=""image.png"" />I&#39;m confused as to where Av=x part comes from, is it based off the Col(A) or is it just an observation?","<p>This is the definition of $$\text{Col}(A)$$:</p>
<p></p>
<p>$$\text{Col}(A) = \{\vec y \in \mathbb R^m : \exists\ \vec x \in \mathbb R^n, A \vec x = \vec y\}$$</p>"
"WP1 Q7: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8b86aib46q%2F1f836e5ef150f03df84aa0739320f52e3d63ecdbfb5af53fd127ba34342e8d49%2FScreenshot_2024-02-10_at_6.41.08_PM.png"" alt=""Screenshot_2024-02-10_at_6.41.08_PM.pngNaN"" width=""650"" height=""90"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8b86aib46q%2Fe6e5de64f277f2058e9757d24b78c1b1349e35212c930cc471138af20f7f8e54%2FScreenshot_2024-02-10_at_6.41.38_PM.png"" alt=""Screenshot_2024-02-10_at_6.41.38_PM.pngNaN"" width=""674"" height=""173"" /></p>
<p></p>
<p>In the last sentence from the solution, I don&#39;t understand where $$\vec{v1}$$ / $$\vec{u1}$$ *$$\vec{u1}$$ = $$\vec{v1}$$ is coming from. </p>
<p></p>
<p>Also, for &lt;= prove, can I say</p>
<p>: since $$\vec{u}$$ x $$\vec{v}$$ = $$\vec{0}$$, $$\vec{u}$$ = $$\vec{0}$$ or $$\vec{v}$$ = $$\vec{0}$$, so for each cases, it makes $$\vec{v}$$ parallel to $$\vec{u}$$ (= $$\vec{0}$$) and $$\vec{u}$$ parallel to $$\vec{v} (= $$\vec{0}$$) </p>","<p>$$\vec u \times \vec v = \vec 0$$ does not imply that $$\vec u = \vec 0$$ or $$\vec v = \vec 0$$ (consider if $$\vec u = \vec v$$).</p>
<p></p>
<p>The proof assumed that $$u_1 \neq 0$$, so remember that since $$u_1, v_1$$ are real valued, the $$u_1$$ is a real number, so</p>
<p></p>
<p>$$\frac{v_1}{u_1} u_1 = \frac{u_1}{u_1}v_1 = v_1$$</p>
<p>We&#39;ve shown that the components of v can be presented as a scalar multiple of a component of u.</p>
<p>We pretty much have<br />$$\begin{bmatrix} v_1 \\ v_2 \\v_3 \end{bmatrix}$$ = $$\begin{bmatrix} \frac{u_1}{v_1} u_1\\ \frac{u_1}{v_1} u_2 \\\frac{u_1}{v_1} u_3 \end{bmatrix}$$</p>
<p>the last line is equivalent to &#39;taking out&#39; $$\frac{u_1}{v_1}$$ out of u and writing the relationship in terms of the vectors $$\vec{u}, \vec{v}$$</p>
<p></p>
<p>2. nah. cross product being 0 vector doesn&#39;t mean one of the vectors is 0. <br />try and find cross product of &lt; 2 0 0 &gt; &lt; 4 0 0 &gt;</p>
<p>We&#39;ve shown that the components of v can be presented as a scalar multiple of a component of u.</p>
<p>We pretty much have<br />$$\begin{bmatrix} v_1 \\ v_2 \\v_3 \end{bmatrix}$$ = $$\begin{bmatrix} \frac{u_1}{v_1} u_1\\ \frac{u_1}{v_1} u_2 \\\frac{u_1}{v_1} u_3 \end{bmatrix}$$</p>
<p>the last line is equivalent to &#39;taking out&#39; $$\frac{u_1}{v_1}$$ out of u and writing the relationship in terms of the vectors $$\vec{u}, \vec{v}$$</p>
<p></p>
<p>2. nah. cross product being 0 vector doesn&#39;t mean one of the vectors is 0. </p>
<p>We&#39;ve shown that the components of v can be presented as a scalar multiple of a component of u.</p>
<p>We pretty much have<br />$$\begin{bmatrix} v_1 \\ v_2 \\v_3 \end{bmatrix}$$ = $$\begin{bmatrix} \frac{u_1}{v_1} u_1\\ \frac{u_1}{v_1} u_2 \\\frac{u_1}{v_1} u_3 \end{bmatrix}$$</p>
<p>the last line is equivalent to &#39;taking out&#39; $$\frac{u_1}{v_1}$$ out of u and writing the relationship in terms of the vectors $$\vec{u}, \vec{v}$$</p>
<p></p>
<p>2. nah. cross product being 0 vector doesn&#39;t mean one of the vectors is 0. try and find the cross product of </p>
<p>$$\begin{bmatrix} 2\\ 0\\0\end{bmatrix}$$ $$\begin{bmatrix} 4\\ 0\\0\end{bmatrix}$$ and see for yourself</p>"
"How did they come to the conclusion that s&#43;r(t-s) is a solution.: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2F03799af1e1039080af737a2240c0f37f8a6353ae824e826436eaa54cddf6d772%2FScreenshot_2024-02-10_at_7.00.50_PM.png"" width=""1480"" height=""634"" alt="""" />","<p>Write $$A\vec x = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$$ so that it represents the system.</p>
<p></p>
<p>$$\vec s, \vec t$$ solve the system, so</p>
<p></p>
<p>$$A \vec s = A \vec t$$. Then,</p>
<p></p>
<p>$$A(\vec s + r(\vec t - \vec s)) = A \vec s + r \underbrace{(A \vec t - A \vec s)}_{=0} = A \vec s$$</p>
<p>hence $$\vec s + r(\vec t - \vec s)$$ also solves the system for all $$r \in \mathbb R$$.</p>
<p></p>
<p>Note that since $$\vec s \neq \vec t$$, taking different $$r$$ parameters indeed yields different solutions. </p>"
"Practice Midterm Question 2a: <p>I saw that I had a different answer than the solution for question 2 on the practice midterm. Is my answer correct?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F28835746ef32a2f9e57e704723ba0fe08a865dcfacbbc0434b44a0f597d1c803%2Fimage.png"" alt=""image.png"" /></p>","<p>I believe the issue here is that you are using the points as vectors for your cross product. When using it to find a normal vector, cross product only applies to two, non parallel vectors which lie on the plane.</p>
<p>The strategy here is to get two vectors by subtracting our points. Think $$\vec{v}=A-B$$ and $$\vec{u}=C-B$$.</p>
<p>This will give you the vectors which link these points, which should then lie on the plane. Then just do exactly what you did above, take the cross product, then dot product the result with a point :)</p>
<p>The student answer is correct. $$A$$ and $$B$$ are on the plane, but the vectors corresponding to $$A$$ and $$B$$ don&#39;t have to be directional vectors of the plane. In fact, this happens if and only if the plane does not pass through the origin (think about why).</p>
<p></p>
<p>Thus, the normal you computed is not necessarily a normal for the plane.</p>"
"WP4 Q11: <p>how did they arrive to this conclusion a must commute with those specific matrixes?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8945sr63tu%2F83fbe42d3263992c73862268f98a5830488dc564ee84bbc24c816224c03b06e5%2Fimage.png"" alt=""image.pngNaN"" /> <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8945sr63tu%2F7759eca11aeed87ddf32df2b9b0f1d97327fb6d83ac529fe347b13095b5f3020%2Fimage.png"" alt=""image.pngNaN"" /></p>","<md>This is an assumption not conclusion.</md>
$$A$$ commutes with <em>every</em> $$2 \times 2$$ matrix, so it must commute with $$B_1$$ and $$B_2$$, which are both $$2 \times 2$$ matrices."
"Practice MT true and false (e): <p>Why isn&#39;t the span of these three in R3?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2Fbd6b5dc2c39c21648ca80d1e3f69746245d5b9540eefd91c1171ff4cf71d859b%2Fimage.png"" alt=""image.png"" /></p>","<md>w is not linearly independent of the other two vectors.</md>
<md>w is not linearly independent with the other two vectors.</md>
If you meant to ask &#34;why isn&#39;t the span of these three vectors <em>equal</em> to $$\mathbb R^3$$&#34;, then consider the vector $$\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$"
"WP4 Q7: <p>I understand everything in this question up to this step. How did we derive that &#34;vector x = vector y&#34; from Px = y?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwawbisf44qe%2F10809c74e16872bfadf9ff639fdf0bf8dba430fe45232ae5ac7b7ee65326f3c3%2Fimage.png"" alt=""image.pngNaN"" width=""595"" height=""112"" /></p>","They want to conclude that $$\vec y \in \text{Col}(P)$$, which means they need to find some $$\vec x$$ so that $$P \vec x = \vec y$$. $$\vec x = \vec y$$ is such a vector for which that statement holds."
"Finding a vector equation of a plane given the scalar equation: <p>Hello!</p>
<p></p>
<p>What is the best way for finding a vector equation of a plane given its scalar equation, especially for the planes that are not going through the origin?</p>
<p></p>
<p>For example, given $$3x-8y-2z=4$$.</p>
<p></p>
<p>Thank you!</p>","From this you know that [ 3 -8 -2 ] is the normal vector, then substitute values for x and y and calculate a point z, now with that you have a point that lies in the plane, from here on you can write your answer in the normal vector form of a plane. 
To find the vector equation of the plane you need 3 points on the plane that don&#39;t line on a line.  So by inspection you can choose 3 points on the plane.  Then take two of these three points to determine one direction vector and another two of these points to determine another direction vector.  Now you have 2 non-parallel direction vectors and a point on the plane (just choose one of the 3) and you can give the vector equation of the plane. "
"What does I_x mean: When doing the practice midterm, the first question involved I_2. What does I_x mean?","$$I_n$$ denotes the square $$n \times n$$ identity matrix: $$1$$s in the diagonal from top left to bottom right, and $$0$$s everywhere else."
"System rank theorem and homogenous matrices: <p>For $$A_{n\times n} \in \mathbb{R}$$ </p>
<p>If $$A\vec{x} = \vec{0} $$ for all $$x \in \mathbb{R}$$ why does the system rank theorem not hold?</p>",Can you explain why it fails to hold?
"practice midterm MC e: <p>How is this false? I was thinking that at least two vectors are needed to span a plane and a plane spans R3 therefore, it would be true.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8rpo1vu6yt%2F31420822f6ed4f0e1e79e896b13b8f607ea131a2414640eea46e902207940cf5%2FScreenshot_2024-02-11_at_7.03.11_AM.png"" alt=""Screenshot_2024-02-11_at_7.03.11_AM.pngNaN"" width=""883"" height=""118"" /></p>","You can pick 3 nonparallel vectors for a plane, and that would clearly only span a plane."
"textbook prood: <p>I&#39;m not sure I understand the backwards implication proof here. How does having the augmented matrix R with the standard basis imply that the system is inconsistent? And also how does doing &#34;reverse ERO&#34; get you back to the b vector?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8rpo1vu6yt%2Fd4552beddf80b3a24ad654c4bdeb8a37495ba661a7b65db8e727d15eabb59043%2FScreenshot_2024-02-11_at_7.45.40_AM.png"" alt=""Screenshot_2024-02-11_at_7.45.40_AM.pngNaN"" width=""736"" height=""285"" /></p>
<p></p>
<p></p>
<p>For more context, this is what being proven and the beginning of the proof  <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8rpo1vu6yt%2F41aaf877791f402ad1229424c367eaf0870cb1a4027047947b1e49244f0cb452%2FScreenshot_2024-02-11_at_7.47.03_AM.png"" alt=""Screenshot_2024-02-11_at_7.47.03_AM.pngNaN"" width=""862"" height=""513"" /></p>","<p>$$R$$ has at least one row of zeros at the bottom (the $$m$$-th row).  So the bottom row of $$[R|\vec{e}_m]$$ is of the form $$[0 \cdots 0|1]$$.  Therefore the system is inconsistent. </p>
<p></p>
<p>A series of EROs got us to the RREF.  For every ERO done, there is an ERO that &#34;undoes&#34; it.  If we do all the EROs that undo all the EROS that we did to get to the RREF, then we will get back to $$A$$ on the LHS and some vector $$\vec{b}$$ on the RHS. </p>"
purpose homogenous systems: what is the purpose of putting a system into a homogenous state? is it purely to get the nullspace? or are there other reasons and significant behind the solution set that is obtained?,"<p>The solution set to all systems of equations which have the coefficient matrix $$A$$ can be expressed in terms of the solution set to the homogeneous system with coefficient matrix $$A$$. </p>
<p></p>
<p>This week we will look at how the nullspace tells us something about the linear transformation that is determined by $$A$$. </p>"
"spanning sets: if you have a spanning set of 4 vectors does that imply that is spans R5? and to generalize that question more does having n vectors in a spanning set imply that that set spans R(n&#43;1), given that all the vectors in that set are not parallel?","<p>The number of vectors in a spanning set doesn&#39;t tell us anything.  For example,  if we add another vector to the spanning set which is a scalar multiple of the vectors in the spanning set, then we will have another spanning set for the same space, but it has more than one vector. </p>
<p></p>
<p>However, if none of these vectors are linear combinations of each other, then we will be able to say something more about the span of these vectors and it has something to do with the dimension of the space.  However, we haven&#39;t got to the formal idea of dimension yet.</p>"
"Ax = e =&gt; rank(A) = m proof: <p>I&#39;m not sure that I understand the proof after they say using the scalar and lineary of vector, matrix multiplication. Also what is the point of introducing xi and b? and adding to that why is showing that Ax = b is consistent relative to the solution when xi is said to be the solution?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8rpo1vu6yt%2F9f8e03cd76a0322d34b14975de01a4e2e949dfe93c57a78512eb7b6374d8fbb3%2FScreenshot_2024-02-11_at_8.06.13_AM.png"" alt=""Screenshot_2024-02-11_at_8.06.13_AM.pngNaN"" width=""528"" height=""332"" /></p>","<p>They are making use of part b) of the System-Rank Theorem which says that if $$A \vec{x} = \vec{b}$$ is always consistent no matter what $$\vec{b}$$ we choose, then the rank of $$A$$ is $$m$$. </p>
<p></p>
<p>So they show that the hypothesis of this statement leads to $$A \vec{x} = \vec{b}$$ always being consistent no matter what $$\vec{b}$$ is chosen. </p>
<p></p>
<p></p>"
matrix mutiplication: what does multiplying two matrices do the the nullspace and the column space? is the result anything specific?,That&#39;s a good question!  We haven&#39;t proved anything about this yet.  So you don&#39;t have any results about this.  But you might want to consider how the nullspace of $$A$$ compares to the nullspace of $$BA$$.    We will investigate ideas related to this later in the course. 
"what does O_n*n in a matrices mean: In question 6 of the sample midterm, the term O_n*n shows up, what does it mean?","<p>the zero matrix, every entry in the matrix is 0</p>
<p></p>
<p>and its also a square matrix </p>
the zero matrix, every entry in the matrix is 0
$$\mathcal O_{m \times n}$$ denotes the $$m \times n$$ zero matrix."
"Proof of Recommended Problems Q2a) &amp; Q2b): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwemr3flutc%2F0001e359b868cbf751d6b5cd81edec83bb71acd971a871de7b1c7ced435ec612%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwemr3flutc%2F1a3e919d6a0b8211656212752ada11a685f61ff20957bb1c17c979af77d079c5%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>For the proof in question  2b), by proving it true and let $$\vec{x} = \vec{e_i}$$ and $$\vec{y} = \vec{e_j}$$. Isn&#39;t there a loss of generality because it is no longer for all $$\vec{x}, \vec{y} $$? </p>
<p>Also couldn&#39;t we prove that 2b) is false using 2c), if we let $$\vec{x} = \vec{y} $$ and get a counterexample?</p>","Notice that we have a &#34;for all&#34; in the hypothesis.  (Remember MATH 135!)  If the hypothesis is true for all vectors  $$\vec{x}$$ and $$\vec{y}$$, then it will still be true when $$\vec{x}$$ and $$\vec{y}$$ are standard basis vectors. "
"False properties of matrix multiplication: Request for counterexamples: <p>Can someone provide counterexamples to these?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F371934ff41116160c5632130a0bbcd69fbc38434333515e88e684f9f8307584b%2FScreenshot_2024-02-11_at_11.12.18_AM.png"" alt=""Screenshot_2024-02-11_at_11.12.18_AM.pngNaN"" /></p>","This doesn&#39;t answer you question exactly but try watching my YouTube video about some similar results and then you may be able to find your own counterexample.   <a href=""https://youtu.be/Af-kJUs4atQ?si=RsUaBoWEzjxmWU8H"" target=""_blank"" rel=""noopener noreferrer"">https://youtu.be/Af-kJUs4atQ?si=RsUaBoWEzjxmWU8H</a> 
Example 4.3.12 in the course notes shows some counterexamples!<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd85z5pt61k%2Ff36c542f5b3ed18a924fc7af115d631f6f18d57dc44c2cde79febe752d1901f7%2FScreenshot_2024-02-11_at_1.29.57_PM.png"" alt=""Screenshot_2024-02-11_at_1.29.57_PM.pngNaN"" />"
"Theorem 3.11.6: (Solutions to 𝐴 #»𝑥 = #»<br />0 and 𝐴 #»𝑥 = #»<br />𝑏 )<br />Let 𝐴 #»𝑥 = #»<br />𝑏 , where #»<br />𝑏 ̸ = #»<br />0 , be a consistent non-homogeneous system of linear equations<br />with solution set ̃𝑆. Let 𝐴 #»𝑥 = #»<br />0 be the associated homogeneous system with solution set<br />𝑆. If #»𝑥𝑝 ∈ ̃𝑆, then ̃𝑆 = { #»𝑥𝑝 &#43; #»𝑥 : #»𝑥 ∈ 𝑆}. Is it always true that a particular solution is the origin of the general solution?","<p>&gt;&gt;Is it always true that a particular solution is the origin of the general solution?</p>
<p></p>
<p>I don&#39;t understand your question. What general solution are you referring to? Are you asking whether $$\vec{0}$$ is in the general solution to $$A\vec{x}=\vec{b}$$ or to $$A\vec{x}=\vec{0}$$? In thie theorem, we are talking about a particular solution to $$A\vec{x}=\vec{b}$$. In other words, any solution to $$A\vec{x}=\vec{b}$$ is a particular solution. </p>
<p></p>
<p>&gt;&gt;Is it always true that a particular solution is the origin of the general solution?</p>
<p></p>
<p>I don&#39;t understand your question. What general solution are you referring to? Are you asking whether $$\vec{0}$$ is in the general solution to $$A\vec{x}=\vec{b}$$ or to $$A\vec{x}=\vec{0}$$?</p>
<p></p>
<p>Here, it is a particular solution to $$A\vec{x}=\vec{b}$$. In other words, any solution to $$A\vec{x}=\vec{b}$$ is a particular solution. </p>
<p></p>"
"Math 136 review session question: <p>isnt this supposed to be 6&#43;2i? Also, will we be tested on this?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F9860521dbc2ab329f0379c03c2dbf83975798492f21f040f65792115dc98cd05%2Fimage.png"" alt=""image.png"" /></p>
<p></p>","We did not cover the complex inner product this year.  (It had been covered in the past.)  It is different than the dot product. 
We did not cover the complex inner product this year.  (It had been covered in the past.)"
"WP4 Recommended Problems 8a): <p>Hello,</p>
<p></p>
<p>I was just wondering why when solving this problem and taking $$(A^2)^T$$, why do we say that this is equal to $$(A^T)^TA^T$$ as opposed to $$A^T(A^T)^T$$, as I know that the order is important? When transposing a product, do we first reverse the order within the brackets, then transpose each term?</p>
<p></p>
<p>Any clarification that you can provide would be greatly appreciated. Thank you!</p>","<md>I think the correct formula should be: $$(A^2)^T = A^TA^T$$? But in general, it should be $$(AB)^T = B^TA^T$$. The order is important because the size matters for matrix products.</md>
<md>I think the correct formula should be: $$(A^2)^T = A^TA^T$$? But in general, it should be $$(AB)^T = B^TA^T$$? The order is important because the size matters for matrix products.</md>
<md>I think the formula should be: $$(A^2)^T = A^TA^T$$? But in general, it should be $$(AB)^T = B^TA^T$$? The order is important because the size matters for matrix products.</md>
$$(A^2)^\top = (AA)^\top = A^\top A^\top$$
$$(A^2)^\top = (AA)^\top = A^\top A$$"
"Question about column and row space: <p>If A ~~~ B by ERO&#39;s, then it is true that:</p>
<p><br />Row(A) = Row(B)<br />Col(A^T) = Row(B)</p>
<p>Col(B^T) = Row(A)<br /><br /></p>
<p>But is it <strong>always</strong> true that</p>
<p>Col(A) ≠ Col(B) ?<br /><br /></p>","<p>&gt;&gt;But is it <strong>always</strong> true that Col(A) ≠ Col(B) ?</p>
<p>This is not necessarily true.</p>
<p></p>
<p>If A=B, then clearly A, B are row equivalent and Col(A)=Col(B).</p>"
Where can I find more proof problems besides sample midterm and WP?: ,"I’m sure there’s a good number of other resources, but one is Professor Bauman’s YouTube channel has some great relavent worked through proofs and problems.<div>Here’s a link: <a href=""https://youtube.com/@ShaneBaumanUWaterloo?si=omC35F8N5xGIjHau"">https://youtube.com/@ShaneBaumanUWaterloo</a></div>
I’m sure there’s a good number of other resources, but one is Professor Bauman’s YouTube channel has some great relavent worked through proofs and problems. Here’s a link <a href=""https://youtube.com/@ShaneBaumanUWaterloo?si=omC35F8N5xGIjHau"">https://youtube.com/@ShaneBaumanUWaterloo</a>
I’m sure there’s a good number of other resources but Professor Bauman’s YouTube channel has some great relavent worked through proofs and problems. Here’s a link <a href=""https://youtube.com/@ShaneBaumanUWaterloo?si=omC35F8N5xGIjHau"">https://youtube.com/@ShaneBaumanUWaterloo</a>"
Complex pivot: <md>How to convert a complex pivot into 1 and the other non-pivot positions into 0? Also will the questions on the midterm involve any complex values?</md>,"<p>If you have a complex pivot $$a+bi$$ (hence nonzero), to normalize it you may multiply the row by $$\frac{1}{a+bi}=\frac{a-bi}{a^2+b^2}$$. </p>
<p></p>
<p>&gt;&gt;Also will the questions on the midterm involve any complex values?</p>
<p>Probably. The numbers on the test will be simple enough for you to complete within 110mins. </p>"
Do homogenous system solutions always have parameters?: Just wondering,No. Consider a homogenous system of an identity square matrix.
"How to make use of square matrix in proof: <p>Hello,</p>
<p></p>
<p>I noticed that in a lot of proofs we are given that the matrix we are working with is nxn, or square. I often feel like this is included for a reason (as opposed to a regular mxn) but I never know how to make use of this in proofs.</p>
<p></p>
<p>I was wondering if there are any noteworthy properties or theorems that you can apply specifically to square matrices to help in proofs.</p>
<p></p>
<p>Thank you!</p>","You can multiply square matrices in any order and that will be valid, whereas if you have not square matrices, you need to pay attention to the order of multiplication.<div><br /></div><div>This is relevant in repeated multiplication (hence why $$A^2$$ is only defined for square matrices).</div><div><br /></div><div>Also, transpose of square matrices is also square, which may be handy when you may need to write $$(AB)^\top = B^\top A^\top$$.</div>
You can multiply square matrices in any order and that will be valid, whereas if you have no square matrices, you need to pay attention to the order of multiplication.<div><br /></div><div>This is relevant in repeated multiplication (hence why $$A^2$$ is only defined for square matrices).</div><div><br /></div><div>Also, transpose of square matrices is also square, which may be handy when you may need to write $$(AB)^\top = B^\top A^\top$$.</div>"
"WA1 Q3b: <p>For the first written assignment Q3b, how did the solutions get the circled red part <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehgz4527lv%2Fef255f51f4cd0d08df63daf6e4bf627d4c2896bd052c9f5c95af6b214abe0482%2Fimage.png"" alt=""image.png"" />?</p>
<p></p>","That is the same vector as taking the perpendicular of $$\vec w$$ onto $$\vec u$$, and then onto $$\vec v$$ (you should verify this is true).
That is the same vector as taking the perpendicular of $$\vec w$$ onto $$\vec u$$, and then onto $$\vec v$$ (you should veri du this is true)."
WP4 warm-up Q7: I&#39;m a little bit confused about what are some methods that could be used to solve this question or is there a trick to do this ,Can you find an easy way to express $$\vec{b}$$ as a linear combination of the columns of $$A$$? 
"RREF transformation marking scheme: <p>For the midterm, do we always have to write down the specific elementary row operations we do for each step of the solution? For exmaple, always write Rm -&gt; Rm-Rn, Rm -&gt; 1/Rm, etc.</p>
<p></p>
<p>Thanks! </p>","For the midterm, yes.   For the final, I would say no.  But maybe some of the instructors would disagree with me. "
"Practice Question: <p>Would this be a valid answer? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdwgzo172ji%2Faf28e687a863e3ca8549406cf1cdbafb6f1b377bd6c7124ed4e2a7266c2343fc%2FIMG_2886.heic"" alt=""IMG_2886.heicNaN"" /></p>","<p>I don&#39;t think we are allowed to make the assumption that if $$A\vec{v}=0$$, then either $$A$$ or $$\vec{v} = 0$$. There&#39;s a section in the course notes:</p>
<p><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwca7ky8m6lh%2Fd1047d15df16cc3d871580ffbbb2f6b926c30949c4ecc8dc840577e82ff70120%2Fimage.png"" alt=""image.pngNaN"" width=""641"" height=""166"" /></p>
<p></p>
<p>Property 2 (I think you used this one), does not apply to matrix arithmetic, and since a vector is just a nx1 matrix, this won&#39;t apply to matrix-vector multiplication either. So I don&#39;t think the argument works.</p>"
"REF: For the final row in REF, it doesn&#39;t always have to be all zero entries, right?","Correct, but if there are any 0 rows, they must be on the bottom. If there are no 0 rows then that is also fine."
"Linearly Independent: What does it mean for a set to be linearly independent, and is that something we could be tested on?","That won’t be tested. It means that no vector in the set can be represented as a linear combination of the other vectors, or, alternatively, the zero vector admits a unique representation as a linear combination of this vectors."
"General Question About proofs: If i have a proof of lets say span{v1, v2, v3} = R^3. Is it feasible to show for the backward direction that all the standard basis vectors, ie e1 e2 and e3 are a subset of the span so R^3 is a subset of the span?","I’m not sure what you mean by the “backward direction”, but you can use the result of WA1 q2a on the midterm."
"About subspace: I understood like.. if the vector solution set of linear equation system is the subspace of R^n, that vector solution set always goes through the origin, and always satisfies the homogenous equation. Is this right?  (a little bit confusing in this concept so my expression might be wrong. thank you for any feedback)",We haven&#39;t formally defined what a subspace is (and at this point you aren&#39;t required to know what it is.)  You will see later that a subspace will always include the zero vector and so if a solution set is a subspace it must include the zero vector.  That means that the system must be a homogeneous system.
"Computing column and row space:: <p>Hello,</p>
<p></p>
<p>In W4 Q4, we are asked for null, column, and row space. In the solutions, they basically just rip out the columns/rows and turn them into spans as the answer:<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwca7ky8m6lh%2Febe97bdcad38cabe71b46472c72c7a2dea13c5112498e92f142306cd6bc83398%2Fimage.png"" alt=""image.pngNaN"" width=""578"" height=""320"" /></p>
<p>However my prof had told us in class that this is a bad answer (especially for column space), and when we did something similar in class we took the transpose of the matrix and computed the row space to get the column space of the original.</p>
<p><br />On a test which method should we use for situations where we need column/row space? I wouldn&#39;t want to take time reducing the transposed matrix into RREF if I didn&#39;t have to.</p>
<p></p>
<p>Thanks!</p>","If your answer is correct and well-justified using material that we covered, then you will earn full marks. Some people have a preference of which methods they prefer."
"column space and matrix equality: <p>Does this also mean that two matrices are equal if and only if they have the same column space?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8rpo1vu6yt%2F509b7b9d56bef32b58bf004d7868cb872f61a739db93202f87d546101173d195%2FScreenshot_2024-02-11_at_5.37.30_PM.png"" alt=""Screenshot_2024-02-11_at_5.37.30_PM.pngNaN"" width=""835"" height=""78"" /></p>",No.<div><br /></div><div>Column space just means the set of things that can be represented as a linear combination of the columns. The same vector in both column spaces doesn’t have to take the same linear combination.</div>
"W4PP Recommended Problems Q2b: The solution for this question uses the fact that x and y can be arbitrary vectors in Rn, and lets x =ei and y=ej. Does that mean that the statement would be true even if the statement said there exists x and y instead of for all?","<p>No, at least not by the logic of the argument.</p>
<p></p>
<p>If something is true for all $$x, y$$, then you get to pick any $$x, y$$ and the statement is still true.</p>
<p></p>
<p>If something is true because there exists $$x, y$$ so that the statement is true, then you get to pick some $$x, y$$ to satisfy it, but you don&#39;t get to know what $$x, y$$ look like.</p>
<p></p>
<p>In this case, you would not know if the statement is necessarily true for $$x = e_i, y = e_j$$.</p>"
"Clarification of System Rank Theorem: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdh2s3927me%2F9bbdabf1f5d73df9e32d1dee5066a863f3b6da3f9bb7b4f8bece233c211cdf75%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>For part (b): suppose we have a matrix whose $$r = m$$ (and is therefore consistent). Now suppose that we add a zero row to the matrix, and the corresponding $$b_{m+1}=0$$.</p>
<p></p>
<p>1. Then $$r\neq m$$, which means that the matrix is not consistent. But wouldn&#39;t both matrices technically yield the same solutions, since we&#39;re simply adding a row with all zeros?</p>
<p></p>
<p>2. Also, if given a matrix whose last row is all zeros (including the corresponding $$b_n$$), can we simply remove it from the matrix, because it doesn&#39;t give any additional information for solving for the solution?</p>","<p>1. Adding a zero row to the coefficent matrix, say at the bottom, and setting the corresponding b to be 0 doesn&#39;t change the ranks of A and [A |b]. So r=m and it is consistent.</p>
<p></p>
<p>Consider $$\left[\begin{array}{ccc|c} \ast & \ast & \ast &\ast\\ 0 & \ast & \ast &\ast\\ 0 & 0 & \ast &\ast\\ \end{array}\right]$$ and  $$\left[\begin{array}{ccc|c} \ast & \ast & \ast &\ast\\ 0 & \ast & \ast &\ast\\ 0 & 0 & \ast &\ast\\ 0 & 0 & 0 & 0 \\ \end{array}\right]$$.</p>
<p></p>
<p>2. Right, a zero row doesn&#39;t affect the solution of a linear system. It will autometically disappear when you convert it to linear equations.</p>
<p></p>
<p>However, only speaking of matrice, you cannot remove a zero row,  because</p>
<p>$$\left[\begin{array}{ccc|c} \ast & \ast & \ast &\ast\\ 0 & \ast & \ast &\ast\\ 0 & 0 & \ast &\ast\\ \end{array}\right]$$ and  $$\left[\begin{array}{ccc|c} \ast & \ast & \ast &\ast\\ 0 & \ast & \ast &\ast\\ 0 & 0 & \ast &\ast\\ 0 & 0 & 0 & 0 \\ \end{array}\right]$$</p>
<p>are completely different matrices (different size, one has full row rank and the other doesn&#39;t). They are equivalent, when you are using them to solve system of linear equations.</p>
<p></p>
<p>In short, you can remove redundant equations, but you cannot remove zero rows in a matrix.</p>
<p></p>
<p>1. Adding a zero row to the coefficent matrix, say at the bottom, and setting the corresponding b to be 0 doesn&#39;t change the ranks of A and [A |b]. So r=m and it is consistent.</p>
<p></p>
<p>Consider $$\left[\begin{array}{ccc|c}
\ast & \ast & \ast &\ast\\
0 & \ast & \ast &\ast\\
0 & 0 & \ast &\ast\\
\end{array}\right]$$ and  $$\left[\begin{array}{ccc|c}
\ast & \ast & \ast &\ast\\
0 & \ast & \ast &\ast\\
0 & 0 & \ast &\ast\\
0 & 0 & 0 & 0 \\
\end{array}\right]$$.</p>
<p></p>
<p>2. Right, a zero row doesn&#39;t affect the solution of a linear system. It will autometically disappear when you convert it to linear equations.</p>
<p></p>
<p>However, only speaking of matrice, you cannot remove a zero row,  because</p>
<p>$$\left[\begin{array}{ccc|c}
\ast & \ast & \ast &\ast\\
0 & \ast & \ast &\ast\\
0 & 0 & \ast &\ast\\
\end{array}\right]$$ and  $$\left[\begin{array}{ccc|c}
\ast & \ast & \ast &\ast\\
0 & \ast & \ast &\ast\\
0 & 0 & \ast &\ast\\
0 & 0 & 0 & 0 \\
\end{array}\right]$$</p>
<p>are completely different matrices (different size, one has full row rank and the other doesn&#39;t). They are equivalent, when you are using them to solve system of linear equations.</p>
<p></p>"
"Q3, why do we not consider x &#43; y = -3: ","<p>Where is $$-3$$ coming from?</p>
<p></p>
<p>$$\sqrt{36} = 6$$, unambiguously (i.e. $$\sqrt{36} \neq -6$$).</p>"
"can rowspace be thought of in this way?: <p>As column space of A is thought of as the set of all matrix-vector multiplications for all real vectors (col(A) = {Ax: x is in the F}), can row space be thought of as the set of all matrix-vector multiplications of the transposition of A and any vector in the F? - (</p>
<p>Row(A) = {A^T x: x is in the F})</p>","Yes, if you mean $$F = \mathbb R^n$$ for the first one, and $$F = \mathbb R^m$$ for the second one.
Yes, if you mean $$F = \mathbb R^n$$."
"Can we &#39;choose&#39; which order we want when multiplying a matrix with both sides of an equation?: <p>Hello,</p>
<p></p>
<p>In W4 Q4 we have to multiply both sides of an equation by $$A^{T}$$ to get our solution:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwca7ky8m6lh%2F9ccfe5e2e5fd6c26442fa23df57b6b058064f415c0378250c621889e4fd8ed94%2Fimage.png"" alt=""image.pngNaN"" width=""1285"" height=""180"" /></p>
<p></p>
<p>However I know that matrix multiplication does not necessarily commute, so in scenarios like this do we get to &#39;choose&#39; which side the $$A^{T}$$ ends up on?</p>
<p></p>
<p>For example if we have that a matrix $$B\vec{x}=\vec{c}$$ , when multiplying both sides by $$A^{T}$$ could we choose if we want<br /><br /></p>
<p>$$A^{T}B\vec{x}=A^{T}\vec{c}$$</p>
<p>or</p>
<p>$$B\vec{x}A^{T}=\vec{c}A^{T}$$</p>","If the dimensions line up, you can do whatever you want, but is that result useful for proving the theorem?"
"Q7 on Warmup Exercises: <p>Without doing any row reduction determine a vector #»x ∈ R 4 such that A #»x = #»b . (a) A =   1 2 3 4 5 6 7 8 9 10 11 12  , #»b =   2 6 10  . (b) A =   1 2 3 4 5 6 7 8 9 10 11 12  , #»b =   6 14 22  .</p>
<p>The only method I know is RREF, is there other way to solve other than RREF?</p>","You can use what you know about the standard basis and column extraction.
<p>The student answer is right; by definition, if $$A\vec x = \vec b$$ is consistent, then $$\vec b \in \text{Col}(A)$$, so there is some $$\vec x = \begin{bmatrix} x_1 \\ \vdots \\ x_n\end{bmatrix}$$ so that</p>
<p></p>
<p>$$\vec b = x_1 A_1 + \cdots + x_n A_n$$ where $$A_i$$ is the $$i$$-th column of $$A$$.</p>
<p></p>
<p>From here, you can see that $$\vec b$$ is exactly one of the columns of $$A$$, so its not hard to identify which values of $$x_i$$ you want.</p>"
"Identity Matrix: Can identity matrix be used as a substitution for RREF when solving Ax=b? Does identity matrix make the calculation more simpler, which is why sometimes this method is used to tackle matrix-vector multiplication or any other matrix-matrix multiplication?","What is this method?
<md>Perhaps you are talking about invertibility?</md>"
"Forms of solutions sets: <p><a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl66pr4f64oe3ti%2Fe4a03e950281db851ddad9cc38d486202f74ea0f35f47b11fbd47fd637bd037c%2FScreenshot_2024-02-11_at_9.42.54_PM.png"" target=""_blank"" rel=""noopener noreferrer"">Screenshot_2024-02-11_at_9.42.54_PM.png</a></p>
<p></p>
<p>Regarding the image above, are the top and bottom expressions considered the same? (With the exception of variables) If there were ever a question that asks for a solution set, which form would be preferable?</p>",Either way of expressing the solution set is fine.  I personally prefer the first because it makes it clearer that the solution set is a plane. 
"Practice 2-4: <p>Hi would the following be true or false and why? Thanks.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb4wzraz68t%2F5cd973906c3875e42e3641739896f5d0e4cfa6454e41079b8f899305856e9de0%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>Perhaps you meant to write $$\mathbb R^n = \text{Span}\{\vec v_1, \ldots, \vec v_k\}$$?</p>
<p></p>
<p>If so, the result is false. Consider $$\vec v_i = \vec e_i$$ for each $$1 \le i \le k$$, then define $$\vec v_{n + 1} = \vec e_1 + \vec e_2$$.</p>
<p></p>
<p>In general, you can almost always add arbitrary vectors into a span and maintain the span (simply by picking a vector in that span).</p>
<p></p>
<p>The idea you are closely describing has to do with the dimension of a vector space, which is the size of a smallest spanning set (we will cover more on this in following weeks).</p>"
"formula sheet for the midterm?: <p>Just to confirm will we be given a reference sheet (a list of all definitions/theorems) as we did in math 135?</p>
<p></p>","No, there will be no formula sheet."
"Question about Q4 proof: To prove this, it is better to use the approach given in the solution or would it be better to let a=c-dt and d=b at the start, then work the other way.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbycwrkt4fz%2F121ac160ea14b1c4afa28db23da8d2ab753b69439bed6ef4fd57d2b19188ab12%2FScreenshot_2024-02-11_at_10.26.56_PM.png"" alt=""Screenshot_2024-02-11_at_10.26.56_PM.pngNaN"" />","You don&#39;t get to pick $$a, b$$. You are given an arbitrary $$\vec x = a \vec u + b \vec v$$, which means you don&#39;t know the values of $$a, b$$, so you can&#39;t set them to anything."
"reference to theorems/definitions: On the midterm, do we need to refer to all the relevant thoerems/definitions in the proof to get full mark? thanks! ","<p>You should cite large, named theorems or results from assignments or quizzes.</p>
<p></p>
<p>In general, your work should be easy to follow, so aim for that standard (don&#39;t jump to the next step after using a big theorem and not citing it, as the reader won&#39;t know how you got to that conclusion).</p>"
"Is this question true or false?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2F815ecd797a2a9ca585deb40a9130169984af46b1f559dd64d37841a2c098e2bd%2Fimage.png"" alt=""image.png"" /></p>
<p>Is this question true or false?</p>","<p>Suppose you had a system $$A \vec x = \vec b$$, $$A \neq \mathcal O$$, where the solution set has at least $$1$$ free variable. Then, we could pick $$\vec u, \vec v \in \mathbb R^n$$ so that $$\vec u \neq \vec v$$ and $$A \vec u = A \vec v$$.</p>
<p></p>
<p>Can you conclude from this?</p>
<p></p>
<p>Hint: vectors in $$\mathbb R^m$$ are just $$m \times 1$$ sized matrices.</p>
<p>Suppose you had a system $$A \vec x = \vec b$$, $$A \neq \mathcal O$$, where the solution set has at least $$1$$ free parameter. Then, we could pick $$\vec u, \vec v \in \mathbb R^n$$ so that $$\vec u \neq \vec v$$ and $$A \vec u = A \vec v$$.</p>
<p></p>
<p>Can you conclude from this?</p>
<p></p>
<p>Hint: vectors in $$\mathbb R^m$$ are just $$m \times 1$$ sized matrices.</p>"
"Counter example: May someone provide a counter example for this question?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fqdedchjgayvq%2F4d91fb74851627cadcc9d38b09bbffdf_2024_02_11.jpeg"" /><p></p>",@400
"Week 4 Q 4: <p>since when is u T #»v = #»u · #»</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc7rqqei65o%2Fa9550147c9985cc2e28c09c06743660a4c6c7214d34feffdb5853b10810d2e39%2FScreenshot_2024-02-12_at_12.00.41_AM.png"" width=""2880"" height=""1800"" alt="""" /></p>",@350
scalar equation: what is the main/general use for a scalar equation?,"<p>Recall that a scalar equation is specific to $$\mathbb R^3$$, and not $$\mathbb R^n$$, and takes the form $$ax + by + cz = d$$.</p>
<p><br />You can write it as $$ax + by + c(z - d/c) = 0$$ or for whichever of $$a, b, c$$ are nonzero. the $$z - d/c$$ corresponds to shifting the plane by moving $$(0, 0, 0)$$ to $$(0, 0, d/c)$$ (and same for every point on the plane).</p>
<p></p>
<p>If we think about planes of the form $$ax + by + cz = 0$$ for a moment, we can clearly see that $$(a, b, c)$$ is a normal of the plane because all points $$(x, y, z)$$ make $$0$$ by dot product with $$(a, b, c)$$. Then, $$ax + by + c(z - d/c) = 0$$ represents a shift in the plane, so the same vector $$(a, b, c)$$ is also a normal for this plane.</p>
<p></p>
<p>Note that this is all just geometric intuition that is not rigorous, but can be made rigorous through the use of algebra. You should not attempt to use any reasoning akin to the above on the midterm; you should always justify your answers with algebra and theorems.</p>"
"Week 4 Q4: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwarf9r4h3w4%2F2091f5d5b029fdee3b39b4e26b786510efc398eb586932ad7a180fc1fe84349f%2FScreenshot_2024-02-12_at_7.49.29_AM.png"" alt=""Screenshot_2024-02-12_at_7.49.29_AM.png"" width=""233"" height=""135"" /></p>
<p>Why can we get rid of the last vector in this simplification? Thank you!</p>","<p>you can use the first two vectors and express them as a linear combination of the last one and having it in there is redundant.</p>
<p></p>
<p>(2 times the first minus the second)</p>"
"q5 (f): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fae6f4dde89ee34e3f4f8bbd1fb9c587c868d85c85921857de3dba72f5642c585%2F34211707748173_.pic.jpg"" alt=""34211707748173_.pic.jpgNaN"" /></p>
<p>why not a matrix like [0 1 | 7]</p>
<p>                                    [1 0 | 8]</p>","Well, in this case the solution set is $$S$$ is just one vector, while in the question, the solution set is all vector with n and m integer."
"Q6: <p>Could someone explain this step?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2F93919ec90912b941407753cc533026453f6ce7fbb2d1f359dc3422e4a61ea366%2FIMG_6074.jpg"" width=""719"" height=""120"" alt="""" /></p>","The steps involved in this conclusion are presented in some proofs so far. The idea consists of considering $$\vec{x} = \vec{e}_i$$, for $$i = 1, 2, \ldots, n$$, since all vectors should be solutions for this system. Then, work with the definition of matrix-vectors multiplication that involves the columns of $$A$$ to understand why $$A$$ must be the zero matrix."
"practice midterm Q8: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwca7ky8m6lh%2Fc543f280d51b7576d7e5ba480251d82d0de3f6ca00af4b779507d69513a6739a%2Fimage.png"" alt=""image.pngNaN"" width=""1171"" height=""164"" /></p>
<p>I saw on some other posts that this question can be made easier by using the result from WA1 q2, but im not sure how. What would the strategy be to write all vectors in R3 as a linear combination of u, u&#43;v, u&#43;v&#43;w ?</p>
<p></p>
<p>Thank you!</p>","Using WA1 Q2, if you can show that u,v,w are vectors in span{u, u&#43;v, u&#43;v&#43;w}, then span{u,v,w} $$\subseteq$$ span{u, u&#43;v, u&#43;v&#43;w}. But we are assuming that span{u,v,w} = $$\mathbb{R}^3$$ and span{u,v,w} $$\subseteq \mathbb{R}^3$$, which would make u, u&#43;v, u&#43;v&#43;w a spanning set."
"Week 4 Q1c): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwarf9r4h3w4%2F7986e69a10f8b5ef758ec362dee75fdf3cee5620a645afa4e48982cabafeb6e9%2FScreenshot_2024-02-12_at_11.31.39_AM.png"" alt=""Screenshot_2024-02-12_at_11.31.39_AM.pngNaN"" width=""618"" height=""169"" /></p>
<p></p>
<p>Why is xp [1 1 1]^t here? Shouldn&#39;t it be [12 12 -11] ?</p>","Why did suggest [12 12 -11]^T?<br /><br />On (b), we have that [1 1 1]^T is a solution, that is why it appears on $$S_2$$. The idea here is that we can determine the solution set of the non-homogeneous system from the homogeneous one if we also have a particular solution, which in this case is [1 1 1]^T."
"Midterm Short Term Absence: Just to confirm, if I declare a 48 hour short term absence today, will the midterm&#39;s weighting transfer over to the final? Will the new assessment structure be 10% Written Assignments, 15% Quizzes, and 75% Final?",That is correct
"One dimensional vectors/matrices: <p>Hello! </p>
<p></p>
<p>What is the actual meaning of a $$1 \times 1$$ matrix, e.g. $$[5]$$?</p>
<p></p>
<p>I suppose that they are different from scalars... In addition, is $$[0]$$ the same as $$\vec 0$$?</p>","<p>Its meaning is no different from that of a $$m \times n$$ matrix. That is, it is an array of 1 row and 1 column.</p>
<p></p>
<p>In that sense, $$[0] = \vec{0}$$.</p>
<p></p>
<p>However, in practical terms, you will observe that operating with such matrices, when well defined, the result is effectively the same as multiplying by a scalar.</p>"
"Grades shown on Learn: Why does my learn score for MATH 136 look like this?<br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd5wc37w5nc%2F011b0b9a201996bd6c11ab69b01fe4a292bc22acc975d5c6e47db41169a20234%2Fimage.png"" alt=""image.pngNaN"" />",We wait until we are done with regrade requests and exemptions to sync the grades with LEARN. But I will go on sync the grades for Q1.
"What does it mean if a span of vectors = a field?: <p>If you have a $$Span\{v_1,v_2,...,v_m\} = \mathbb{R}^4$$, does this mean that at least 4 vectors in the spanning set are all orthogonal to each other?</p>
<p></p>
<p>Edit: corrected typo (from $$\in$$ to $$=$$).</p>","<p>I&#39;m not sure what you mean</p>
<p></p>
<p>$$\text{Span}\{\vec v_1, \ldots, \vec v_m\}$$ and $$\mathbb R^4$$ are both sets of vectors, so perhaps you meant to write $$\text{Span}\{\vec v_1, \ldots, \vec v_m\} =\mathbb R^4$$?</p>
<p></p>
<p>It does not have to be the case that there are 4 vectors $$\vec v_i, \vec v_j, \vec v_k, \vec v_\ell$$ which are all pairwise orthogonal. For instance,</p>
<p></p>
<p>$$\text{span}\left\{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}\right\} = \mathbb R^4$$.</p>
<p></p>
<p></p>"
"homogenous solution set: <p>When a homogenous system has a non-trivial solution does that it always mean there are infinitely many solutions to the system?</p>
<p></p>
<p></p>","Yes, that&#39;s right! I am eager to see how you prove it."
"practice 4 - q4 How can it be possible?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2Fd6eb7db807b67c879cfa62c36fd16dcab0b64c2cdf0988ee1786cbfb9d2cb97c%2Fimage.png"" alt=""image.pngNaN"" />",@350. You can expand: $$u^\top v$$ is matrix multiplication ($$1 \times n$$ by $$n \times 1$$).
"WP4 Q7 - Properties of Matrix Multiplication: <p>I came up with a similar solution to Q7 from the Week 4 recommended problems, however I am not sure if I used this concept correctly: </p>
<p></p>
<p>Is it true that properties of matrix multiplication can also apply to matrix vector multiplication? Namely, I noticed that we have the following properties: </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2Fa1c6f423853214466723629733068b0913af8dd3ca45053df4b05c10ab47db2c%2Fimage.png"" alt=""image.png"" /></p>
<p>I used a) except C was a vector, which I assume is allowed if you let p=1. A similar property was used in the posted solution to this question. I remember talking about this in lecture before but I just wanted to confirm before the midterm. </p>","Yes, that is correct.<br /><br />Vectors in $$\mathbb R^n$$ are just $$n \times 1$$ sized matrices."
sets and lines and planes: Given a spanning set how would we determine if the spanning set represents a line or a plane?,"<p>This topic is more closely related to linear independence, and we won&#39;t be asking you to determine if a given span of vectors represents a line or a plane.</p>
<p></p>
<p>The method for doing so is to remove &#34;unnecessary&#34; vectors from the span: e.g. if $$\text{span}\{\vec v_1, \vec v_2\} = \text{span}\{\vec v_1\}$$, then we can remove $$\vec v_2$$.</p>
<p></p>
<p>If there is just one vector left, then the span is a line. If there are two, it is a plane.</p>"
"w4pp Q9: What does the highlighted part mean?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Fa61a9b40330146967d9bffc9454718ec5134a1c355d230eca00e626f5c14514e%2FScreenshot_2024-02-12_at_2.50.39_PM.png"" alt=""Screenshot_2024-02-12_at_2.50.39_PM.pngNaN"" />",$$i > j$$ means the lower triangle of the matrix (i.e. the stuff below the diagonal).
"Prove SpanA = SpanB: <p>Is my solution for this question correct?</p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2Feeb4f0c67ba3adf018f4e25abf4913fe1d88a7903ff8c89bfe316c6967d2cd02%2Fimage.jpg"" target=""_blank"" rel=""noopener noreferrer"">image.jpg</a></p>","<p>I didn&#39;t check through your work, but it seems fine.</p>
<p></p>
<p>An easier way to see this result is to use WA1 q2a, and show that each vector in $$A$$ is a linear combination of vectors in $$B$$, and each vector in $$B$$ is a linear combination of vectors in $$A$$ (by inspection I can already see this is true, so it should not be hard to verify).</p>
<p>I didn&#39;t check through your work, but it seems fine.</p>
<p></p>
<p>An easier way to see this result is to use WA1 q2a, and show that each vector in $$A$$ is a linear combination of vectors in $$B$$, and each vector in $$B$$ is a linear combination of vectors in $$A$$ (by inspection I can already see this is true).</p>"
"Notation problem: Hi, I am wondering whether it is correct or not to write something like this: Null([A|$$\bar{b}$$])","No, the null space only refers to matrices, not systems."
"Practice Midterm Q3: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwarf9r4h3w4%2Ff103ce62055248e65cd475e0ca40a9a4ff48d61f5470fcaefd92fe19fdec5f86%2FScreenshot_2024-02-12_at_3.58.09_PM.png"" alt=""Screenshot_2024-02-12_at_3.58.09_PM.pngNaN"" width=""937"" height=""985"" /></p>
<p></p>
<p>Hi, can someone explains the highlighted part a little bit? I cannot find the corresponding formula for this step. Thank you!</p>","It utilized this Definition 1.5.15 in the textbook. The formula derives the angle between vectors so if you substitute the vector x , the given [1 1 0]^T, and the solution to it sqrt(6) you will get that highlighted solution."
"WP3 T/F: <p>For these 2 questions the counterexample simplifies to have a row of 0, so how is it a counterexample?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2F98c6023aec3ad6729f1c0a169f5b597a1d0f686c8cf63be11429f0ee2d205b86%2Fimage.png"" alt=""image.pngNaN"" /></p>",I think its because the matrix itself does not have any zero rows.
"[PP3] Q5 (d)(e): If it is stated that &#34;then Ref(A) has a row of zeros&#34; will the entire sentence be True? Thanks in advance :D<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2Fc3a80074d31b472228955e006a55f2019dd94b85f82e0a6f97114e04a29de914%2Ff16fe7db974442927d7ac275a77696e.png"" alt=""f16fe7db974442927d7ac275a77696e.png"" />","<p>i would believe e would be true not d</p>
<p>(I believe these are trick questions, since the A wouldnt be the one having the row of zeros instead it will be RREF(A).)</p>
i would believe e would be true not d"
Why can’t rank be equal to zero: Wouldn’t that just make the system inconsistent🤔,"That would mean that there are zero pivots in the matrix, and the only way to get this is if the matrix is the zero matrix itself."
Question about 9c) on practice midterm.: Does the question imply that we should assume u and v are not parallel?,Yes.
"Q8 Sample Midterm: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2F5b62b1d27026711ed3dd0462bb18d1c6e1eb8c1264a0b7a6b1b4cd2e9f206622%2Fimage.png"" alt=""image.png"" /></p>
<p>Can I just stop the computation without trying to solve for a,b,c? it is evident that $$x_1,x_2,x_3 \in \mathbb{R}$$ so its a subset of $$Span\{\vec{u},\vec{v},\vec{w} \}$$</p>","<p>No. We are trying to solve $$x_1, x_2, x_3$$ in terms of $$a, b, c$$. We don&#39;t yet know that there exist such $$x_1, x_2, x_3$$.</p>
<p></p>
<p>A faster way to do this question altogether is to use WA1 q2a</p>
<p>No. We are trying to solve $$x_1, x_2, x_3$$ in terms of $$a, b, c$$.</p>
<p></p>
<p>A faster way to do this question altogether is to use WA1 q2a</p>"
"Assumptions from External Sources: Can we use some of the proofs that we have already prove? 

for example if a question asks for proving equality of two spans, can we just invoke
WA1 Q2(a) to show, instead of doing it using matrices?

Similarly, proofs we have done in class such as span(u, v) = span (u, u - av) for a in R?

thanks","Yes, so long as “external” refers to material that has been covered in any capacity of this course."
"Question about matrix: <p>If A is a matrix, Does</p>
<p>$$A (\vec{v} +  \vec{u} ) = A  \vec{v}  + A  \vec{u} $$?</p>
<p>Thank you</p>",yes
"Does anyone know the correct way to M is a subset of the span{ v1 .. vk, w1..wk}: This question is from the internet with no solution<br /><img src=""https://cdn-uploads.piazza.com/paste/lkqwdl9os1tne/bf961b6598a0bf61b633a6c942278bf8cec8152331346ced4fb7564c93accdfd/Screenshot%2B2024-02-1.._%281%29.png"" />","<div>I feel like it would be like this. Just taking a shot at it.</div><div><img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwda7cwkc6ez%2Fdulrigxyvsjn%2FIMG_3606_2024_02_12.jpeg"" /></div><p></p>"
"practice midterm q9 b: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2Fd61e58be146f7672b017347cf725b5eaacee93c084e16b17f17b76b69a1cfdb1%2FScreenshot_2024-02-12_at_5.44.41_PM.png"" alt=""Screenshot_2024-02-12_at_5.44.41_PM.pngNaN"" />Could someone explain why rank cannot be zero? There are nothing wrong with vector u and v being zero vector. The question doesn&#39;t say they cannot be zero vector?","In the beginning of question 9 it assumes $$\vec u$$ and $$\vec v$$ are non zero, I thought the same thing as you when I first looked at it
In the beginning of question 9 it assumes $$\vec u$$ and $$\vec v$$ are non zero, I thought the same thing when I first looked at it"
"Proving a span is a spanning set for a space: <p>Hello,</p>
<p></p>
<p>Im still a little confused on how we prove a set is a spanning set for a space. We have multiple theorems (namely system rank theorem II) but im not sure how to put this into practice. Is turning the span into an augmented matrix and putting it into RREF enough?</p>
<p></p>
<p>Thank you!</p>","I’m pretty sure that the rank(A) = n when A is the matrix you’re trying to show is a spanning set and n is  the n in F^n, will prove that it is a spanning set of the given space."
Quiz 2 Remark: When will the quiz 2 remark request link be available?,They are available now.
Midterm solution: When will be the solution  uploaded?,"We are working hard on grading the Midterm at the moment. The grades are expected to be released Monday, at which point we will also be posting the solutions."
"Are excluded quizzes shown on crowdmark?: I used my short term term absence on quiz 3, but it still shows up on crowdmark. Is this normal?",Yes.  That is normal. 
Latex Solution Template: Will the solution template for latex be uploaded?,"Yes, they be posted shortly."
WP 5: When will solutions be posted?: When will solutions to WP 5 be posted? Thank you.,Tomorrow morning. 
".Tex file for WA2: <p>Could we please get the .tex file for WA2? It just helps go a little faster when typesetting// and looking how to format. </p>
<p>Thanks</p>",I just added it.
Coverage For WA 2: Up till what topic does WA 2 cover? ,"up to 5.4 (inclusive)
5.4, inclusive"
"Question 3 (a): <p>In the question, it is stated that: </p>
<div>
<div>
<div>
<ol><li>
<p>(a)  Let b ∈ R3. Using a one-sided inverse, derive a formula for the solution of the</p>
<p>#»</p>
<p>equation A x = b .<br />Note: You do not need to compute this one-sided inverse yet. That is part (b).</p>
</li></ol>
<p>The wording of the question is confusing me. Should we derive a formula for bot m&gt;n and m&lt;n or should we just do it for the one which is applicable to the aforementioned matrix A?</p>
</div>
</div>
</div>",You only need to do whichever one is applicable to the given $$A$$.
"WA2 Q1c): What does finding the inverse gotta do with the product of elementary matrices in (b).. I was planning to solve for the super augmented matrix I3<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Feec456f35384f49ecddaee0024899e2a4d6577e6923e9ce11e6fc31dfad45db6%2FScreenshot_2024-02-18_at_3.06.28_PM.png"" alt=""Screenshot_2024-02-18_at_3.06.28_PM.pngNaN"" />","<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwax1tzj04v0%2F5c6a509541884117fcf259e188d2c6a8751ed7af281ae8a766e782d316adee96%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>if $$I_n=E_k...E_1A$$ what does that mean about $$E_k...E_1$$?</p>"
"Not understand the relationship between standard matrix with linear transformation: <p>Suppose there is a linear transformation T that&#39;s determined by matrix A.</p>
<p>Then what&#39;s the difference between the standard matrix of T and A? </p>
<p>Aren&#39;t they the same?</p>","<p>Yes (proposition 5.5.5).</p>
<p></p>
<p>The course notes provide lots of intuition why this is true.</p>
<p></p>
<p>In general, we only need to know how a linear transformation acts on the standard basis or some other basis (we&#39;ll talk more about this later). This is because for $$\vec x = \begin{bmatrix} x_1 \\ \vdots \\ x_n\end{bmatrix}$$,</p>
<p></p>
<p>$$T(\vec x) = T(x_1 \vec e_1 + \cdots + x_n \vec e_n) = x_1T(\vec e_1) + \cdots + x_nT(\vec e_n)$$</p>
<p></p>
<p>This is what makes linear transformations so powerful. To know how $$T$$ transforms <em>any</em> vector $$\vec x$$, we actually only need to know how it transforms $$\vec e_1, \ldots, \vec e_n$$.</p>
<p></p>
<p>But, if you recall from a previous chapter, for a matrix $$A$$, $$Ae_i$$ is the $$i$$-column of $$A$$ (column extraction). So, if we take the vector $$T(\vec e_1)$$ and put that as the first column in our matrix $$A$$, and do the same for each of $$T(\vec e_i)$$, this exactly gives us our matrix version of $$T$$: that is,</p>
<p></p>
<p>$$T(\vec x) = \underbrace{\begin{bmatrix} T(\vec e_1) \mid \cdots \mid T(\vec e_n)\end{bmatrix}}_{\text{this is a matrix}} \vec x$$ for all $$\vec x \in \mathbb R^n$$.</p>"
how to write zero matrix latex: Just wondering what command can display a zero matrix symbol,Great that you figured it out
"How thorough should WA solutions be?: How much of our calculations should we show in our written assignments? For example, when multiplying matrices, do we need to show each step of splitting up the matrix and multiplying each vector every time? Or should we show our calculations the first time, to show that we know the process before skipping the steps the times after? (This also applies to finding matrix inverses)<br /><br />I know this applies to proofs less because you can sometimes get away with doing things without justification, but there are not many proofs in WA2 and showing the multiplication every time could get very tedious.","<p>You do not need to write out the dot product of each row by column for every entry.</p>
<p></p>
<p>I&#39;m not sure what you mean by &#34;you can sometimes get away with doing things [in proofs] without justification&#34;. You should never be able to get away with anything in proofs without justification. A proof should leave your reader feeling convinced that the result is true, which may include citing already-proven theorems or making your own claims and proving them.</p>"
"Composite Linear Transformation: <p>Just a question:</p>
<p>Isn&#39;t composite linear transformation essentially a matrix-matrix multiplication?</p>","<p>I don&#39;t understand your question. A map is not a matrix. A linear transformation has matrices representations.</p>
<p></p>
<p>If you are talking about linear transformations determined by matrices, then by definition the composition is determined by matrices multiplication. </p>
<p></p>
<p>On the other hand a linear transformation can (but not necessarily) be represented by a matrix. The matrix representation w.r.t. $$\mathcal{E}$$ of $$T\circ S$$ is $$[T]_{\mathcal{E}} [S]_{\mathcal{E}}$$.</p>"
"Linear Transformation: <p><strong>Definition 5.2.1: Linear Transformation</strong></p>
<p>We say that the function T:Fn&amp;#x2192;Fm&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;T:Fn→Fm&#xfffd;:&#xfffd;&#xfffd;→&#xfffd;&#xfffd; is a <strong>linear transformation</strong> (or <strong>linear mapping</strong>) if, for any x&amp;#x2192;,y&amp;#x2192;&amp;#x2208;Fn&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;x⃗ ,y⃗ ∈Fn&#xfffd;→,&#xfffd;→∈&#xfffd;&#xfffd; and any c&amp;#x2208;F&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;c∈F&#xfffd;∈&#xfffd;, the following two properties hold:</p>
<ol><li>T(x&amp;#x2192;&#43;y&amp;#x2192;)=T(x&amp;#x2192;)&#43;T(y&amp;#x2192;)&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;T(x⃗ &#43;y⃗ )=T(x⃗ )&#43;T(y⃗ )&#xfffd;(&#xfffd;→&#43;&#xfffd;→)=&#xfffd;(&#xfffd;→)&#43;&#xfffd;(&#xfffd;→) (called <strong>linearity over addition</strong>).</li><li>T(cx&amp;#x2192;)=cT(x&amp;#x2192;)&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;T(cx⃗ )=cT(x⃗ )&#xfffd;(&#xfffd;&#xfffd;→)=&#xfffd;&#xfffd;(&#xfffd;→) (called <strong>linearity over scalar multiplication</strong>).</li></ol>
<p>We refer to Fn&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;Fn&#xfffd;&#xfffd; here as the <strong>domain</strong> of T&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;T&#xfffd; and Fm&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;Fm&#xfffd;&#xfffd; as the <strong>codomain</strong> of T&#34; role=&#34;presentation&#34; style=&#34;box-sizing: border-box; display: inline-table; font-style: normal; font-weight: normal; line-height: normal; font-size: 16px; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; position: relative;&#34;&gt;T&#xfffd;, as we would for any function.</p>
<p></p>
<p>Here, the definition check if each property holds individually, whereas the alternate version of linear transformation states T(cx&#43;y), where both property holds simulatenously. When is it appropriate to check each property individually or simulatenously?</p>",I&#39;m not sure what happened with your post.  But I think I can answer your question.  You can use either form to check whether a  transformation is linear.  Usually when proving that it is linear it can be easier to use the version where the two properties are combined into one property.  When showing it&#39;s not linear it&#39;s usually easier to show that one of the two properties fails. 
"Are the midterm solutions out yet?: <p>Can&#39;t seem to find them on Learn. </p>
<p></p>
<p>Thanks!</p>",I believe they will be posted soon. 
"Not understanding the part for product of elementary matrix: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweg63r8w7db%2Ffd26064f832ea3fc934b4fcbbc03631e366347f45cf046a880dbfa3c859f9902%2FScreen_Shot_2024-02-19_at_7.50.26_PM_%282%29.png"" width=""1893"" height=""1065"" alt="""" />I&#39;m confused about how to get from applying row operation to yielding to the elementary matrices and the correct sequences of elementary matrices. ","<p>For every ERO there is a corresponding elementary matrix which when you multiply it on the left of matrix $$A$$, the product will produce a matrix which is the same as if the ERO was done to $$A$$.  To find the elementary matrix, simply apply the ERO to the identity matrix. </p>
<p></p>
<p>For example, if you did three row operations to $$A$$ to get $$R$$, then $$R=E_3E_2E_1A$$.</p>
<p>For every ERO there is a corresponding elementary matrix which when you multiply it on the left of matrix $$A$$, the product will produce a matrix which is the same as if the ERO was done to $$A$$.  To find the elementary matrix, simply apply the ERO to the identity matrix. </p>
<p></p>
<p>If you did three row operations to $$A$$ to get $$R$$, then $$R=E_3E_2E_1A$$.</p>"
"WA2 Q2: Negative exponent?: <p>What would it mean for the matrix B to be raised to a negative exponent, i.e. B^-2? Would that mean it&#39;s (B^-1)^2???</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F95f57db56ee119dadf69caf056eba81bbab2afd8b1e91657f64d3ce92bfa0e9f%2FScreenshot_2024-02-19_at_9.34.13_PM.png"" alt=""Screenshot_2024-02-19_at_9.34.13_PM.pngNaN"" /></p>","I would say that  B^-2 is (B^-1)(B^-1), given that B^2 is (B)(B). I also think that B^0 is just I_n, the identity matrix, since B^0 = (B^1)(B^-1)= I_n. I cannot say for sure though, so I will wait for an instructor&#39;s input on this.
I would say that  B^-2 is (B^-1)(B^-1), given that B^2 is (B)(B). I also think that B^0 is just I_n, the identity matrix. I cannot say for sure though, so I will wait for an instructor&#39;s input on this.
<p>In general, for positive $$k$$, $$A^{-k}$$ is defined as</p>
<p></p>
<p>$$(A^{-1})^k$$</p>
<p></p>
<p>Note that this implicitly assumes that $$A^{-1}$$ exists. For the definition of Nilpotent, you should take a positive $$k$$. That is,</p>
<p></p>
<p>$$A$$ is nilpotent $$\iff$$ there exists <em>positive</em> $$k$$ so that $$A^k = \mathcal O$$.</p>"
WA2 Q1d: I can think of a way to find one solution for x.. is it possible to find all solutions? <br />Do I use the theorem involving particular solutions?,"<p>If a matrix $$A$$ is invertible, then there will only be one solution for each system $$A \vec x = \vec b$$:</p>
<p></p>
<p>Suppose $$A$$ is invertible with inverse $$A^{-1}$$. Then, suppose $$A \vec x = \vec b$$. Then, applying $$A^{-1}$$ on the left on both sides, find $$\vec x = A^{-1} \vec b$$. This indicates that all solutions are exactly the vector $$A^{-1}\vec b$$.</p>"
"Q1 a): I&#39;m a little bit confused about the definition of the corresponding elementary matrices. If a matrix E satisfies the condition that B = EA but isn&#39;t elementary, because it&#39;s obtained after several EROs, does it still count as a corresponding elementary matrix?","No.  An elementary matrix corresponds to exactly one ERO.  Since it took Christina 6 EROs to get to RREF, you should have 6 elementary matrices. "
"Q3 a): When deriving the formula, do we have to simplify our answer? ",You don&#39;t need to calculate the one sided inverse.  Your answer should be a product of a matrix and a vector. 
"5.6 and 5.7 Coverage: I noticed that for 5.6, my section only covered reflection and rotation but not proj and perp, and for 5.7, we haven&#39;t done identity transformation and corollary 5.7.8. Will these criteria be tested? ",These things could be tested because there is really nothing new in these ideas. 
WA2 Q2: Is the best way to prove a matrix is nilpotent to simply trial and error to find a k such that A^k = the zero matrix or is there a more calculated approach,"You can keep multiplying it by itself to sequentially check $$k=1,k=2, k=\cdots$$"
"Midterm regrade: If i request a regrade for midterm for a specific question, will there be a grade down or up for other questions as well, or just the requested question be regraded?",".
they remark the whole thing
I believe they will only remark the questions you ask to be regraded.  I&#39;ll pass this question along for confirmation. "
"Q1 (e): Hi, may I ask how do we get vector u dot product vector u is 0?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2F750f824877832a6c745f619ef06c21e9e98d3fd7b4824a1b1b59422cc28c5c81%2F__2024-02-21_10.01.09.png"" alt=""__2024-02-21_10.01.09.png"" />","By definition of cross product, $$\vec{u} \times \vec{v}$$ is orthogonal to both $$\vec{u}$$ and $$\vec{v}$$. By the assumption, $$\vec{u} \times \vec{v} = \vec{u}$$. So , $$\vec{u}$$ is orthogonal to itself, so $$\vec{u} \cdot \vec{u} = 0$$."
"WA2Q1: For part d), can we use A^-1*(vector b)=vector x directly?",It can&#39;t hurt to justify why this is true. 
"matrix and vector: <p>Can all vector be considered as a matrix? If it is yes, does that means all theorem that apply to a matrix can be apply to a vector? Is the opposite true? Can all theorem that apply to a vector be apply to a matrix?</p>
<p>Thank you</p>","<p>Yes. Every vector is a matrix, but not every matrix is a vector. Theorems that apply to metrices can also be applied to vectors, as long as the sizes are compatible. </p>
<p></p>"
"WA2 Q1c and Q1d: For parts c and d, do we have to find the exact solutions for A^(-1) and x by computing all the matrix multiplications, or can we just leave the answers in terms of the Ei&#39;s for part c and A(-1) for part d, respectively?","<p>You c) need to find $$A^{-1}$$ explicitly by multiplying the matrices together. </p>
<p></p>
<p>For d) you should give the explicit solution along with how you determined it. </p>"
"WA2: Q2: Can we use, without proof, that if you multiply a matrix by any zero matrix, you get another zero matrix?",Yes.  Provided the matrices are of the correct sizes. 
Q5 c&#xff09;: Can we use the number of u1 u2 u3 provided in b)?,"Are you asking whether we can use $$\vec u_1, \vec u_2, \vec u_3$$ as defined in 5b) to complete 5c)? If so, the answer is yes. Part c) says T defined as in part b) so we are supposed to use them, and I don&#39;t think it would be possible to complete this question otherwise."
"Question for linear transformation: For definition 5.2.1, if we want to prove questions like if T(x) is a linear transformation or not, do we only need to prove one of the two conditions or do we need to prove both to ensure the proof holds",Both of them have to be true.   You need to prove both.  It is possible that a transformation may satisfy one but not the other. 
"[General question]: <p>Hi, is it ok to add vectors after  c$$\begin{bmatrix} 2x_{1} + 3x_{2}\\ 6x_{1} - 5x_{2}\\ 2x_{1} \end{bmatrix}$$ &#43; $$\begin{bmatrix} 2y_{1} + 3y_{2}\\ 6y_{1} - 5y_{2}\\ 2y_{1} \end{bmatrix}$$?</p>
<p><br />(e.g.  c$$\begin{bmatrix} 2x_{1} + 3x_{2}\\ 6x_{1} - 5x_{2}\\ 2x_{1} \end{bmatrix}$$ &#43; $$\begin{bmatrix} 2y_{1} + 3y_{2}\\ 6y_{1} - 5y_{2}\\ 2y_{1} \end{bmatrix}$$ &#43; $$\begin{bmatrix} 0\\ 0\\ 0 \end{bmatrix}$$)</p>
<p><br />Thanks :D<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F6098460e7f70728e7bea8bf7dd1bf98db26812691d3a9b117cf9b981085ef70a%2Fd3ad29251143a1f0477d74dc17a61fc.jpg"" alt=""d3ad29251143a1f0477d74dc17a61fc.jpgNaN"" /></p>",Can you write out what you mean? I’m not sure what “after the purple line” means.
"How to create a super augmented matrix in latex: <p>Hi there,</p>
<p></p>
<p>I was wondering how I would be able to write a super augmented matrix in latex.</p>
<p></p>
<p>I tried using \am{4}{1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 2 &amp; 3 &amp; 4} but it interprets the matrix as a normal augmented matrix.</p>","<p>Try<br /><br /></p>
<p>\left[</p>
<p>\begin{array}{cc|cc}</p>
<p>1 &amp; \tfrac{1}{5} &amp; \tfrac{1}{5} &amp; 0\\</p>
<p>1 &amp; 2 &amp; 0 &amp; 1</p>
<p>\end{array}</p>
<p>\right]</p>
<p></p>
<p>$$
\left[
\begin{array}{cc|cc}
1 & \tfrac{1}{5} & \tfrac{1}{5} & 0\\
1 & 2 & 0 & 1
\end{array}
\right]$$</p>

<p></p>
<p>You may put a few \! to reduce the spacing</p>
<p></p>
<p>\left[\!\!</p>
<p>\begin{array}{cc|cc}</p>
<p>1 &amp; \tfrac{1}{5} &amp; \tfrac{1}{5} &amp; 0\\</p>
<p>1 &amp; 2 &amp; 0 &amp; 1</p>
<p>\end{array}</p>
<p>\!\!\right]</p>"
"Required to remember standard matrices for rotations/reflections?: <p>In 5.6 of the course notes, there are many examples of linear transformations related to rotations, reflections, etc. Are we required to remember the associated standard matrices for these transformations? For example, in Professor Bauman&#39;s notes, there is an example which references a clockwise rotation around the origin (see screenshot). Should this standard matrix be memorized or would it be provided in the question?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwenjljntyg%2F6f599d4904ff064f9224540e2a19b02d1094fee090f3ab3f41b0281d41087d12%2Fimage.png"" alt=""image.png"" /></p>","<p>In general, we won&#39;t ask you to memorize specific kinds of linear transformations.</p>
<p></p>
<p>For rotations, this isn&#39;t too hard to derive on the spot by drawing out the unit circle and figuring out where $$\vec e_1, \vec e_2$$ would get mapped to.</p>"
"Lesson: <p>Can someone help me to explain how we get 0 and cx2 &#43; y2 here?<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F8dd9cad9f42329d1915bedf2770a60cc546ad38b1c5437cfe7e2affef4dd28b3%2FScreenshot_2024-02-23_at_7.17.11_PM.png"" alt=""Screenshot_2024-02-23_at_7.17.11_PM.pngNaN"" width=""389"" height=""263"" /></p>
<p>This part<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2Fa68c6fa7d4b1147851c3633a9792a72b78b40cc63aa48091678a4761e2b408db%2FScreenshot_2024-02-23_at_7.17.53_PM.png"" alt=""Screenshot_2024-02-23_at_7.17.53_PM.png"" /></p>","<p>First we use the property of zero maps zero, then you can see here:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F6b531afb06bde6faffb81d4f0d21e3c5a62abbf83bc8edc614009a05d4eb21e5%2Fimage.png"" alt=""image.png"" /></p>
<p>we are left with $$2d^2-2d$$ is 0, which means $$d=0$$ or $$d=1$$. So then we look at $$d=0$$ (I&#39;m not sure where the rest of the $$d=1$$ case is but it&#39;s not relevant to your question). </p>
<p></p>
<p>So we are looking at $$T(c\vec x + \vec y)$$ now. Since $$d=0$$, we know that the m=1 entry has to be 0 (we have $$3d^2x-2dxy= 3(0)^2x-2(0)xy = 0$$).</p>
<p></p>
<p>Then the m=2 entry is just $$x$$. However, I think there might be a typo in the definition of the transformation. I think it should read $$y+ 2d^2-2d$$ which then would make sense that the m=2 entry is $$cx_2+y_2$$. How the transformation is defined it should be $$cx_1 + y_1$$, but I don&#39;t think that was the intention. </p>
<p></p>
<p>That was kind of wordy so let me know if you need clarification, it would probably make more sense if I wrote out the steps and showed you. Hope that helps!</p>
<p></p>
<p></p>"
"Proof for Proposition 5.5.4: <p>Hello!</p>
<p></p>
<p>I do not fully understand the proof for the Proposition 5.5.4 in the textbook. Could anyone help explain this, please?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcf5nkeh7gy%2Fabb61fc529ffeb0b3a7c32378cd347d04c159d62f8f5f2185038ec5efed73b7b%2F__2024-02-23___8.15.03.png"" alt=""__2024-02-23___8.15.03.pngNaN"" /></p>
<p></p>
<p>Thanks a lot!</p>","<p>Since $$T$$ takes in vectors from $$\mathbb R$$ and outputs vectors in $$\mathbb R$$, it must be a $$1 \times 1$$ sized matrix (recall if a matrix is $$m \times n$$, then it takes in vectors in $$\mathbb R^n$$ and outputs vectors in $$\mathbb R^m$$).</p>
<p></p>
<p>Then, since $$\mathcal E = \{1\}$$, and $$[T]_{\mathcal E}$$ is $$1 \times 1$$, the only entry in $$[T]_{\mathcal E}$$ is $$T(1)$$.</p>
<p></p>
<p>From there, since $$T(x) = [T]_{\mathcal E}(x)$$ for all $$x \in \mathbb R$$, rewrite $$[T]_{\mathcal E}$$ as $$[T(1)]$$, and the matrix-vector product $$[T(1)]x$$ is the same as multiplication of real numbers, so</p>
<p></p>
<p>$$T(x) = T(1)x$$ for all $$x \in \mathbb R$$.</p>"
Prove the span of three vectors equals R3: How can we prove the span of three vectors equals R3? Can we prove by using the RREF of three vectors is the identity matrix so that these three vectors are linear independent?,"We have not talked about linear independence yet, so no.<div><br /></div><div>Show that any vector can be represented as a linear combination of your three vectors.</div>"
"Overleaf Not Working: <p>Hey,</p>
<p>The server on my overleaf side is down today, is anybody having the same problem with Overleaf today, where their server is down? </p>","My overleaf file seems to be working right now, maybe try a different browser?
I was using overleaf earlier and it was fine. "
"Understanding Ranks of Matrixes: This might be a really easy question, but if two matrixes A and B are equal, does that imply rank(A) = rank(B)","Yep! When we say two matrices are equal, that literally means that every entry is the exact same. You might be also thinking of equivalent matrices, which is like the relationship between A and RREF of A. In this case, they too also have the same rank. The rank of matrices won&#39;t change if you are only using valid EROs. "
"WA2: Q2: Can we use $$(B^{^{-1}}B)^{^{c}} = (B^{^{-1}})^{^{c}}(B)^{^{c}} , c\in\mathbb{N}$$ without proof (where B is a matrix)?","Correct me if I&#39;m wrong, but I don&#39;t think this would actually hold. $$(AB)^c$$ is like $$ABABABAB...$$ $$c$$ times, whereas $$A^cB^c$$ is $$AAA....BBB...$$ $$c$$ times. We know that matrix multiplication doesn&#39;t commute, so these are not the same computations.
<p>I see you changed your original question.  Your original statement was not true.</p>
<p></p>
<p>You can&#39;t use this without proof.  You should prove it. </p>"
WA2 Q2 - Assumptions about RREF of a Nilpotent Matrix: Are we allowed to assume that the RREF of a nilpotent matrix is also nilpotent?,No.
Quiz 3 coverage: What is the coverage of quiz 3?,Quiz 3 will cover up to the end of chapter 5. 
"WA2 Work Shown: For WA, do we need to show the process of how we perform EROs to obtained RREF? Or we can skip steps and just state the final RREF in proofing?","No, you don&#39;t have to show details of performing EROs. Do the calculation on scratch paper and state your final RREF."
"Higher order variable in linear transformation: Hi, I just want to ask do we need to prove that the linear transformation should only have linear functions for transformation. Can we use this property directly?","<p>What does it mean for a linear transformation to have a linear function for transformation?</p>
<p></p>
<p>In general, transformation is a synonym for function. Both deterministically return an output if given an input.</p>
<p></p>
<p>Linear transformations are the exact same as linear functions. They take in vectors and return vectors, which is what we mean when we write $$T : \mathbb R^m \to \mathbb R^n$$; $$T$$ takes a vector from $$\mathbb R^m$$ and outputs a vector in $$\mathbb R^n$$.</p>
<p></p>
<p>I&#39;m not sure what your question means. Feel free to clarify.</p>"
"WA2 Q3a: For Q3a, does solving the system Ax=b refer to the specific matrix provided, or is A a general mxn matrix? ",For a general matrix. The next step will ask you to use the given matrix.
"WA2 Q1(c): For (c), I don&#39;t see where could we use an identity matrix if we just multiply all the elementry matrices in (b) together to get the inverse of A.",Technically you can multiply the identity matrix at the end of that product. 
"Solutions for WP6: <p>Hi,</p>
<p></p>
<p>I was wondering when the solutions to WP6 will be posted.</p>
<p></p>
<p>Thank you.</p>",They will be posted on Thursday morning. 
"Matrix vector multiplication in Q1 d): In Q1 d), should I do (A^-1)x=b or Ax=b. If it&#39;s the latter, why do I need to use part c) in the answer?","<p>Remember we have $$A$$ and we have $$\vec b$$, so we&#39;re solving for $$\vec x$$. We can use $$A^{-1}$$ to solve for $$\vec x$$. Can you see how? Not sure how much I can say on this since an assignment question, but I hope that makes sense</p>
<p></p>
<p>Edit: check out @535, it might answer your question!</p>
Remember we have $$A$$ and we have $$\vec b$$, so we&#39;re solving for $$\vec x$$. We can use $$A^{-1}$$ to solve for $$\vec x$$. Can you see how? Not sure how much I can say on this since an assignment question, but I hope that makes sense"
"Chapter: 5 Example: 5.6.4: <p>Can anyone help me out on how we got $$w_{\text{1}}^2$$ - $$w_{\text{2}}^2$$<br />in $$\text{refl}_\vec{w}(\vec{e_1})$$<br />and $$w_{\text{2}}^2$$ - $$w_{\text{1}}^2$$ in $$\text{refl}_\vec{w}(\vec{e_1})$$. Thanks.<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwd5wc37w5nc%2F4856b57453825fb0f17ad7212a042cf2ed769dfed7be6c73cf526a7532e86014%2Fimage.png"" alt=""image.png"" /></p>
<p></p>","<p>There is a $$-\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$ and $$-\begin{bmatrix} 0 \\ 1 \end{bmatrix}$$.</p>
<p></p>
<p>So, for the first entry of $$\text{refl}_{\vec w}(\vec e_1)$$,</p>
<p></p>
<p>$$-1 + \dfrac{2}{w_1^2 + w_2^2} w_1^2 = \dfrac{-w_1^2 - w_2^2}{w_1^2 + w_2^2} + \dfrac{2w_1^2}{w_1^2 + w_2^2} = \dfrac{1}{w_1^2 + w_2^2}(w_1^2 - w_2^2)$$</p>
<p></p>
<p>and similar for the other.</p>
<p>It looks a little funny being presented in the textbook like that, but all we&#39;re doing is simplifying the expression. We want to multiply the standard basis vector $$\vec e_1$$ to get a common denominator, like so:</p>
<p>$$\begin{bmatrix} -1\\ 0 \end{bmatrix}\cdot\frac{w_1^2 +w_2^2}{w_1^2 +w^2_2}$$ = $$\frac{1}{w_1^2 +w^2_2}\cdot\begin{bmatrix} -w_1^2 -w_2^2\\ 0\end{bmatrix}$$. Then from there we can add it together to get one matrix</p>"
"WA2 Q4 -- Notation in the question: <p>Just to be clear, $$x^2_1$$ is referring to the number $$x_1$$ being squared, correct?</p>
<p></p>
<p>The notation looks a little weird to me so I wanted to check that I understand correctly</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2Fbcd2700e16e886ac082490144e8567b79baad0b9379b54262b4f9482fbaf8acd%2Fimage.png"" alt=""image.png"" /></p>",yes
When will the answer of wp6 be released?: ^,@605
What does Ker(T) mean?: What does Ker(T) mean? It&#39;s in question 5.,"<p>$$\text{Ker}(T)$$ denotes the kernel of $$T : \mathbb R^n \to \mathbb R^m$$:</p>
<p></p>
<p>$$\text{Ker}(T) := \{\vec x \in \mathbb R^n : T(\vec x) = \vec 0\}$$</p>"
What does one-to-one and onto mean?: What does one-to-one and onto mean? It&#39;s in Q5 c).,"<p>A function $$f : \mathbb R^n \to \mathbb R^m$$ is one-to-one (also know as injective) if $$f(a) = f(b) \implies a = b$$.</p>
<p></p>
<p>A function $$f : \mathbb R^n \to \mathbb R^m$$ is onto (also known as surjective) if for all $$\vec y \in \mathbb R^m$$, there exists $$\vec x \in \mathbb R^n$$ so that $$f(\vec x ) = \vec y$$.</p>"
"Question About Using Without Proof: <p>Is it true and can we use without proof that</p>
<p>If AB=I, then we must also have A^kB^k=I for any k?</p>","<p>This is true (assuming positive integer $$k$$).</p>
<p></p>
<p>Hint: try by induction.</p>"
"Are all non-nilpotent matrix invertible?: If so, then saying a matrix isn&#39;t nilpotent is the same as saying $$A\vec{x}=\vec{b}$$ is consistent/has a solution $$A^{-1} \vec{b}$$ for all $$\vec{b} \in \mathbb{F}^{n}$$?","<p>That is a very interesting question!</p>
<p></p>
<p>The answer is no. </p>
<p></p>
<p>This is the converse of one of the questions on the homework. If it was true, then we would have another characterization of invertibility (not nilpotent), but unfortunately this is not true.</p>
<p></p>
<p>Consider the $$x$$-line at $$y = 0$$ in $$\mathbb R^2$$:</p>
<p></p>
<p>$$\mathcal L := \{(x, 0) \in \mathbb R^2 : x \in \mathbb R\}$$</p>
<p></p>
<p>Consider the projection $$\text{proj}_{\mathcal L}$$ of $$(x, y)$$ onto $$\mathcal L$$:</p>
<p></p>
<p>$$\text{proj}_{\mathcal L}((x, y)) = (x, 0) = \text{proj}_{\vec e_1}((x, y))$$</p>
<p></p>
<p>When we repeatedly apply this projection function, we should expect that future applications don&#39;t change it (when you project a vector into itself, it doesn&#39;t change). This is to say:</p>
<p></p>
<p>$$\text{proj}_{\mathcal L}(\text{proj}_{\mathcal L}((x, y))) = \text{proj}_{\mathcal L}((x, y))$$</p>
<p></p>
<p>We know that the projection is a linear function, so we can find a matrix $$T$$ for this transformation. Then,</p>
<p></p>
<p>$$T^2\begin{bmatrix} x \\ y \end{bmatrix} = T\begin{bmatrix} x \\ y \end{bmatrix}$$ for all $$\begin{bmatrix} x \\ y \end{bmatrix} \in \mathbb R^2$$</p>
<p></p>
<p>Then, we can conclude that $$T^2 = T$$.</p>
<p></p>
<p>From here, I will leave the rest for you to figure out:</p>
<p>- How does this show that $$T$$ is not nilpotent?</p>
<p>- How do we know that $$T$$ is not invertible?</p>
<p>That is a very interesting question!</p>
<p></p>
<p>The answer is no. </p>
<p></p>
<p>This is the converse of one of the questions on the homework. If it was true, then we would have another characterization of invertibility (not nilpotent), but unfortunately this is not true.</p>
<p></p>
<p>Consider the $$x$$-line at $$y = 0$$ in $$\mathbb R^2$$:</p>
<p></p>
<p>$$\mathcal L := \{(x, 0) \in \mathbb R^2 : x \in \mathbb R\}$$</p>
<p></p>
<p>Consider the projection $$\text{proj}_{\mathcal L}$$ of $$(x, y, z)$$ onto $$\mathcal L$$:</p>
<p></p>
<p>$$\text{proj}_{\mathcal L}((x, y)) = (x, 0) = \text{proj}_{\vec e_1}((x, y))$$</p>
<p></p>
<p>When we repeatedly apply this projection function, we should expect that future applications don&#39;t change it (when you project a vector into itself, it doesn&#39;t change). This is to say:</p>
<p></p>
<p>$$\text{proj}_{\mathcal L}(\text{proj}_{\mathcal L}((x, y))) = \text{proj}_{\mathcal L}((x, y))$$</p>
<p></p>
<p>We know that the projection is a linear function, so we can find a matrix $$T$$ for this transformation. Then,</p>
<p></p>
<p>$$T^2\begin{bmatrix} x \\ y \end{bmatrix} = T\begin{bmatrix} x \\ y \end{bmatrix}$$ for all $$\begin{bmatrix} x \\ y \end{bmatrix} \in \mathbb R^2$$</p>
<p></p>
<p>Then, we can conclude that $$T^2 = T$$.</p>
<p></p>
<p>From here, I will leave the rest for you to figure out:</p>
<p>- How does this show that $$T$$ is not nilpotent?</p>
<p>- How do we know that $$T$$ is not invertible?</p>"
quiz3 coverage and solution to WP6: May I ask quiz 3 will start from where to the end of chapte5 and when will the solution to WP6 be posted?,"Quiz 3 will cover until the end of Chapter 5, and WP6 solutions will be posted on Thursday morning."
WA2 Q2a - Showing Matrix Multiplication: Do we need to show the Matrix Multiplication for B or can we just say multiplying it out gives the zero matrix?,You don&#39;t have to show your work multiplying out matrices.
"Question 5 part b): <p>When you substitute vector u1 u2 and u3 into T, will we get the vector [ -2x1 &#43; x3 / 6x1 &#43; x2 - 4x3 / 4x1 - 3x2 = x3] = [0 0 0]</p>
<p></p>","I don&#39;t understand your question. What do you mean by substitute vector $$\vec u_1, \vec u_2, \vec u_3$$ into $$T$$?"
"WA2 Q4: Hi, I am wondering when it says if T is a linear transformation does that mean in our proof we can assume T is a linear transformation or would we have to show that it is? Thanks.",“If $$T$$ is a linear transformation” means you can assume that $$T$$ is linear.
"Question about Matrix-Vector multiplication: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2F0cb98bfc49a2b8fb2f52daeb9e6d540fafbf8aa74effb30315a2e320f237b8d3%2FScreenshot_2024-02-28_at_12.20.28_AM.png"" alt=""Screenshot_2024-02-28_at_12.20.28_AM.pngNaN"" />Could someone explain why the result of multiply two matrix give a number?shouldn&#39;t it be a 1 * 1 Matrix?</p>
<p>Moreover, if A is a matrix and vx is a vector, is the operation vxA defined? Is Avx same as vxA?</p>
<p>Thank you</p>","<p>I&#39;m not sure what you&#39;re describing.</p>
<p></p>
<p>In your screenshot, the $$\vec a_i$$ are all column vectors.</p>
<p></p>
<p>In general, for $$A \in M_{m \times n}(\mathbb F)$$, and $$B \in M_{n \times k}(\mathbb F)$$, $$AB \in M_{m \times k}(\mathbb F)$$.</p>
<p></p>
<p>We can check the dimensions of $$\vec v$$ ($$n \times 1$$ matrix) and $$A$$ ($$m \times n$$ matrix) to see that $$\vec vA$$ is not well defined.</p>
<p>I&#39;m not sure what you&#39;re describing.</p>
<p></p>
<p>In your screenshot, the $$\vec a_i$$ are all column vectors.</p>
<p></p>
<p>In general, for $$A \in M_{m \times n}(\mathbb F)$$, and $$B \in M_{n \times k}(\mathbb F)$$, $$AB \in M_{m \times k}(\mathbb F)$$.</p>
<p></p>
<p>We can check the dimensions of $$\vec v$$ ($$\mathbb n \times 1$$ matrix) and $$A$$ ($$\mathbb m \times n$$ matrix) to see that $$\vec vA$$ is not well defined.</p>"
WA2 Q4: Can we assume that there is only 1 possible value for a and b?,"You should not make this assumption; if it is the case that there is only 1 possible value for $$a, b$$, then you should argue why."
"General WA Q: Do i have to show the process of matrix operations i did to get matrix in RREF or can i just give the starting and final RREF matrix (in latex it takes a while to type all of those that i did on paper in rough work, which is why im asking)","In general, at this point in the course I would say that you don&#39;t need to show the row operations unless the row operations are important to note.  For example, if you are calculating a determinant and you do row operations, it will be helpful to know what row operations you did. "
"Short Term Absence for WAs: <p dir=""ltr"">I couldn&#39;t find anything on the course outline regarding short term absences with written assignments. Are the grades redistributed like they were in MATH 135?</p>",Since we have far fewer assignments in this course than in MATH 135 the weight of the assignment is moved to the final exam. 
"Week 5 - Q2: Can anyone explain the solution for this?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fc08b6fd8eef2400d0c2aafbd6c2643de2ac48d632ce98aeee3fd471fb46deee2%2Fimage.png"" alt=""image.pngNaN"" />","I may be wrong, but I believe the implication follows from the definition of a one-to-one linear transformation. So by Invertibility Criteria – Second Version (since A is a square matrix), the matrix A is invertible.
I may be wrong, but I believe the implication follows from the definition of a one-to-one matrix. So by Invertibility Criteria – Second Version (since A is a square matrix), the matrix A is invertible.
I may be wrong, but I believe the implication follows from the definition of a one-to-one matrix. So by Invertibility Criteria – Second Version, the matrix A is invertible."
"Week 5 - Q4: <p>Whenever we factor a matrix like this and are left with a &#34;raw constant&#34;, is it always the case to slap on the identity matrix on it?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Ff2f62761a17a522927ed8bc6a1ff3884813b1be1100bc45ef1e81b27df4de545%2Fimage.png"" alt=""image.pngNaN"" /></p>","Yes, but for general cases you need to distinguish which side to put the identity matrix. For example if you need to factor $$AB - 7A$$, you do $$AB - A(7I_n) = A(B-7I_n)$$. On the other hand $$BA-7A=(B-7I_n)A$$.
Yes, but you have to distinguish which side to put the identity matrix. For example if you need to factor $$AB - 7A$$, you do $$AB - A(7I_n) = A(B-7I_n)$$."
"course notes exercise: <p>COuld someone please share their solution to this exercise?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2F3a09a58b19bd2a89baf6987617ead987f70ac4331ba18fb6d755d27d096333f5%2FScreen_Shot_2024-02-28_at_7.52.40_PM.png"" alt=""Screen_Shot_2024-02-28_at_7.52.40_PM.pngNaN"" /></p>",Maybe you could share your solutions and people could comment on them. 
"General questionn about span: <p>In general, how do I prove that a span{v1, v2} covers the entire F^2? assuming v1 and v2 are in F^2</p>
<p>Like do I just prove that they aren&#39;t parallel?</p>","I believe you can just prove that the standard basis vectors are linear combinations of the vectors in your given span. <div><br /></div><div>Since the standard basis vectors span $$\mathbb{F}^n$$, by one of our proofs in WA1 (question 2 I believe?) this would imply that $$\mathbb{F}^n \subseteq Span$$, then say something related to how Span is clearly a subset of $$\mathbb{F}^n$$ (since every linear combination will be in $$\mathbb{F}^n$$) and that should be sufficient. </div><div><br /></div><div>There might be a more efficient way to do this using newer concepts but I’m not sure, I’ll let an instructor weigh in on that!</div><div><br /></div><div>Edit: as for proving whether the vectors in the spanning set are parallel, this might hold for $$\mathbb{F}^2$$, I’m not entirely sure. However, it certainly does not hold for $$\mathbb{F}^n$$ for $$n&gt;2$$, you can see why in <a href=""https://piazza.com/class/lqzes2lznxt50s?cid=400""></a><a href=""https://piazza.com/class/lqzes2lznxt50s?cid=400"">@400</a></div>
I believe you can just prove that the standard basis vectors are linear combinations of the vectors in your given span. <div><br /></div><div>Since the standard basis vectors span $$\mathbb{F}^n$$, by one of our proofs in WA1 (question 2 I believe?) this would imply that $$\mathbb{F}^n \subseteq Span$$, then say something related to how Span is clearly a subset of $$\mathbb{F}^n$$ (since every linear combination will be in $$\mathbb{F}^n$$, and that should be sufficient. </div><div><br /></div><div>There might be a more efficient way to do this using newer concepts but I’m not sure, I’ll let an instructor weigh in on that!</div><div><br /></div><div>Edit: as for proving whether the vectors in the spanning set are parallel, this might hold for $$\mathbb{F}^2$$, I’m not entirely sure. However, it certainly does not hold for $$\mathbb{F}^n$$ for $$n&gt;2$$, you can see why in <a href=""https://piazza.com/class/lqzes2lznxt50s?cid=400"">@400</a></div>
I believe you can just prove that the standard basis vectors are linear combinations of the vectors in your given span. <div><br /></div><div>Since the standard basis vectors span $$\mathbb{F}^n$$, by one of our proofs in WA1 (question 2 I believe?) this would imply that $$\mathbb{F}^n \subseteq Span$$, then say something related to how Span is clearly a subset of $$\mathbb{F}^n$$ (since every linear combination will be in $$\mathbb{F}^n$$, and that should be sufficient. </div><div><br /></div><div>There might be a more efficient way to do this using newer concepts but I’m not sure, I’ll let an instructor weigh in on that!</div><div><br /></div><div>Edit: as for proving whether the vectors in the spanning set are parallel, this might hold for $$\mathbb{F}$$, I’m not entirely sure. However, it certainly does not hold for $$\mathbb{F}^n$$ for $$n&gt;2$$, you can see why in @400</div>
I believe you can just prove that the standard basis vectors are linear combinations of the vectors in your given span. <div><br /></div><div>Since the standard basis vectors span $$\mathbb{F}^n$$, by one of our proofs in WA1 (question 2 I believe?) this would imply that $$\mathbb{F}^n \subseteq Span$$, then say something related to how Span is clearly a subset of $$\mathbb{F}^n$$ (since every linear combination will be in $$\mathbb{F}^n$$, and that should be sufficient. </div><div><br /></div><div>There might be a more efficient way to do this using newer concepts but I’m not sure, I’ll let an instructor weigh in on that!</div>
To add on, you can show that the matrix $$A = [ \vec v_1 \mid \vec v_2]$$ is onto/surjective (i.e. $$\text{col}(A) = \mathbb F^2$$), or that this same matrix $$A$$ is one-to-one/injective since it is a square matrix."
"Short Term Absence on Quiz: I understand from the course outline that if you use the 48h policy on a quiz, it’s weight is moved to the exam.<div><br /></div><div>However, I know one of the quizzes will be dropped. Will this still hold if we use the 48h policy (in which case we’d only be assessed on 3 quizzes since one is dropped and one is shifted to the exam)?</div>","Yes, that&#39;s correct. "
"Week 6 - Mobius Problem: <p>Hey, I&#39;m not too sure how I can approach this problem from the Mobius (there&#39;s also no feedback).</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fb71e9816f7a913321a247156b90deb096639e46a9898b2b5970798bef4b412a4%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>I believe this problem stems from Example 5.6.2 of the course notes, specifically part (b). I&#39;ll give you the chance to look through it yourself, and if you have further questions about it I&#39;d be happy to help!</p>
<p></p>
<p>My lecture didn&#39;t cover this content explicitly and I&#39;m not sure about the coverage of your lecture. However, some of the specific processes from chpt 5 that might be helpful to us, like this one, are only presented in the course notes to my knowledge. </p>
I personally don&#39;t think you should stress about this question."
Coverage for quiz 3: what would be the coverage for quiz 3?,@588
"One-to-one and onto proofs: For a linear transformation T, aside from getting its standard matrix and its rank and using this proposition to prove if its onto or one-to-one, what other methods are there? This is assuming the T is not a transformation determined by a matrix A.  <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2Fed896f3cf914e71f6632cd8d876a21ef968e5f3ce1784e234999b80b34af9bfc%2FScreen_Shot_2024-02-29_at_6.58.11_PM.png"" alt=""Screen_Shot_2024-02-29_at_6.58.11_PM.pngNaN"" />",You can directly use the definition of onto and one-to-one i.e. $$T$$ is onto iff $$\text{Ran}(T) = \mathbb R^m$$ and $$T$$ is one-to-one if $$T(\vec x) = (\vec y) \implies \vec x = \vec y$$.
"Why do we need two ways to prove ?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2F73398f55e8ab0ee95e72b4fa53fbf371fc7e8b3cd3e57dae2787a11a74a3ed50%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2F8b8d8e8bf263c567c3c8c2203048cb46d84e99c39377071b73111b08dfb37e63%2Fimage.png"" alt=""image.pngNaN"" /></p>",It says &#34;(note only one of these would suffice for a proof)&#34; (the reason for this being that $$I_n - A$$ is square).
A matrix can be both onto and one-to-one for m=n right: ,"<p>Yes!</p>
<p></p>
<p>Such a matrix is invertible, and every invertible matrix corresponds to a linear transformation that is onto and one-to-one.</p>
<p>Yes!</p>
<p></p>
<p>Such a matrix is invertible, and every invertible matrix is onto and one-to-one.</p>
Yep! Check out invertibility criteria 2nd version at the end of chapter 5.4– all of the statements listed there are equivalent and what you described is either on there or directly related to one of the statements there!
Yep! Check out invertibility criteria 2nd version at the end of chapter 4– all of the statements listed there are equivalent and what you described is either on there or directly related to one of the statements there! "
"Range: <p>Can we directly read the range from its transformation? Like it says R3 to R2 can I directly says the range is R2?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2Fc06d7a7df83a89eb69cc2f7ae3c5056179a1849d2dc8cfb0adcebfba07bc40ca%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>If $$T$$ is onto/surjective, then yes, $$\text{Range}(T) = \mathbb R^2$$.</p>
<p></p>
<p>However, this is not always the case. What if there is some vector $$\vec b \in \mathbb R^2$$ so that for all $$\vec x \in \mathbb R^3$$, $$T(\vec x) \neq \vec b$$? (this would show that $$\vec b$$ is not in the range of $$T$$, even though it is in the codomain).</p>
<p>If $$T$$ is onto/surjective, then yes, $$\text{Ran}(T) = \mathbb R^2$$.</p>
<p></p>
<p>However, this is not always the case. What if there is some vector $$\vec b \in \mathbb R^2$$ so that for all $$\vec x \in \mathbb R^3$$, $$T(\vec x) \neq \vec b$$? (this would show that $$\vec b$$ is not in the range of $$T$$, even though it is in the codomain).</p>"
"standard matrix for projection: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2F5f0d689e5e153feae4f3b0e86a7c52cc914499a2475511efd6a7969faebe0e6b%2FScreenshot_2024-03-01_at_3.23.37_PM.png"" alt=""Screenshot_2024-03-01_at_3.23.37_PM.pngNaN"" />Could someone explain how do we know the standard matrix  is proj w (e1) proj w (e2)? Thank you",This follows from the definition of the standard matrix. See p134 in the course notes.
"Proving linearity Week6 PP Q1a: <p>Can I prove this in the opposite direction? (I start with T(cx&#43;y) and show it equals to cT(x) &#43; T(y).</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F087c6ad1a983bdeae0c839b78fae4548977fcd4e2dfab964c450071a6b2be64d%2FScreenshot_2024-03-01_at_15.47.51.png"" alt=""Screenshot_2024-03-01_at_15.47.51.pngNaN"" /></p>","Yep, since it’s an equal sign, it’s kinda like an iff, so either direction works 
Yes.  If $$a=b$$, then $$b=a$$. "
Special Transformations: Do we need to memorize the standard matrix for all the special transformations introduced in 5.6 for quiz3?,"<p>Professor Shane Bauman only recommends memorizing the rotation one</p>
<p></p>
<p>@576_f1, @646_f1</p>"
"Week6 PP Q3d: <p>In the solution for q3d it states that counter-examples can be provided by appropriate zero transformation. Can someone give an example? I&#39;m kind of confused by &#34;appropriate zero transformation&#34;.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F243d6c55888fe65b396a6c1ff59a95d5c631e05db8d51ceedacae6bbacde6b48%2FScreenshot_2024-03-02_at_00.09.15.png"" alt=""Screenshot_2024-03-02_at_00.09.15.pngNaN"" /></p>",The zero transformation is the transformation that maps every vector in the domain to the zero vector of the codomain.  So the standard matrix of this transformation will be a zero matrix of the corresponding size. 
"typo?: <p>Is this a typo?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F1bb9afa77040a999ee8f78ca29b1308719c39d5ac5e0719b9ad0c80efb3c46a2%2F17521709358270_.pic.jpg"" alt=""17521709358270_.pic.jpgNaN"" /></p>","<p>Yes.  That&#39;s a typo.  I&#39;ll correct it. </p>
<p></p>"
Understanding onto and one to one: I am struggling to understand how to argue if it is onto or one-to-one with criteria other than the rank,"<p>Assuming your question is how to actually show the criteria in order to show that something is onto/one-to-one</p>
<p></p>
<p>From one-to-one criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F52a12b57ec4c760c857d556c48a8c009a2f1781148c83ada982ded4685fc6fe3%2Fimage.png"" alt=""image.png"" width=""448"" height=""178"" /></p>
<p>for (b), you basically just want to find the nullspace of $$A$$.  This will let you see if the only element in the nullspace is the zero vector. In the case that you are given a linear transformation and not the corresponding matrix for it, you would need to first find the matrix. (c is essentially the same as d)</p>
<p></p>
<p>For onto criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F71481029816dc9038f581efe51d7e6be0a2a3c6b9242b74d3ba28c5e2b18e92b%2Fimage.png"" alt=""image.png"" width=""525"" height=""158"" /></p>
<p>Showing that $$\text{Col}(A)=\mathbb{F}^m$$ means to show that the columns of $$A$$ span all of $$\mathbb{F}^m$$. One way to do this would be to show that each of the basis vectors $$\vec{e}_1, \dots, \vec{e}_m$$ can be expressed as a linear combination of the columns of $$A$$.</p>
<p></p>
<p>If you are asking why the criteria other than rank work, there are proofs in the course notes that should give an idea to why they make sense. summary:</p>
<p></p>
<p>for one-to-one, the only solution to $$T_A(\vec{b})=\vec{0}$$ is $$\vec{b}=\vec{0}$$ is important in showing that if $$T_A(\vec{a})=T_A(\vec{b})$$, then $$\vec{a}=\vec{b}$$. This is because we can rearrange this into $$T_A(\vec{a})-T_A(\vec{b})=T_A(\vec{a}-\vec{b})=\vec{0} \implies \vec{a}-\vec{b} = \vec{0}$$ as the only vector such that $$T_A(\vec{b})=\vec{0}$$ is the zero vector by assumption. So $$\vec{a}=\vec{b}$$. If there were another solution to $$T_A(\vec{b})=\vec{0}$$, we could not conclude that $$\vec{a}-\vec{b}=\vec{0}$$</p>
<p></p>
<p>for onto, $$T_A(\vec{b}) = A\vec{b}$$, so the column space of $$A$$ represents the range of $$T_A$$, onto means the range of $$T_A$$ is all of $$\mathbb{F}^m$$, so it is the same as showing the column space is all of $$\mathbb{F}^m$$</p>
<p>Assuming your question is how to actually show the criteria in order to show that something is onto/one-to-one</p>
<p></p>
<p>From one-to-one criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F52a12b57ec4c760c857d556c48a8c009a2f1781148c83ada982ded4685fc6fe3%2Fimage.png"" alt=""image.png"" width=""448"" height=""178"" /></p>
<p>for (b), you basically just want to find the nullspace of $$A$$.  This will let you see if the only element in the nullspace is the zero vector. In the case that you are given a linear transformation and not the corresponding matrix for it, you would need to first find the matrix. (c is essentially the same as d)</p>
<p></p>
<p>For onto criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F71481029816dc9038f581efe51d7e6be0a2a3c6b9242b74d3ba28c5e2b18e92b%2Fimage.png"" alt=""image.png"" width=""525"" height=""158"" /></p>
<p>Showing that $$\text{Col}(A)=\mathbb{F}^m$$ means to show that the columns of $$A$$ span all of $$\mathbb{F}^m$$. One way to do this would be to show that each of the basis vectors $$\vec{e}_1, \dots, \vec{e}_m$$ can be expressed as a linear combination of the columns of $$A$$.</p>
<p></p>
<p>If you are asking why the criteria other than rank work, there are proofs in the course notes that should give an idea to why they make sense. summary:</p>
<p></p>
<p>for one-to-one, the only solution to $$T_A(\vec{b})=\vec{0}$$ is $$\vec{b}=\vec{0}$$ is important in showing that if $$T_A(\vec{a})=T_A(\vec{b})$$, then $$\vec{a}=\vec{b}$$. This is because we can rearrange this into $$T_A(\vec{a})-T_A(\vec{b})=T_A(\vec{a}-\vec{b})=\vec{0} \implies \vec{a}-\vec{b} = \vec{0}$$ as the only vector such that $$T_A(\vec{B})=\vec{0}$$ is the zero vector by assumption. So $$\vec{a}=\vec{b}$$. If there were another solution to $$T_A(\vec{b})=\vec{0}$$, we could not conclude that $$\vec{a}-\vec{b}=\vec{0}$$</p>
<p></p>
<p>for onto, $$T_A(\vec{b}) = A\vec{b}$$, so the column space of $$A$$ represents the range of $$T_A$$, onto means the range of $$T_A$$ is all of $$\mathbb{F}^m$$, so it is the same as showing the column space is all of $$\mathbb{F}^m$$</p>
<p>Assuming your question is how to actually show the criteria in order to show that something is onto/one-to-one</p>
<p></p>
<p>From one-to-one criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F52a12b57ec4c760c857d556c48a8c009a2f1781148c83ada982ded4685fc6fe3%2Fimage.png"" alt=""image.png"" width=""448"" height=""178"" /></p>
<p>for (b), you basically just want to find the nullspace of $$A$$.  This will let you see if the only element in the nullspace is the zero vector. In the case that you are given a linear transformation and not the corresponding matrix for it, you would need to first find the matrix. (c is essentially the same as d)</p>
<p></p>
<p>For onto criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F71481029816dc9038f581efe51d7e6be0a2a3c6b9242b74d3ba28c5e2b18e92b%2Fimage.png"" alt=""image.png"" width=""525"" height=""158"" /></p>
<p>Showing that $$\text{Col}(A)=\mathbb{F}^m$$ means to show that the columns of $$A$$ span all of $$\mathbb{F}^m$$. One way to do this would be to show that each of the basis vectors $$\vec{e}_1, \dots, \vec{e}_m$$ can be expressed as a linear combination of the columns of $$A$$.</p>
<p></p>
<p>If you are asking why the criteria other than rank work, there are proofs in the course notes that should give an idea to why they make sense. summary:</p>
<p></p>
<p>for one-to-one, the only solution to $$T_A(\vec{b})=\vec{0}$$ is $$\vec{b}=\vec{0}$$ is important in showing that if $$T_A(\vec{a})=T_A(\vec{b})$$, then $$\vec{a}=\vec{b}$$. This is because we can rearrange this into $$T_A(\vec{a})-T_A(\vec{b})=T_A(\vec{a}-\vec{b})=\vec{0} \implies \vec{a}-\vec{b} = \vec{0}$$ so $$\vec{a}=\vec{b}$$. If there were another solution to $$T_A(\vec{b})=\vec{0}$$, we could not conclude that $$\vec{a}-\vec{b}=\vec{0}$$</p>
<p></p>
<p>for onto, $$T_A(\vec{b}) = A\vec{b}$$, so the column space of $$A$$ represents the range of $$T_A$$, onto means the range of $$T_A$$ is all of $$\mathbb{F}^m$$, so it is the same as showing the column space is all of $$\mathbb{F}^m$$</p>
<p>Assuming your question is how to actually show the criteria in order to show that something is onto/one-to-one</p>
<p></p>
<p>From one-to-one criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F52a12b57ec4c760c857d556c48a8c009a2f1781148c83ada982ded4685fc6fe3%2Fimage.png"" alt=""image.png"" width=""448"" height=""178"" /></p>
<p>for (b), you basically just want to find the nullspace of $$A$$.  This will let you see if the only element in the nullspace is the zero vector. In the case that you are given a linear transformation and not the corresponding matrix for it, you would need to first find the matrix.</p>
<p></p>
<p>For onto criteria:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F71481029816dc9038f581efe51d7e6be0a2a3c6b9242b74d3ba28c5e2b18e92b%2Fimage.png"" alt=""image.png"" width=""525"" height=""158"" /></p>
<p>Showing that $$\text{Col}(A)=\mathbb{F}^m$$ means to show that the columns of $$A$$ span all of $$\mathbb{F}^m$$. One way to do this would be to show that each of the basis vectors $$\vec{e}_1, \dots, \vec{e}_m$$ can be expressed as a linear combination of the columns of $$A$$.</p>
<p></p>
<p>If you are asking why the criteria other than rank work, there are proofs in the course notes that should give an idea to why they make sense. summary:</p>
<p></p>
<p>for one-to-one, the only solution to $$T_A(\vec{b})=\vec{0}$$ is $$\vec{b}=\vec{0}$$ is important in showing that if $$T_A(\vec{a})=T_A(\vec{b})$$, then $$\vec{a}=\vec{b}$$. This is because we can rearrange this into $$T_A(\vec{a})-T_A(\vec{b})=T_A(\vec{a}-\vec{b})=\vec{0} \implies \vec{a}-\vec{b} = \vec{0}$$ so $$\vec{a}=\vec{b}$$. If there were another solution to $$T_A(\vec{b})=\vec{0}$$, we could not conclude that $$\vec{a}-\vec{b}=\vec{0}$$</p>
<p></p>
<p>for onto, $$T_A(\vec{b}) = A\vec{b}$$, so the column space of $$A$$ represents the range of $$T_A$$, onto means the range of $$T_A$$ is all of $$\mathbb{F}^m$$, so it is the same as showing the column space is all of $$\mathbb{F}^m$$</p>"
"W6PP How did we obtain the vector from the equation?: What do x1 and x2 represent in this equation and how did we obtain the vector for the line?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2F63a9ab42c4a5a86484691d9dade1c30e6b8b9f094d901f4ac01baca51b4dafe7%2Fimage.png"" alt=""image.png"" />","It&#39;s probably clearer if we write the equation as $$-2x_1=x_2$$.  This means that the second coordinate of any vector on the line is -2 times the first coordinate.  So any vector on the line has the form $$[t,  -2t]^T$$.
It&#39;s probably clearer if we write the equation as $$-2x_2=x_1$$.  This means that the second coordinate of any vector on the line is -2 times the first coordinate.  So any vector on the line has the form $$[t,  -2t]^T$$. "
"What is the standard matrix of reflection?: <p>Is it the same as rotation with the angle being π?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2Fa17f2f629a14ac517e7ff6fad3f4c418984a04ddacbdc4bae45e7c4b5f075087%2Fimage.png"" alt=""image.pngNaN"" /></p>",No.  It&#39;s a reflection in the given line.  See example 5.6.4 in the course notes. 
Invertibility Criteria: Is it important we memorize the entire list?,"Yes.  This is one of the most important ideas in the course.  (It will continue to grow as the course goes on.)
<p>Should we also know the entire second version of the invertibility list criteria?</p>
<p></p>"
Transformation Invertible: How can we show a transformation is invertible?,"To show that $$T$$ is invertible, show that there is a transformation $$S$$ so that $$ST\vec x = \vec x$$ and $$TS\vec x = \vec x$$ for every $$\vec x$$."
"Question 8b): <p>Hello,</p>
<p></p>
<p>I was wondering if someone could provide a hint for how to solve 8b) on WP5?</p>
<p></p>
<p>Would it be correct to start with using a variation of the hint in part a), wherein $$A\epsilon M_{3 x 3}(\mathbb{R})$$ such that $$A^3 = - I^{_{3}}$$ (that&#39;s a subscript - the equation editor is just not working with the I)?</p>
<p></p>
<p>Thanks!</p>","For starters, we would be interested in $$A \in M_{3 \times 3}(\mathbb R)$$ so that $$A^2=-I$$.<div><br /></div><div>For further hints, my solution for this question requires a lot of content we haven’t covered yet, so it may be a good idea to leave this aside and come back when we have covered a bit more material (eigenvalues).</div>
<md>I don't think there exists such a matrix, try using determinants.</md>"
invertible matrix: Are we allowed to show a matrix isn&#39;t invertible by showing the matrix&#39;s determinants is 0 on quiz 3? even though determinants aren&#39;t covered for the quiz?,No. You may not use the determinant for quiz 3.
"Range and Span. Kernel and Span: <p>How is range and span connected? How does the number of vectors inside the span element related to the field of the range?</p>
<p>For example, if we let Range(T)=F^2 and why does Span have 2 vectors. I mean Span{[a,b]^T [c,d]^T} is an example of span with 2 vectors. </p>
<p>The other question why if the Kernel is a span, then one-to-one test fails?</p>","<p>If we are considering a linear transformation  $$T$$, then range of $$T$$ is equal to the column space of its standard matrix.  The column space of a matrix is the span of its columns.  So at this point, that&#39;s the connection that we have between the range of a linear transformation and the span of a set of vectors.  Later in chapter 8 we will see more connections between these concepts.  <br /><br />At this point we haven&#39;t developed any ideas about the number of vectors in a spanning set and the span of those vectors.  This connection will also be explored in chapter 8. </p>
<p></p>
<p>I think you are mixing up the idea of a spanning set and the span of the vectors in the spanning set.  The span is the set of all linear combination of the vectors in the spanning set.  </p>
<p></p>
<p>The kernel of a linear transformation is all vectors that are mapped to the zero vector of the codomain.  We know for certain that the zero vector of domain is in the kernel because for a linear transformation the zero vector of the domain is always mapped to the zero vector of the codomain.  However, if there are more vectors in the domain which map to the zero vector of the codomain, then the transformation will not be one-to-one because there will be distinct vectors which map to the same vector. </p>"
"wp5 q2: <p dir=""ltr""><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Faace20215547cc77b8bc9a40a9642eced6fbb70a43ddcbbbc102c8e19cdd3e35%2F__2024-03-02_21.05.55.png"" alt=""__2024-03-02_21.05.55.pngNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2F7a184e8e4314af00900fcfa8dacf44e48aff71dcf8366670ebf5ce1a72ff9d6c%2FIMG_18C9C6D03743-1.jpeg"" alt=""IMG_18C9C6D03743-1.jpegNaN"" />For this question, i wonder can we randomly pick vector b such that Ax = b has unique solution in order to prove that nullity(A) = 0 other than picking zero vector</p>","Yes, that would also work."
"Mobius Question Projection: <a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweg63r8w7db%2Fff3dc318e772cf48643e41f28c02e2fd3a898fa25d2a163a12c7354b481148fa%2FScreen_Shot_2024-03-02_at_9.26.56_PM.png"" target=""_blank"" rel=""noopener noreferrer"">Screen_Shot_2024-03-02_at_9.26.56_PM.png</a>. Why is my answer incorrect? I calculated the standard matrix by calculating each standard matrix. I&#39;m unsure why my answer was incorrect, was it because of my calculation?","@646, there’s a specific method to do this in the course notes, but it’s not something we should worry about for the quiz"
"wp5 q5(b): <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fd95b71acadcae0149904fa518175aba063ca0c7d1bb2a1034e4e8e35790866cd%2FIMG_5E30039AA024-1.jpeg"" alt=""IMG_5E30039AA024-1.jpegNaN"" />I&#39;m confused about this question, can anyone explain it to me? Thanks!!!","<p>Recall a certain factoring identity:</p>
<p></p>
<p>$$1 - x^k = (1 - x)(1 + x + x^2 + \cdots + x^{k - 1})$$</p>
<p></p>
<p>Hence since $$A^k = \mathcal O$$,</p>
<p></p>
<p>$$1 = (1 - x)(1 + x + x^2 + \cdots + x^{k - 1})$$</p>
<p></p>
<p>Of course, for real numbers $$x$$, it makes sense to say</p>
<p></p>
<p>$$\dfrac 1 {1 - x} = 1 + x + x^2 + \cdots + x^{k - 1}$$</p>
<p></p>
<p>But it is nonsensical to write</p>
<p></p>
<p>$$\dfrac 1 {1 - A} = 1 + A + A^2 + \cdots + A^{k - 1}$$</p>
<p></p>
<p>because we don&#39;t have division of matrices.</p>
<p></p>
<p>However, the factoring identity still gives</p>
<p></p>
<p>$$1 = (1 - A)(1 + A + A^2 + \cdots + A^{k - 1})$$</p>
<p></p>
<p>Which is what we wanted.</p>"
"WP6 Q2: <p>How do we get this result? I don&#39;t remember we have learnt a formula to solve a standard matrix of refl.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwai8f51s2ag%2F539f468daf52fd9b6cd9206a8d193cf610159ad1001644183327c1318a6a83d8%2F_____2024-03-02_230919.png"" width=""651"" height=""117"" alt="""" /></p>","<p>5.6.4 in the course notes:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2Fedd58d3574933fb803ad978c46d12418a1e8cbd554c514a8f393fcee712e1595%2Fimage.png"" alt=""image.pngNaN"" /></p>"
"WP6 Question 5b): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F1b537a19ea5a05eb54fce7c0691a7ca6f65b5db7cd11038af044ce6cf5f28b03%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Is there not a converse direction proof because it is basically by definition that Range(T) belongs to F^m?</p>","Yes. It is clear that $$\text{Range}(T) \subseteq \mathbb F^n$$, so it suffices to only show that $$\text{Range}(T) \supseteq \mathbb F^n$$."
"About special standard matrix: In quizzes and exams, I wonder whether we will be provided with the standard matrix for special standard matrices like rotation, reflection and so on? Thanks","<p>No. We won&#39;t ask you to give the matrix for rotations or reflections and such, but they are useful for thinking of examples and/or counterexamples.</p>
<p></p>
<p>e.g. for T/F questions, it can be useful to consider these matrices for examples or counterexamples.</p>"
determinant: Is determinant only defined for square matrices?,Yes
"Range: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwatgzzdt48x%2F6997e77d84f981fb481af684ce4e680d206e6ddedf0e566b9951ff2ef3927461%2FCapture.PNG"" width=""411"" height=""126"" alt="""" /></p>
<p>Could someone explain how Range(T) is not in r4, aren&#39;t the vectors in the span in r4?</p>","(iv) is saying that the span of those two vectors is equal to $$\mathbb R^4$$, not that  the span is contained in $$\mathbb R^4$$."
"Wp6 q2: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Ff2d7b664b9b8fc76b8537df3cd39338fd7edc20cf7f13082f53a82e88dba053f%2Fimage.jpeg"" alt=""image.jpeg"" />In terms of this question, I wonder if Range(T1• T2)  must equal to Fp other than just belonging to?</p>
<p></p>","<p>Do you mean $$\text{Range}(T_2 \circ T_1)$$? $$T_1 \circ T_2$$ is not necessarily defined.</p>
<p></p>
<p>In this case, if $$\text{Range}(T_2) \neq \mathbb F^p$$, we cannot have that $$\text{Range}(T_2 \circ T_1) = \mathbb F^p$$. This is because</p>
<p></p>
<p>$$\text{Range}(T_2 \circ T_1) = \{ \vec b \in \mathbb F^p : \exists \vec y, (T_2 \circ T_1)(\vec y) = \vec b\}$$</p>
<p></p>
<p>So if $$\vec b \in \text{Range}(T_2 \circ T_1)$$, then</p>
<p></p>
<p>$$(T_2 \circ T_1)(\vec y) = \vec b \iff T_2(T_1(\vec y)) = \vec b$$, hence there exists $$\vec z \in \mathbb F^m$$ (namely, $$\vec z = T_1(\vec y)$$) so that</p>
<p></p>
<p>$$T_2(\vec z) = \vec b$$</p>
<p></p>
<p>This shows that $$\vec b \in \text{Range}(T_2)$$, so</p>
<p></p>
<p>$$\text{Range}(T_2 \circ T_1) \subseteq \text{Range}(T_2)$$.</p>
<p></p>
<p>The intuition behind this is that the range of $$T_2 \circ T_1$$ is the range of $$T_2$$, but restricting the input of $$T_2$$ to the output of $$T_1$$:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2Fd0376ff06a446f6d6a9c0a5c791adc4b75bffb9a7b4c2d84ce7f224d0348a130%2Fimage.png"" alt=""image.pngNaN"" /></p>"
"wp6 q4: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2F0fbb99dd2aa687a19f0c2a32d0d19cb0884b9db6c1e067d51b14f2014d67e91a%2Fimage.jpeg"" alt=""image.jpegNaN"" />I wonder if it is a correct proof. Thanks!</p>
<p></p>","<p>You have the right idea, but I would prove both directions separately.</p>
<p></p>
<p>Your proof here does not make it clear that any element in the span of $$\{T(\vec v_1), \ldots, T(\vec v_k)\}$$ is also in $$\text{Range}(T)$$</p>
<p></p>
<p>i.e. if I give you $$a_1T(\vec v_1) + \cdots + a_k T(\vec v_k)$$ for my choice of $$a_1, \ldots, a_k$$ (i.e. you don&#39;t know anything about $$a_1, \ldots, a_k$$), can you still show that there is a vector $$\vec z \in \mathbb F^n$$ so that</p>
<p></p>
<p>$$T(\vec z) = a_1T(\vec v_1) + \cdots + a_k T(\vec v_k)$$?</p>"
"prove linear: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2F4b32c37d263e0869bfe75124632d6aaa14f2b60ad0fbdf9a8ba28c5e908bac7a%2FIMG_0082.PNG"" alt=""IMG_0082.PNGNaN"" />For type of question like i, can we just find the standard matrix of T first (in this case do ii first)? Since there is a theorem said that every transformation determined by a matrix is linear.","<p>Yes, you could.</p>
<p></p>
<p>These problems are all for practice and not being marked, so it is up to you how you decide to approach them</p>"
"Find Elementary Matrix: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwei8tg2jp%2F867794bb53e08ee72ca47a3153109ef3675ea9ab05d16cbed770a763f29db39f%2F__2024-03-03___1.05.50.png"" alt=""__2024-03-03___1.05.50.pngNaN"" width=""394"" height=""228"" /></p>
<p>Why do we need the second step, can we just do R2 -&gt; R2&#43; 4R1 to get $$\begin{bmatrix}-2 & -3\\ 0 & 4 \end{bmatrix}$$</p>",You are right; it is unnecessary
"pp6 q6: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwatgzzdt48x%2F05ae176e9290868ac2a5dd08d9bf6d433310a1fca1595e96a56759b0eaae9380%2FCapture.PNG"" width=""538"" height=""388"" alt="""" /></p>
<p>I&#39;m not understanding the y = T(S(y)) and how does it mean that its in the Range</p>","<p>$$\vec y = T(S(\vec y))$$ because $$T$$ is invertible and $$S$$ is its inverse.</p>
<p></p>
<p>This shows that $$\vec y \in \text{Range}(T)$$ because there exists $$\vec z \in \mathbb F^n$$ (namely $$\vec z = S(\vec y)$$ so that</p>
<p></p>
<p>$$T(\vec z) = \vec y$$.</p>"
"[Problems about coverage] (5.7.6-.9): <p>Hi, wanna ask whether these definitions and corollary will covered in Quiz 3, somehow I didn&#39;t remember whether I learned them during lectures or not.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F2dc3c48bf3f735e675d6962de01afe450daa7e21f6b6faee86629f0f2ea4e136%2F8ed7d5db08602aedfff4bf3f822cf2a.jpg"" alt=""8ed7d5db08602aedfff4bf3f822cf2a.jpgNaN"" /></p>","Yes, the coverage for quiz 3 is up until the end of chapter 5."
"wp6 q6: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fd63a213cf4719aeb4b73930f853db029a9451fcd5ca68fd54c56f87b19851f61%2F__2024-03-03_13.35.55.png"" alt=""__2024-03-03_13.35.55.pngNaN"" />For Q6(a) the right-forward direction, can we assume T is invertible such that m = n and rank(A) = m = n, then by invertible criteria - second version, T is onto and 1-to-1","<p>Does right-forward mean $$\implies$$?</p>
<p></p>
<p>If so, you can&#39;t make the assumption that $$n = m$$ (this is true, but you would have to prove it to be so).</p>"
"wp6 q6(a): <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fbc58106470699d670b18fb42f2a47190c8c114380fa01fca89d95633f287148e%2F123B58E4-2913-46DB-9116-B1D72FC73A97.jpeg"" alt=""123B58E4-2913-46DB-9116-B1D72FC73A97.jpegNaN"" />In terms of this answer, since we&#39;re not given that S is onto, I don&#39;t think the range of S can represent all vectors in Fn, in that case, why can we directly apply S to both sides to get the result?","$$S$$ is defined to be the inverse of $$T$$ and satisfies the property that<div><br /></div><div>$$S(T(\vec x))=\vec x$$</div><div><br /></div><div>(I believe it states this in the blurb above the question)</div>
$$S$$ is defined to be the inverse of $$T$$ and satisfies the property that<div><br /></div><div>$$S(T(\vec x))=\vec x$$</div>"
"Week 6 - Practice Problems Q5: <p>I don&#39;t fully get why the solution was allowed to assume what we were trying to prove. Maybe I&#39;m misunderstanding the idea of one-to-one? Because if you suppose, T(x) = T(y), does that not automatically suggest x = y? Or is it that you have to definitely show that they&#39;re equal - in this case idn = idn?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F1a57fd6b0010f6c61c397ea237272eec2ef0c9603ecf181d136a5a275599099f%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F8a06329594a2d57b1b594def434fb17a75562af9247ee69ba4f06cde61d2aab2%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>A function $$T$$ is <em>injective</em> or <em>one-to-one</em> if $$T(\vec x) = T(\vec y) \implies \vec x = \vec y$$.</p>
<p></p>
<p>This is to say that if $$T$$ is injective, then whenever $$T(\vec x) = T(\vec y)$$, then you must have $$\vec x = \vec y$$.</p>
<p></p>
<p>Another way to think of injectiveness is by thinking of it as a uniqueness of output: that any output $$T(\vec x)$$ is unique in that no other value $$\vec y$$ gets mapped to $$T(\vec x)$$: i.e.</p>
<p></p>
<p>If $$\vec y\neq \vec x$$, then $$T(\vec y) \neq T(\vec x)$$ (this is actually the contrapositive).</p>
<p></p>
<p>To show that a function is injective, you should start by saying &#34;suppose $$T(\vec x) = T(\vec y)$$ for some $$\vec x, \vec y \in \mathbb F^n$$&#34;. Then, you should conclude by showing that $$\vec x = \vec y$$.</p>"
WA2 Solutions: Would it be possible to post the solutions to the written assignment before the quiz?,The solutions have been posted.
"WP6 Question 7: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F6d3f95fe4b92f1163053c8d8028c46351e79d820b75bc03e30b094479d5d7a21%2Fimage.png"" alt=""image.png"" /></p>
<p>What is Q4 in the solution to question 7 referring to? I looked at the question 4&#39;s in the problem set and the solutions to those didn&#39;t seem to be relevant.</p>",I believe it should say Q6 (the question above this one).
"What’s the difference between Ta and T?: In WP5 Q8A it tells us to find a T, transformation, that satisfies a condition. But the hint talks about an A value, so would that mean T and Ta are the same?","<p>$$T_A$$ and $$T$$ are both linear transformations.</p>
<p></p>
<p>$$T_A$$ is the linear transformation that is explicitly defined from the matrix $$A$$, by defining</p>
<p></p>
<p>$$T_A(\vec x) = A \vec x$$</p>
<p></p>
<p>whereas $$T$$ is just some linear transformation (aka linear function) that is not defined using a matrix.</p>
<p></p>
<p>It turns out that every linear transformation can be defined with a matrix, and every matrix defines a linear transformation (5.5), so writing $$T_A$$ or $$T$$ is just for notation.</p>
<p></p>
<p>For WP5 Q8, if you find such a matrix that satisfies the hint, what happens when you take $$T_A$$ for that $$A$$?</p>"
one-to-one linear transformations: if a linear transformation is one-to-one does this imply that it is consistent with the only solution to homogenous system the trivial solution?,"<p>Yes.</p>
<p></p>
<p>If $$T$$ is injective/one-to-one, then $$T \vec x = T \vec y \implies \vec x = \vec y$$.</p>
<p></p>
<p>Then, if $$T \vec x = \vec 0$$, then since $$T \vec 0 = \vec 0$$,</p>
<p></p>
<p>$$T \vec x = T \vec 0$$, so we must have $$\vec x = \vec 0$$.</p>
<p></p>
<p>This shows that any vector which is a solution to the homogenous system $$T \vec x = \vec 0$$ must be exactly the zero vector.</p>"
onto proof: to prove something is onto is it sufficient to show rank(A) = m or do you need to show col(a) = Fm? or both?,"<p>Onto or surjectivity describes transformations, not matrices.</p>
<p></p>
<p>To show a transformation is surjective, you should show that $$\text{Range}(T) = \mathbb F^m$$.</p>"
significance of standard matrix: I was wondering what the significance/importance of the standard matrix is and when its result is used,"<p>We alway say a standard matrix <strong>of a linear transformation T</strong>. </p>
<p></p>
<p>Using only the definition, to show $$T:\mathbb{F}^m\to \mathbb{F}^n$$ is onto, you need to check <em>every element</em> in $$\mathbb{F}^n$$ and show that it has preimage under $$T$$. (This is difficult.)</p>
<p></p>
<p>Using the standard matrix of $$T:\mathbb{F}^m \to \mathbb{F}^n$$, you can do algebra (like EROs) on $$[T]_{\mathcal{E}}$$ (an $$n\times m$$ matrix) and figure out its rank. If $$\mathrm{rank}([T]_{\mathcal{E}})=n$$ (<em>full row rank</em>) then it is onto. (Onto Criteria)</p>
<p>We alway say a standard matrix <strong>of a linear transformation T</strong>. </p>
<p></p>
<p>Using only the definition, to show $$T:\mathbb{F}^m\to \mathbb{F}^n$$ is onto, you need to check <em>every element</em> in $$\mathbb{F}^n$$ and show that it has preimage under $$T$$. (This is difficult.)</p>
<p></p>
<p>Using the standard matrix of $$T$$, you can do algebra (like EROs) on $$[T]_{\mathcal{E}}$$ and figure out its rank. If $$\mathrm{rank}([T]_{\mathcal{E}})=n$$ (<em>full row rank</em>) then it is onto. (Onto Criteria)</p>
<p>We alway say a standard matrix <strong>of a linear transformation T</strong>. </p>
<p></p>
<p>Using only the definition, to show $$T:\mathbb{F}^m\to \mathbb{F}^n$$ is onto, you need to check <em>every element</em> in $$\mathbb{F}^n$$ and show that it has preimage under $$T$$. (This is difficult.)</p>
<p></p>
<p>Using the standard matrix of $$T$$, you can do algebra (like EROs) on $$[T]_{\mathcal{E}}$$ and figure out its rank. If $$\mathrm{rank}([T]_{\mathcal{E}})=n$$ (full row rank) then it is onto. (Onto Criteria)</p>
<p>We alway say a standard matrix <strong>of a linear transformation T</strong>. </p>
<p></p>
<p>Using only the definition, to show $$T:\mathbb{F}^m\to \mathbb{F}^n$$ is onto, you need to check <em>every element</em> in $$\mathbb{F}^n$$ and show that it has preimage under $$T$$. (This is difficult.)</p>
<p></p>
<p>Using the standard matrix of $$T$$, you can do algebra (like EROs) on $$[T]_{\mathcal{E}}$$ and figure out its rank. If $$\mathrm{rank}([T]_{\mathcal{E}})=m$$ (full row rank) then it is onto. (Onto Criteria)</p>
<p>We alway say a standard matrix <strong>of a linear transformation T</strong>. </p>
<p></p>
<p>Using only the definition, to show $$T:\mathbb{F}^m\to \mathbb{F}^n$$ is onto, you need to check <em>every element</em> in $$\mathbb{F}^n$$ and show that it has preimage under $$T$$. (This is difficult.)</p>
<p></p>
<p>Using the standard matrix of $$T$$, you can do algebra (like EROs) on $$[T]_{\mathcal{E}}$$ and figure out its rank. If $$\mathrm{rank}([T]_{\mathcal{E}})=m$$ then it is onto. (Onto Criteria)</p>"
"invertibility and linear transformations: for some linear transformation TA, does being onto and one-to-one imply anything meaningful about the invertibility of A?","<p>For a general linear transformation $$T$$, if $$T$$ is NOT one-to-one, then $$T$$ can NOT be invertible. (Definition)</p>
<p></p>
<p>If $$T$$ is one-to-one, $$T$$ may NOT be invertible. (Definition)</p>
<p></p>
<p>If $$T_A$$, determined by a SQUARE matrix $$A$$, is one-to-one, then $$T_A$$ is invertible. (Invertibility Criteria)</p>
<p>For a general linear transformation $$T$$, if $$T$$ is NOT one-to-one, then $$T$$ can NOT be invertible. (Definition)</p>
<p></p>
<p>If $$T$$ is one-to-one, $$T$$ may NOT be invertible. (Definition)</p>
<p></p>
<p>If $$T_A$$, determined by a SQUARE matrix $$A$$ is one-to-one, then $$T_A$$ is invertible. (Invertibility Criteria)</p>
<p>For a general linear transformation $$T$$, if $$T$$ is NOT one-to-one, then $$T$$ can NOT be invertible. (Definition)</p>
<p></p>
<p>If $$T$$ is one-to-one, $$T$$ may NOT be invertible. (Definition)</p>
<p></p>
<p>If $$T_A$$ determined by a square matrix $$A$$ is one-to-one, then $$T_A$$ is invertible. (Invertibility Criteria)</p>"
"[WP6 Q3] appropriate zero transformation: <p>Hi, I am wondering what a counterexample zero transformation looks like for d). <br />Will O$$_{n\times n}$$ be a valid one?<br />Thanks<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2Ff1cc0fb9ff9897d353f114e638fb2e0b6e0910c1d203dc218a3ea01e39181ca8%2F0ea66045c2f3963391be6304fad481d.png"" alt=""0ea66045c2f3963391be6304fad481d.pngNaN"" /></p>
<div>
<div></div>
</div>","Yes. If you have $$A=\mathcal{O}_{3\times 3}$$, then $$T_A:\mathbb{F}^3 \to \mathbb{F}^3$$ is neither one-to-one or onto.
Yes. If you have $$A=\mathcal{O}_{3\times 3}$$, then $$T_A:\mathbb{F}^3 \to \mathbb{F}^3$$ is not one-to-one or onto."
projection onto plane: will showing projections onto planes be on the quiz tomorrow?,See @646.
"Question about differences in notation: What&#39;s the difference between the verticle vector [1 2] and [1, 2]^T?","<p>verticle vector[1, 2] represents a column vector, it is the standard way to write a vector in linear algebra, and [1,2]^T is another way to represent it use transpose, with a horizontal array of numbers, but it is also a column vector. You can use transpose to get one from another one, they are refer to the same thing, same vector, just different way to represent the vector.</p>
<p></p>
We use the notation $$[1,2]^T$$ for the sake of space so that we can write the vector horizontally and not take up a lot of space"
"[WP6 Q6 a]: Hi, wanna ask why we can directly let S(y) = x. (why is this well-defined). Thanks<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F2d243f6aa01709813284de6af8c852ef6e6825238a37b9f4f78ba1169ccf65ef%2F1709520927822.png"" alt=""1709520927822.pngNaN"" />","We have a one-to-one and onto transformation $$T$$.  That means for every $$\vec{y}$$ in the codomain there is exactly one $$\vec{x}$$ in the domain such that $$T(\vec{x})=\vec{y}$$.  Therefore, to define $$S$$ we say $$S(\vec{y})$$ is equal to the vector which $$T$$ maps to $$\vec{y}$$.  Since $$T$$ is onto there is always such an $$\vec{x}$$ and there is only one.  So $$S$$ is well-defined for its domain (which is the co-domain of $$T$$). "
"PP6: <p>Just to confirm, we are not responsible for these types of questions for tomorrow&#39;s quiz, right?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8mampgg62v%2F4084fce438731d41718ade5d98ca89af5c65b54230c0ee52e9d9b19f8cb53fa7%2FScreen_Shot_2024-03-03_at_10.33.40_PM.png"" alt=""Screen_Shot_2024-03-03_at_10.33.40_PM.pngNaN"" /></p>","<p>Q2 no. </p>
<p></p>
<p>Q3 is valid.  To find the standard matrix you just need to calculate the projections of the standard basis vectors onto $$\vec{w}$$ and they would become the columns of your matrix.  You could then use this matrix to answer part (b). </p>"
"Is there no difference between the rank of a matrix and that of the standard matrix?: <p>For onto criteria we have:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2Fe30e715fd1895a9836723bc5a017a5a6f3c4594a0939ff14ef6d9ffd644aa03a%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>And the property of standard matrix also says:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2F97582ca321ef3939b36badb5129b9512a5164132245e96bbddf432ae3ff79219%2Fimage.png"" alt=""image.pngNaN"" /></p>","When it comes to solving problems, there is no difference. This is guarenteed by property (b) in your second picture. "
"PP4 Q8 Why did they write A^2 as A(A^T): <p>Is there any reason that they wrote<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F618b8ea6e3e5c567b6a2dfd8c909cf83b30b579f648340f218b23d0bb06f5659%2Fimage.png"" alt=""image.png"" /></p>
<p>instead of $$(A^2)^T = (AA)^T = A^TA^T = A^2$$?</p>",Both are correct.
"PP6 Q1: <p>Would the statement still be true if T here is F^m -&gt; F^n ? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8mampgg62v%2F16e5eb5ab3e7c4a9df6c731650cbfde20ffac92006ada39a441087544afc6ba6%2FScreen_Shot_2024-03-04_at_7.44.05_AM.png"" alt=""Screen_Shot_2024-03-04_at_7.44.05_AM.pngNaN"" /></p>",No.  Can you find a counterexample?
short term absence declartion for quiz: I just submitted a short term absence. Is there anything else I should do to avoid writing the quiz tonight and have the weight shifted to the final?,It would be good to email math136@uwaterloo.ca and your instructor about this as well. 
"[Composition of Linear Transformation is Linear]: Hi, I am wondering if the converse of this proposition is wrong. (and if it is wrong, any hints to disprove the converse). Thanks in advance :D<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2Fcb56586dcecee4e2e9f705196ca68d1a2ef8f78019f3414ae91ea7ffa1ca6ddf%2F1709563433608.png"" alt=""1709563433608.pngNaN"" />","I think you can just make the inner one non linear and make the outer one multiply by the zero marrix
I think you can just make the inner one non linear and make the outer one multiply by the zero vector
The converse is not true.  Can you give an example of two non-linear transformations whose composition is linear? "
"how to solve this: <p>i dont understand how they arrived upon the answer, how do you project onto a plane, is there a specific formula for that?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2Fdd5af51a31bcd748525aa3b23b50098cf4ea5deddd78b8f91a8bfeee5d7d9c22%2Fimage.png"" alt=""image.pngNaN"" /></p>","@646, there is a specific way to do this from the course notes, but we don’t need to memorize it (see instructor follow up as well)"
"Wp6 Q5: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwarf9r4h3w4%2F36c6510296d14ffd03257dc4dd09b161c311b56c64d9cc16d6a1b0ecc3a4487e%2FScreenshot_2024-03-04_at_11.51.41_AM.png"" alt=""Screenshot_2024-03-04_at_11.51.41_AM.pngNaN"" width=""744"" height=""288"" /></p>
<p></p>
<p>This is the solution for Q5 and why do we know we should use [0v2-0v3, 1v3-0v1, 0v1-1v2]^T ? According to the cross rule, shouldn&#39;t it be [0v2-0v3, -(1v3-0v1), 1v2-0v1]^T?  </p>
<p></p>
<p>In addition, can someone also explains how do they prove that T is a linear transformation? I cannot really get this part too. Thank you!</p>","<p>By Proposition 1.9.5, we know that the cross product is linear in the second argument. This means that</p>
<p></p>
<p>1. $$\vec x \times (\vec y + \vec z) = \vec x \times \vec y + \vec x \times \vec z$$ for all $$\vec x, \vec y, \vec z \in \mathbb R^3$$; and,</p>
<p>2. $$\vec x \times (c \vec y) = c(\vec x \times \vec y)$$ for all $$\vec x, \vec y \in \mathbb R^3$$, $$c \in \mathbb R$$</p>
<p></p>
<p>This also means that the function $$T : \mathbb R^3 \to \mathbb R^3$$ defined by</p>
<p></p>
<p>$$T(\vec x) = \vec v \times \vec x$$ is linear for all fixed vectors $$\vec v$$.</p>
<p></p>
<p>The cross product</p>
<p>$$\vec v \times \vec x := \begin{bmatrix} 
v_2x_3 - v_3x_2 \\
-(v_1x_3 - v_3x_1) \\
v_1x_2 - v_2x_1
\end{bmatrix}$$</p>
<p>so taking $$\vec x = \begin{bmatrix} 1 \\ 0 \\ 0\end{bmatrix}$$, find</p>
<p></p>
<p>$$\vec v \times \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ v_3 \\ - v_2 \end{bmatrix}$$</p>"
"Exercise Chapter 5.6 Textbook: <p>Hello! I was doing the textbook practice for Chapter 5.6 and was having some trouble with part c. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdjis2njc7%2Fbcf5ba84b835c10236d97bffca953569fec2c47a43b981295627bbc79f2b84ae%2Fimage.png"" alt=""image.png"" width=""308"" height=""140"" /></p>
<p>From my a geometrically perspective, I would assume it is not onto since the perp is always going to be perpendicular to w, and not ever vector is perpendicular to some arbitrary vector.  However I do not know how I can construct this in a concrete proof. Hints are greatly appreciated! Thank you </p>","That is good intuition!<div><br /></div><div>If that is the case, then what if you pick a vector $$\vec y$$ which is not orthogonal to $$\vec w$$, and suppose for contradiction there is $$\vec x$$ so that</div><div><br /></div><div>$$\text{perp}_{\vec w}(\vec x)=\vec y$$?</div>"
"Wp5 Q2 e): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwarf9r4h3w4%2Fac2e95def5d4eb4f5e42964080bf3505a96e6338e578451c8e3e06fa5b641140%2FScreenshot_2024-03-04_at_12.10.01_PM.png"" alt=""Screenshot_2024-03-04_at_12.10.01_PM.png"" width=""220"" height=""122"" /></p>
<p></p>
<p>Why is this one not an elementary matrix?</p>",An elementary matrix corresponds to exactly one elementary row operation
"Do we need to memorize..: Do we need to memorize how to determine standard matrix of projection, rotation and reflection?",You don’t need to but they can be helpful when thinking about examples or counterexamples 
Practice 5 recommended question 7: Does anyone know how does row 3 come? I cannot compute that.<div><br /></div>,Do you mean for the row reduction? Would you like to share your work?
"invertibility and square matices: <p>it is enough to say that given a matrix is invertibility the matrix must be square?</p>
<p></p>
<p>Also if we have a transformation on a square matrix are we allowed to just assume that the transformation maps to two spaces in the same dimension?</p>","<p>1. I believe so since the invertibility criteria requires RREF(A) = In and the identity matrix only applies to a square matrix</p>
<p>2. I&#39;m not sure I understand your question</p>
Yes and yes."
"Content Qioz 3: Just to clarify, this quiz covers mainly week 5 and 6 correct?","No, it’s 4.5-5.7"
What is meant by program entry point: It says we need to include a program entry point fr the midterm. Does it just meant we write int main(void){}?,"this is math homie
I think you posted this in the wrong Piazza.<div><br /></div><div>Good luck with your midterm!</div>"
"Using Chapter 6 content in quiz 3: So I accidently used the triangle determinant method in chapter 6 for the last question of the quiz. Provided I get the correct answer, how many marks will I get cut off? <br /><br />EDIT: I used the fact that the determinant of a diagonal matrix is the product of the diagonal entries.",I&#39;m not sure what you mean.  Do you mean that you used the adjugate to calculate the inverse of $$A$$?
Q3: What does  entries of each row of A add to zero means,"<p>It means if</p>
<p></p>
<p></p>
<p>$$A =
\begin{bmatrix}
A_{11} & A_{12} & \cdots & A_{1n} \\
A_{21} & \ddots & \ddots & A_{2n} \\
\vdots & \ddots & \ddots & \vdots \\
A_{m1} & A_{m2} & \cdots & A_{mn}
\end{bmatrix}
$$</p>
<p></p>
<p>Then</p>
<p></p>
<p>$$\sum_{i = 1}^n A_{ij} = 0$$, for all $$1 \le j \le m$$</p>
<p></p>
<p>i.e.</p>
<p>$$A_{11} + A_{12} + \cdots + A_{1n} = 0$$</p>
<p></p>
<p>and</p>
<p></p>
<p>$$A_{21} + A_{22} + \cdots + A_{2n} = 0$$</p>
<p></p>
<p>and so on.</p>"
Fixed Point: Can fixed points be zero vectors? Or is it an eigenvector with $$\lambda$$ = 1?,"<p>The zero vector is always a fixed point in any linear transformation (because a linear transformation $$T$$ must satisfy $$T(\vec 0) = \vec 0$$).</p>
<p></p>
<p>Thus, since it is always a fixed point, we specify that eigenvectors are always nontrival (we are not interested in trivial cases); i.e. $$\vec 0$$ is not an eigenvector.</p>
<p>The zero vector is always a fixed point in any linear transformation (because a linear transformation $$T$$ must satisfy $$T(\vec 0) = \vec 0$$).</p>
<p></p>
<p>So we specify that eigenvectors are always nontrival; i.e. $$\vec 0$$ is not an eigenvector.</p>"
There is supposed to be MATH 136 tutoring in the MATH tutoring center: but no one is there.,"<p>I&#39;ll report this to the person who looks after the tutorial centre.</p>
<p></p>"
"minimizing distractions: I am not efficient with my time. I get easily distracted during time I am supposed to be studying and it has become quite a problem. So the time I put in does not always have a strong linear association with my results. Of course I do have quite a lot of other problems too, but this is one that is the most concerning. The solution to this problem is obviously just to focus and not get distracted, but only I don&#39;t focus. Sounds so easy but I just can&#39;t do it. I have tried to go to the library instead of staying in my room, tried setting a schedule to force myself to get certain things done within a reasonable time frame, but I still find that I am quite distracted by youtube, social media and random stuff that is not related to what I should be doing. Thank you for taking the time to read this I would really appreciated if you could share with me how you guys stay focused not to get distracted and what you think I should do to start the progress  towards minimizing distractions.","Highly recommend the pomodoro technique (or animedoro if you’re into anime). Also, it helps to set smaller goals since they help you feel more accomplished, and I know for me personally feeling accomplished makes it easier to continue working. A final tip would be to study to understand content, and to not be so strict with giving yourself a fixed time to study. The first 10-15 minutes of studying is usually the hardest to do, but once you’re engaged in whatever you’re studying time flies easier. Best of luck with your studying :)
<p>The student answer gives some great advice.  I have never used the pomodoro technique myself but I went to a workshop about cognitive science and learning and it was strongly recommended. </p>
<p></p>
<p>I wonder if it would be helpful to print out the problems you were working on (assignment or practice problems) and work completely on paper.  Then your device wouldn&#39;t be a distraction. </p>
<p></p>
<p>Another suggestion is to study with a partner and monitor each other&#39;s behaviour.  You don&#39;t have to talk to each other but you can support each other in keeping yourself on task. </p>
<p></p>
<p> </p>"
"Showing work for determinant calculations: For a 3x3 matrix A, do I have to show every step of how I calculate the determinant of A on WA3? I learned an efficient way for calculating determinant of 3x3 matrices so I&#39;m wondering if I can jump to the numer answer of det(A)","<p>If your method is efficient, then it shouldn&#39;t take too much time to write it down. Can you post an example of your method?</p>
<p></p>
<p>If the question is not fill-in-the-blank, we expect to see as many details as possible, especially the key steps of the calculation. Performing two or multiple EROs at once is fine. It&#39;s important to illustrate how you deduce the answer and which tools you use. If you are expanding the determinant along a row or column, you need to specify which row or column. If performing EROs, specify what EROs you are using and how you are obtaining a triangular matrix.</p>"
"Cramer rule: IN TERMS OF CRAMER RULE, DO YOU ONLY REPLACE ARBITRARY COLUMN ENTRIES WITH CONSTANT VECTORS FROM THE COEFFICIENT MATRIX OR CAN IT CAN ANY OTHER VECTORS?","<p>I&#39;m not sure what you mean. What are these &#34;arbitrary column entries&#34; referring to and what are the &#34;constant vectors from the coefficient matrix&#34;?</p>
<p></p>
<p>Cramer&#39;s rule states that when $$A$$ is invertible, the (necessarily unique) solution to $$A \vec x = \vec b$$ is given by a particular vector.</p>"
"Cramers rule proof confusion: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fa8debea7e77bc308d8fad0e82cbc0b6cef09d654ea9a4921255a5932af16edb0%2Fimage.png"" alt=""image.png"" width=""626"" height=""107"" /></p>
<p></p>
<p>I am confused on this sum, is it supposed to be $$\displaystyle\sum_{k=1}^n?$$  $$(B_j\in M_{n\times n})$$</p>","<p>I believe there is a typo, and it should write</p>
<p></p>
<p>$$\text{det}(B_j) = \displaystyle\sum_{\color{red}{k = 1}}^{\color{red}n} (B_j)_{kj}(C(B_j))_{kj}$$</p>"
"Area of shapes in R^n?: <p>Does this theorem generalize for all $$\mathbb{R}^n?$$ with $$n$$ vectors?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F7ec86e22d68c0ec4d1750c21dc1a8c31a4afa4beffa41f28f3e9005faaed6dd6%2Fimage.png"" alt=""image.pngNaN"" width=""1003"" height=""203"" /></p>
<p>I tried to write some formulas for the area/volume but I did not know how to get a determinant</p>
<p></p>
<p>Edit: I think if we assume this is true then the generalization directly follows, but now the problem shifts to why this is true:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Feda9415212de276033458a5297232520ebe7d3c05a73d1b62e16a155f83f822b%2Fimage.png"" alt=""image.png"" width=""654"" height=""76"" /></p>","<p>You may want to check out the notion of a lattice and the volume of a lattice:</p>
<p></p>
<p>https://cseweb.ucsd.edu/classes/wi10/cse206a/lec1.pdf</p>"
"how do i understand math 136 concepts better?: i spend a lot of my time studying for quizzes and midterms, but my marks don&#39;t reflect the amount of time i put into learning the concepts. are there any tips on how to understand the content better, i feel like i can only do computational problems and not any application. all tips on how i can use my study time more efficiently and be able to apply my learning better are appreciated!! ","This might be a question that is easier discussed in person rather than on Piazza.   You may want to reach out to your instructor.  If they don&#39;t have time to chat, I (Shane) would be willing to chat. 
3b1b has a series &#34;Essence of linear algebra&#34; on youtube which is basically a high level overview that gives some intuition and visualization of some concepts, it might help"
"Using Theorem from Class: On WA3, am I allowed to use a corollary to a theorem that my professor told us in class but that he didn&#39;t prove (and I don&#39;t believe it&#39;s in the course notes)?","Yes, you can use any results that were given in class. "
"A general question for WA3: For WA3 are we allowed to perform elementary operations along columns, similar to how we perform EROs along rows?","<p>You can derive a similar matrix for column operations.</p>
<p></p>
<p>Hint: if you want to perform a column operation on $$A$$, this is the same as performing a row operation on $$A^\top$$. </p>"
"Q5 capital C notation: <p>What does the capital C notation mean in this question? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fbb1109b43c046ad8b7d9b0fc03b7b96330ab658f58d1777322d8a92e5d1a7998%2Fimage.png"" alt=""image.png"" /></p>","<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fidt8cfyuv993c4%2Fae07d2d8257e96e7f6318cf3290d75f727555bb85cd801c805f56eadd34ed8af%2FScreenshot_2024-03-07_at_1.02.40_PM.png"" alt=""Screenshot_2024-03-07_at_1.02.40_PM.pngNaN"" />"
"Question about standard matrices: <p>Hello,</p>
<p></p>
<p>I was wondering if it is true that a linear transformation $$T : \mathbb{R}^n\to \mathbb{R}^n$$ will always be determined by a nxn matrix. I feel like it makes sense but I&#39;m not sure if it is 100 true so I wanted to ask.</p>","<p>Yes this is true!</p>
<p></p>
<p>I found these parts in the course notes, not sure if your question is entirely proven by these definitions, but they can at least provide some confirmation:</p>
<p>(Definition 5.1.1 -- Function Determined by a Matrix, if you can&#39;t see the screenshot)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F7c507542704c093759fd869a6439521a549838e5820f7cd8190338e708fbf8ae%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Note that the definition uses m and n to specify both the matrix and the linear transformation in a deliberate way. ($$A_{mxn} \Rightarrow T_A: \mathbb{F}^n \rightarrow \mathbb{F}^m$$)</p>
<p></p>
<p>We also know from 5.5 that every linear transformation is determined by a matrix, and you can see in 5.5.5 that they use the same conventions of m and n:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F64bdae1b5e6222673bfb1c5a8fba9b2925e50a5923dec12ee83419208d108d8c%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Yes this is true!</p>
<p></p>
<p>I found these parts in the course notes, not sure if your question is entirely proven by these definitions, but they can at least provide some confirmation:</p>
<p>(Definition 5.1.1 -- Function Determined by a Matrix, if you can&#39;t see the screenshot)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F7c507542704c093759fd869a6439521a549838e5820f7cd8190338e708fbf8ae%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Note that the definition uses m and n to specify both the matrix and the linear transformation in a deliberate way. ($$A_{mxn}$$ is defined by $$T_A: \mathbb{F}^n \rightarrow \mathbb{F}^m$$)</p>
<p></p>
<p>We also know from 5.5 that every linear transformation is determined by a matrix, and you can see in 5.5.5 that they use the same conventions of m and n:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F64bdae1b5e6222673bfb1c5a8fba9b2925e50a5923dec12ee83419208d108d8c%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Yes this is true!</p>
<p></p>
<p>I found these parts in the course notes, not sure if your question is entirely proven by these definitions, but they can at least provide some confirmation:</p>
<p>(Definition 5.1.1 -- Function Determined by a Matrix, if you can&#39;t see the screenshot)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F7c507542704c093759fd869a6439521a549838e5820f7cd8190338e708fbf8ae%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Note that the definition uses m and n to specify both the matrix and the linear transformation in a deliberate way. ($$A_{mxn} = T_A: \mathbb{F}^n \rightarrow \mathbb{F}^m$$)</p>
<p></p>
<p>We also know from 5.5 that every linear transformation is determined by a matrix, and you can see in 5.5.5 that they use the same conventions of m and n:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F64bdae1b5e6222673bfb1c5a8fba9b2925e50a5923dec12ee83419208d108d8c%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fidt8cfyuv993c4%2Ff12decd1b5f7f146c1c40bf7aaafcf57b2e3f1468e1829a2d5f1b8ea1bdf6dc2%2FScreenshot_2024-03-07_at_8.50.42_PM.png"" alt=""Screenshot_2024-03-07_at_8.50.42_PM.pngNaN"" />"
q1: What happens if we multiply n*1 unit vector and the transpose of that vector,Can you write out your equation? I don’t understand your question.
Theorems: Not exactly sure how to keep track of and memorize so many different theorems sometimes I miss one and then I&#39;m completely done on the quiz,"<p>I would go through the course notes.</p>
<p></p>
<p>Everything is very well-organized with all of the relevant proofs right there, and if you have been attending lectures, they may provide a different insight to specific topics than how your professor may have motivated it.</p>
<p></p>
<p>Try to focus less on memorizing theorems and more on knowing why the theorems are true. This will give you lots of intuition for proof questions, as opposed to blindly applying your catalogue of theorems and hoping one of them gives you a correct proof.</p>"
Q1 WA3: Can we use u^T u = 1 if u is a unit vector directly or we nee to explain it?,It becomes clear if you state that $$\vec{u}^T\vec{u}$$ is a $$1 \times 1$$ matrix with entry $$ \vec{u} \cdot \vec{u}.$$
"WA3: If a matrix is one-to-one, can we just say it is also onto by invertibility criteria second version? ","<p>I love that theorem, everything is equivalent so if you have one condition you have all of them. </p>
<div></div>
<div>HOWEVER, be careful because this criteria only applies to square matrices </div>
<div></div>
<div>If it is square, you can do that no problem! If it isn’t, you’ll have to go about proving in a different way. <br /><br />(in fact, if it&#39;s not a square matrix, being one-to-one implies that it&#39;s <em>not </em>onto!)</div>
I love that theorem, everything is equivalent so if you have one condition you have all of them. <div><br /></div><div>HOWEVER, be careful because this criteria only applies to square matrices </div><div><br /></div><div>If it is square, you can do that no problem! If it isn’t, you’ll have to go about proving in a different way. </div>
We can&#39;t say that a matrix is one-to-one.  This is a term that refers to transformations.  However, if a linear transformation is one-to-one and it has a square standard matrix, then yes, you can say that it is also onto. "
"[WA3] Q5(b) Dot product or multiplication?: I almost have the solution for Q5(b) however I&#39;m having trouble with the dot product part in the last line, if it is that. On the LHS of what we&#39;re trying to prove we have matrix multiplication so we have a matrix. However on the other side we have what looks like a dot product, meaning we have a scalar. These can&#39;t be equal to each other (and in my working I&#39;m just getting the scalar multiplication of det(A) and the standard basis vector for j). Is this meant to be multiplication or a dot product?","The left side is actually matrix vector multiplication ($$A\vec x$$) and the right side is scalar multiplication of a vector ($$c \vec x$$) so they will both yield a vector <div><br /></div><div>I agree that it could be misleading with the dot on the right side, since historically we’ve been careful about giving that the specific meaning of the dot product. </div><div><br /></div><div>Does that make more sense?</div>
Yeah, I can see why this is confusing.  The notation $$\cdot$$ was used to show that we are multiplying $$\vec{e}_j$$ by the scalar $$\det(A)$$.  What is meant is $$(\det(A))\vec{e}_j$$.  Does that help?"
"[WA3] Q2: Hi, any hints for proving T is one-to-one and onto, I am working with the direction that SoT is one-to-one and onto implies that S and T are one-to-one and onto, I proved that S is one-to-one and onto now but do not know how to deal with T. Thanks","I recommend considering the standard matrices of $$S$$, $$T$$ and $$S \circ T$$. 
Maybe you can think about the kernal and range of them, think about the definition :) Then use invertiblity criteria second version to conclude onto/one-to-one from one-to-one/onto."
"WA2 LaTex Bonus: Hi! I typed my solutions in LaTex for WA2, but I did not receive the LaTex bonus on Crowdmark. I noticed that in WA1, if the solutions were typed in LaTex, the mark received could go beyond 100. Is this also the case in WA2? Should I submit a regrade request then? Thanks!","happened to me with wa1<div>just submit a regrade request of any question and the typeset bonus will be added</div><div><br /></div>
I updated your mark."
How do you express a n*n matrix where very element is one?: How do you express a n*n matrix where very element is one?,"<p>I would say</p>
<p></p>
<p>&#34;Let $$A \in M_{n \times n}(\mathbb R)$$ be the $$n \times n$$ matrix with all $$1$$s; that is, let</p>
<p></p>
<p>$$A := \begin{bmatrix} 1 & \cdots & 1 \\ \vdots & \vdots & \vdots \\ 1 & \cdots & 1 \end{bmatrix}$$&#34;</p>
To add to the instructors answer, you can also use &#34;\ddots&#34; in LaTeX to get diagonal dots in the centre."
What is e_j?: What does e_j represent?,"<p>Assuming you&#39;re asking about question 5 on WA3,<br /><br />its the vector with every entry equal to 0 except the j&#39;th entry which is 1</p>
<p></p>
<p>$$\begin{equation}\vec{e}_j=\begin{bmatrix}0\\ \vdots \\ 1 \\\vdots\\0\end{bmatrix}\in\mathbb{R}^n\end{equation}$$</p>
<p></p>
<p>in summary just a vector with n rows where the j&#39;th row&#39;s entry is 1 and the other entries are 0</p>
<p>Assuming you&#39;re asking about question 5 on WA3,<br /><br />its the vector with every entry equal to 0 except the j&#39;th entry which is 1<br /><br />         [ 0 ]<br />         | :  |<br />e_j = | j  |  in R^n</p>
<p>         | :  |<br />         [ 0 ]</p>
<p></p>
<p>in summary just a vector with n rows where the j&#39;th row&#39;s entry is 1 and the other entries are 0</p>"
"Just double checking:: <p>Just double checking, but $\ve_j$ is defined to be a vector of only zeros except a single 1 on the jth row, right?</p>
<p></p>
<p>Edit: Just noticed a similar question load literally right below this one, sorry my internet is bad lol</p>","Yes, we write $$\vec{e_j}$$ for the jth standard basis vectors (only the jth entry is 1 and others are zeros). "
"Quiz 3: Hi, just wondering when quiz 3 and its solutions will be posted on LEARN?",I&#39;ll check with the coordinator. 
WA3: Can we use without proof that if a linear transformation is onto and one-to-one then its standard matrix must be square?,"I think you could probably just have the proof in just in case  since its basically a 1 line proof, just invoking one-to-one and onto criteria"
"WA3 Q1: if A^2=I_n, does it mean det(A^2)=det(I_n)?","Yes. Your two matrices $$A^2$$ and $$I$$ are the same, so they should have the same determinants."
WA3 q5a: what does Cjk(A) mean? Does it mean the entry on A with jth row and kth column?,"<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2F32124af0a99a2bea2ce612847d3c7698c09259af02e7ca26a43ab46ab77f246a%2Fimage.png"" alt=""image.pngNaN"" />"
"Easter Monday classes?: I checked the course schedule, and it appears that Monday, April 1st (Easter Monday) has Quiz 5 scheduled there. <div><br /></div><div>I was under the impression that we don’t have classes on Good Friday / Easter Monday, is this not the case?</div><div><br /></div><div>Should I also check with math 138 to see what the schedule is or is this a university-wide policy?</div><div><br /></div><div>Thanks for your help!</div>",The university is open on Easter Monday. This day is not a holiday. 
"Q3: If matrix A is not invertible, does it mean that the determinant will be 0? Or can non-invertible matrix also result in non-zero determinants?","A matrix $$A$$ is invertible if and only if det$$(A) \neq 0$$. So, there are no non-invertible matrices with a non-zero determinant.
To add on to the student answer, a <em>square </em>matrix $$A$$ is invertible if and only if $$\text{det}(A) \neq 0$$"
"similar matrix: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2F24b032f0c32aa844d3b818638736be205fc9306fbf77c255d1ba05106ef51911%2FScreenshot_2024-03-09_at_5.48.41_PM.png"" alt=""Screenshot_2024-03-09_at_5.48.41_PM.pngNaN"" />Just wondering will A and B have the same RREF? That is, we can get B by performing EROs in A?","No. You should try a few small examples by starting with something like $$A=\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}$$ and an invertible matrix $$P$$, and seeing if you can create a counter example."
Latex bonus: What will be the maximum mark for a written assignment after adding the latex bonus? 110?,"Each assignment is worth 2.5 percent, as I remember, and the bonus is 0.25% for each, thus it can reach a maximum of 110%. Yes."
"Crammer&#39;s Rule with a non-unique solution: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4v5pzre7le%2F880c2c9140395cd68f44b34d94c190a07d2d23ba133b13b219c4030a86fac46e%2Fimage.png"" alt=""image.pngNaN"" width=""698"" height=""233"" /></p>
<p>What happens when Ax = b has a non-unique solution. How will that look like and will the crammer&#39;s rule still hold? I think it would since technically the rules never said x needs to be a unique solution</p>","Actually maybe I am wrong because det(A) neq 0 iff  invertible iff rank(A) = n iff unique solution to Ax = b
Actually maybe I am wrong because det(A) = 0 iff not invertible iff rank(A) = n iff unique solution to Ax = b
Yes, because $$A$$ is invertible, the solution will always be unique."
"q1: Can we say 1*1 matrix a scalar? So, for example [1] can i just say it 1?","@466, yes you can say it&#39;s just a scalar"
"Cramer&#39;s Rule proof: <p>I have two questions about the proof of Cramar&#39;s Rule:</p>
<p></p>
<p>1) I don&#39;t understand this step. How can we just say A = Bj?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2F8837eb579a7f594abbba6cf59c58818413d2d1bb82f67abe0a9266c9d053b6e5%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>2) Also for this formula:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2F0ae9e0150a22807334ae9e7b5ee7223d5b4ffc3b4215b6bce1142dd156e9133a%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>should it be $$\sum_{j = 1}^{k}$$?</p>","<p>1. Since we are only swapping out one column to get $$B_j$$ from $$A$$, the cofactor, which contains the (k, j)th minor, doesn&#39;t actually include that column, so we can just replace $$B_j$$ with $$A$$. I&#39;d suggest looking at the sample calculations to see why this is true </p>
<p></p>
<p></p>
<p>2. When we&#39;re evaluating along the <strong>jth column</strong>, the formula looks different than the row formula:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F98a940fcbe379c567c28e3aa995a1c355432bfd05578f26f1835a37ea2e08b5e%2Fimage.png"" alt=""image.png"" /></p>"
"Rank: Can I say that for all n*n matrix A and B, Rank(AB) &lt;= Rank(A) and Rank(AB) &lt;= Rank(B)? If I can, how to prove it?","<p>Good question. We do have $$\mathrm{rank}(AB)\leq \min\{\mathrm{rank}(A), \mathrm{rank}(B)\}$$, but this is not in the course notes. There are various ways to prove it. Since we haven&#39;t learned dimension or linear independence, so far the only method is solving system of linear equations and counting pivots.</p>
<p></p>
<p>Hint: Compare the nullspaces of $$AB$$ and $$B$$. Also you need to use the fact that nullity is equal to the number of free parameters (and System Rank Theorem).</p>
<p></p>
<p>Good question. We do have $$\mathrm{rank}(AB)\leq \min\{\mathrm{rank}(A), \mathrm{rank}(B)\}$$, but this is not in the course notes. There are various ways to prove it. Since we haven&#39;t learned dimension or linear independence, so far the only method is solving system of linear equations.</p>
<p></p>
<p>Hint: Compare the nullspaces of $$AB$$ and $$B$$. Also you need to use the fact that nullity is equal to the number of free parameters (and System Rank Theorem).</p>
<p></p>
<p>Good question. We do have $$\mathrm{rank}(AB)\leq \min\{\mathrm{rank}(A), \mathrm{rank}(B)\}$$ for square matrices, but this is not in the course notes. There are various ways to prove it. Since we haven&#39;t learned dimension or linear independence, so far the only method is solving system of linear equations.</p>
<p></p>
<p>Hint: Compare the nullspaces of $$AB$$ and $$B$$. Also you need to use the fact that nullity is equal to the number of free parameters (and System Rank Theorem).</p>
<p></p>
<p>Good question. We do have $$\mathrm{rank}(AB)\leq \min\{\mathrm{rank}(A), \mathrm{rank}(B)\}$$ for square matrices and there are various ways to prove it. Since we haven&#39;t learned dimension or linear independence, so far the only method is solving system of linear equations.</p>
<p></p>
<p>Hint: Compare the nullspaces of $$AB$$ and $$B$$. Also you need to use the fact that nullity is equal to the number of free parameters (and System Rank Theorem).</p>
<p></p>"
"[Matrix Invertibility]: Hi, wondering if I have an invertible matrix, is it right to state the Null(A) = {$$\bar{0}$$} from the invertibility criteria 2nd version? (Even if there is no related Linear transformation $$T_{a}$$ be given)","Yes, this is true, and you can use that theorem as proof 👍<div><br /></div><div>Although just to nitpick a little, the actual statement is Null(A)={$$\vec 0$$} since both are sets. It might get you docked marks if you say Null(A) = $$\vec 0$$ because a set can’t equal a vector. Aka the Null(A) is equal to the set that only includes the zero vector.</div>"
"onto, one-to-one, and invertibility: If a square matrix is one-to-one, does it mean it&#39;s also onto since rank = n = m?","yep
A matrix is never one-to-one because this term applies to transformations.  However if the mapping determined by the matrix is one-to-one, then because the matrix is square, the mapping will also be onto. "
"[Summation related problem]: Hi, if I got a summation$$\sum_{j=1}^{n} C_{jk}(A)$$, can I just see the C$$_{jk}$$(A) part as a constant and simplify the summation into C$$_{jk}$$(A)?","I don&#39;t understand your question. This is a sum over $$C_{1k}(A), C_{2k}(A),\dots , C_{nk}(A)$$. You can simplify it to $$C_{jk}(A)$$. Which $$j$$? Any $$j$$?
I don&#39;t understand you question. This is a sum over $$C_{1k}(A), C_{2k}(A),\dots , C_{nk}(A)$$. You can simplify it to $$C_{jk}(A)$$. Which $$j$$? Any $$j$$?
I don&#39;t understand you question. This is a sum over $$C_{1k}(A), C_{2k}(A),\dots , C_{nk}(A)$$. You can simplify it to $$C_{jk}(A)$$ which $$j$$? Any $$j$$?
I don&#39;t understand you question. This is sum over $$C_{1k}(A), C_{2k}(A),\dots , C_{nk}(A)$. You can simplify it to $$C_{jk}(A)$$ which $$j$$? Any $$j$$?"
"Fixed points and eigenvectors: <p>1)Why do we care about fixed points in a linear transformation?<br />2) Fixed points are not always eigenvectors. since the zero vector is always a fixed point but cannot be an eigenvector.<br /><br /></p>
<p>Is my understanding in 2) correct?</p>","<p>Your understanding 2) is correct.</p>
<p></p>
<p>Fixed points of a transformation are inputs that are unaffected by the linear transformation.  They give us some understanding of the geometry of the transformation. </p>
<p></p>
<p>We won&#39;t do much more with fixed points in this course, but they do show up in future mathematics.  You can read more here to get some examples:  https://en.wikipedia.org/wiki/Fixed_point_(mathematics)</p>"
"[Invertibility Criteria]: <p>Hi, will I get marks deducted if I use invertible criteria after determining the square matrix is onto and one-to-one?</p>
<p><br />(Instead of using the invertibility criteria -- second version directly, b/c I was unsure about whether it was right to do so at that time)</p>
<div>
<div></div>
</div>","<p>First, we don&#39;t say a sqaure matrix is one-to-one or onto. One-to-one and onto are properties for maps. So you can say $$T_A$$ (a linear transformation defined by $$\vec{x}\to A\vec{x}$$) is one-to-one or onto.</p>
<p></p>
<p>Yes, once you have shown that $$T_A$$ is one-to-one or onto, using invertibility criteria, you can conclude that $$A$$ is invertible.</p>
<p>First, we don&#39;t say a sqaure matrix is one-to-one or onto. One-to-one and onto are properties for maps. So you can say $$T_A$$ (a linear transformation defined by $$\vec{x}\to A\vec{x}$$) is one-to-one or onto.</p>
<p></p>
<p>Yes, once you have shown that $$T_A$$ is one-to-one or onto, using invertibility criteria, you can conlude that $$A$$ is invertible.</p>"
roots of a polynomial vs roots of a equation: Are the roots of a equation the same as the root of a polynomial? I assume that polynomial and equation mean the same thing?,"<p>Roots of a polynomial $$p(x)$$ are the <em>solutions of the equation $$p(x)=0$$.</em></p>
<p><em></em></p>
<p>Equation has to have an equal sign. If you set a polynomial equal to zero you have a polynomial equation, i.g. quadratic equation, cubic equation, etc. If you set a trigonometric function equal to zero you have an trig equation. </p>"
"WP8 Q9 Recommended: <p>Hello,</p>
<p></p>
<p>The question asks &#34;Is there a matrix $$A \in M_{3 \times 3}(\mathbb{R})$$ that has exactly two real eigenvalues?&#34;.</p>
<p></p>
<p>Does exactly two real eigenvalues include repeated roots? For example, I found </p>
<p></p>
<p>$$A = \left[\begin{matrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0  \end{matrix}\right]$$ to have eigenvalues $$\lambda_1 = 2, \lambda_2 = -1, \lambda_3 = -1$$. If the repeated eigenvalue -1 is only counted once, then $$A$$ would satisfy the criteria of having exactly two real roots. Would this be a valid example?</p>","Repeated eigenvalues are counted as well, so your example would count 3 real eigenvalues.
<p>$$C_{A}(λ) = c_3λ^3+c_2λ^2+c_1λ+c_0$$</p>
<p>Because A has exclusively real entries I believe we could apply Conjugate Root Theorem, since $$C_{A}(λ)$$ is of degree 3 there are 3 roots, CJRT says that if a complex number $$z$$ is a root so is $$\bar{z}$$ having exactly 2 real roots would imply that the third in final root is complex which is a contradiction as complex roots come in pairs for polynomials with real coefficients</p>
<p></p>
<p>I could be extremely wrong though and I&#39;m open to anyone elses ideas</p>"
Bijection: I came upon the term bijection I am wondering what exactly it means and do we use it in our course? Thanks.,"Being bijective is equivalent to being invertible, so bijections are invertible maps.
Being bijective is equivalent to being invertible."
"WA3 Q1 a): Since u^Tu is a 1x1 matrix, then if we have [1], can we say that [1] = In?","[1] is the 1 by 1 identity matrix. This is not necessarily equal to $$I_n$$ (the n by n identity matrix).
Depends on what you’re trying to say<div><br /></div><div>If n=1 then yes, this is true, but say n=3 then this equation doesn’t make sense ( a 1x1 matrix cannot be equal to a 3x3 matrix) </div>"
"Characterisitc polynomial and eigenvalue over c: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4v5pzre7le%2F05d4506da1df49216a9789a3c4723d8871b1e57c5d8d085d2d2f6fae7cb4573b%2Fimage.png"" alt=""image.pngNaN"" width=""625"" height=""428"" /></p>
<p></p>
<p>I am wondering if the fact that the sum of eigenvalue over = trace(A) true for eigenvalue over R as well or is it just for eigenvalue over C. <br /><br />Also we know the product of complex eigenvalue is equal to the determinant. Is the product of real eigenvalues equal to the determinant as well? <br /><br />I looked at a few examples and so far both seemed to be true at least for the examples we have looked at. If both are true why are they true? How can I prove it?</p>",See @803.
"coefficent of characterisitc polynomial: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4v5pzre7le%2F05d4506da1df49216a9789a3c4723d8871b1e57c5d8d085d2d2f6fae7cb4573b%2Fimage.png"" alt=""image.pngNaN"" width=""759"" height=""519"" /></p>
<p></p>
<p>Is it still true that Cn = (-1) ^ (n) where n is the highest degree of the polynomial if we are looking at the characteristic polynomial over C?</p>","<p>Yes. This formula holds for all square matrices over all field $$\mathbb{F}$$, including $$\mathbb{F}=\mathbb{C}$$.</p>
<p></p>
<p><strong>You need to be careful</strong> that the eigenvalues are found over $$\mathbb{C}$$. Sometimes, from a real matrix, you may obtain characteristic equation $$\lambda^2+1=0$$ with no real eigenvalues, but it has two complex eigenvalues over $$\mathbb{C}$$. This formula works for eigenvalues over $$\mathbb{C}$$.</p>
<p></p>
<p>&gt;&gt;Is the product of real eigenvalues equal to the determinant as well? </p>
<p>If an $$n\times n$$ matrix $$A$$ has $$n$$ real eigenvalues then it is true. </p>
<p></p>
<p>If you are interested in the proof, search <a href=""https://en.wikipedia.org/wiki/Vieta%27s_formulas"" target=""_blank"" rel=""noopener noreferrer"">Vieta&#39;s formulas</a>.</p>
<p>Yes. This formula holds for all square matrices over all field $$\mathbb{F}$$, including $$\mathbb{F}=\mathbb{C}$$.</p>
<p></p>
<p><strong>You need to be careful</strong> that the eigenvalues are found over $$\mathbb{C}$$. Sometimes, from a real matrix, you may obtain characteristic equation $$\lambda^2+1=0$$ with no real eigenvalues, but it has two complex eigenvalues over $$\mathbb{C}$$. This formula works for eigenvalues over $$\mathbb{C}$$.</p>
<p></p>
<p>&gt;&gt;Is the product of real eigenvalues equal to the determinant as well? </p>
<p>If an $$n\times n$$ matrix $$A$$ has $$n$$ real eigenvalues then it is true. </p>
<p>Yes. This formula holds for all square matrices over all field $$\mathbb{F}$$, including $$\mathbb{F}=\mathbb{C}$$.</p>
<p></p>
<p><strong>You need to be careful</strong> that the eigenvalues are found over $$\mathbb{C}$$. Sometimes, from a real matrix, you may obtain characteristic equation $$\lambda^2+1=0$$ with no real eigenvalues, but it has two complex eigenvalues over $$\mathbb{C}$$. This formula works for eigenvalues over $$\mathbb{C}$$.</p>
Yes. This formula holds for any (number) field $$\mathbb{F}$$, including $$\mathbb{F}=\mathbb{C}$$."
General Question: Is it correct that a unit vector times it&#39;s transpose is equal to the identity matrix?,"$$u^Tu$$ yes, the reverse not necessarily.
$$\vec{u}^T \vec{u}$$ produces a $$1 \times 1 $$ matrix.  What is the entry in this matrix?"
"Can we assume that n is greater than 1 q5 (a): In q5 part (a), it says let A be an n x n matrix and then attempts to use the minor. However, seeing as n is not restricted, it&#39;s possible that A is a 1 x 1 matrix. This would then mean that the minor is a zero x zero matrix? After some research, it looks like there is no widely agreed upon convention for the determinant of the empty matrix. Also, seeing as the course has not touched upon this, can I assume that n &gt; 1?","Why don&#39;t you case out $$n = 1$$ separately? i.e. you don&#39;t have to use the method of minors/cofactor expansion for $$n = 1$$. You can directly compute the determinant of a $$1 \times 1$$ matrix.
Why don&#39;t you case out $$n = 1$$ separately? i.e. you don&#39;t have to use the method of minors/cofactor expansion for $$n = 1$$."
WA 3 q5 b: Can we assume that det (A) does not equal to 0?,"A is invertible, so by the Invertible Iff the Determinant is Non-Zero theorem, det(A) does not equal 0. "
"WA3 Q1a: Can anyone give any hints for Q1a? I have tried for a long time but I always end up with a quadratic equation, from which I dont know where to go ahead? ","Try expanding and simplifying the term with the square in your quadratic equation 
We don&#39;t post general hints on Piazza.  You could try asking a private question. "
"practice problem 7: <p>how can I know this part? (highlighted)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2Fef9b39891cbbc38bb4e076eacb4b2e3a49c9be819262e644849cdef49a2adfbc%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>Example 5.6.1 in the course notes:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkrvaikp4js87pd%2F8366ae372ef541853a4dcd89eb36f6b42c051eda78b34d34f4f63b8725f22a52%2Fimage.png"" alt=""image.png"" /></p>"
"Cofactors: <p>I&#39;m just a bit confused with the idea of cofactors. What is it exactly? Is it a matrix or a value? Is the cofactor different to the matrix of cofactors and how do they relate?</p>
<p></p>
<p>Thank you</p>",You shouldn&#39;t be asking for hints publicly on Piazza.  Private posts are ok 
0x0 Matrix: Is the idea of a 0x0 matrix well-defined? Are we ever supposed to consider 0x0 matrices in this course?,"I&#39;m pretty sure it is well-defined but we don&#39;t ever have to talk about it in this course
I&#39;m pretty sure it is well-defined but we don&#39;t ever have to talk about it
We don&#39;t consider $$0 \times 0$$ matrices in this course. "
"Effect of EROs on the Determinant - &#34;Row Subtraction&#34;: <p>Hello,</p>
<p></p>
<p>I&#39;d like get some clarification on the effect of Row Addition on determinants. According to the course notes, &#34;If B is obtained from A by adding a non-zero multiple of one row to another row, then the determinants of A and B are the same.&#34; </p>
<p></p>
<p>Here are two example EROs:</p>
<p>1. $$R_1 \rightarrow -R_3 + R_1$$ </p>
<p>2. $$R_1 \rightarrow -R_1 + R_3$$</p>
<p></p>
<p>Will the second one produce a different determinant?</p>
<p></p>
<p>Thank you!</p>","Yes, the second one affects the determinant since it is not an elementary row operation. You are first scaling $$R_1$$ by -1, then adding $$R_3$$ to $$R_1$$, so the determinant is multiplied by -1
No, the second one is not an elementary row operation since &#34;row subtraction&#34; is not actually an ERO. You are first scaling $$R_1$$ by -1, then adding $$R_3$$ to $$R_1$$."
"WA3 Q5: Are we free to assume that $$n\geq2$$ for both parts in this question? The cofactor wouldn’t even be defined for $$n=1$$ to my knowledge. <div><br /></div><div>I saw a post about this yesterday, but I can’t find it anymore </div>","Yes, please assume that $$n\ge2$$ for this question.
@806"
"Typo in course notes?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2F2c802f4c03e83ef4710a84061b78ef758845eff49cef98bddd77bdf56c547b23%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>shouldn&#39;t the matrix A-3I be $$\begin{bmatrix} -2 & -1\\ -1 & -2 \end{bmatrix}$$?</p>","We’re not subtracting 3 from each entry, only the ones on the diagonal
<p>Recall the identity matrix is a diagonal matrix with $$1$$s in the diagonal and $$0$$s everywhere else.</p>
<p></p>
<p>$$A - 3I = \begin{bmatrix} 1 & 2 \\ 2 & 1\end{bmatrix} - 3 \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} = \begin{bmatrix} -2 & 2 \\ 2 & -2\end{bmatrix}$$</p>"
Test3: When can we get back our test 3 result?,Thursday
"question from class: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2F4683a6d7a6fa8711eadd913fc9d1904ed328199d0609239f11f12c1e2ae2c3d5%2FIMG_B169E65A48D4-1.jpeg"" alt=""IMG_B169E65A48D4-1.jpegNaN"" />This is an example from class. I&#39;m confused with the question why we can get that there are 3 eigenvalues in total. Thanks!","For an $$n \times n$$ matrix $$A$$, the characteristic polynomial $$C_A(\lambda)$$ has degree $$n$$. Recall that we define the characteristic polynomial as a function of $$\lambda$$, the eigenvalues we are searching for, by setting<div><br /></div><div>$$C_A(\lambda) = \text{det}(A - \lambda I)$$</div><div><br /></div><div>If you expand $$\text{det}(A - \lambda I)$$ using cofactor expansion, you can find the highest degree of $$\lambda$$ is $$n$$. Thus, for this example, we should expect the characteristic polynomial $$C_A(\lambda)$$ to be cubic, so we can expect 3 eigenvalues $$\lambda_1, \lambda_2, \lambda_3$$.</div>"
"WP7 error: <p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2Fdcc6c0e5e839f3012e4c8eadfcec3763919704642a66950c034dc8052a673d04%2Fimage.png"" alt=""image.png"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2F356f1d8adba7325b1990053c33a3f68e3cffee586cfbf84b74e549159310b8c9%2Fimage.png"" alt=""image.png"" /></p>
<p>I think the question is missing the initial -2, it should be 32</p>","The rest of the solution is on the next page. 
I agree.  I&#39;ll fix it."
"[PP7] Recommend Q1: <p>Hi, will this kind of question be tested on the quiz/final? And is it a must that we need to justify the distance just like the solution did? Thanks in advance :D<br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2Fe6d72275dd62532c3df2132063e14fafd351347f811d6a856004ef53034d9943%2F1710438952692.png"" alt=""1710438952692.pngNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F77d1e6f29e89ee6d2d07692a28543ddfdd25356d7d67e536509b967e2e58840a%2F1710438974158.png"" alt=""1710438974158.png"" /></p>","<p>We won&#39;t ask you questions that will specifically require the usage of some derived standard matrix (such as reflections, rotations, etc.).</p>
<p></p>
<p>For this question, it is necessary to justify the distance between the transformed vertices like this. It may be intuitively obvious that a reflection won&#39;t change the distance between consecutive vertices, but the whole point of the computation is to verify that that idea is actually correct.</p>"
"[PP7] recom Q2: <p>Hi, can anyone tell me where we cover this property (property of diagonal matrices)? I didn&#39;t find it.<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2Fd25c17f0c6cf81ceac9f100daed663409f678c4ae58124f83f4dc42092683785%2F1710447679336.png"" alt=""1710447679336.pngNaN"" /></p>
<p></p>",Think about what happens when you multiply $$DD^{-1}$$ where $$D$$ is diagonal. What do the diagonal entries of $$DD^{-1}$$ look like?
"Weekly exam: When can we get back our week test result, i think now it is probably already on Thursday. ",Some TAs seem to be holding up the marking.  It looks like it is almost done. 
"Whats the best way of finding these complex roots?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2F8c9201a50a6dbc818930405db673c79915b8352355c3594807a8f3f6560e74a8%2FScreenshot_2024-03-14_at_6.58.10_PM.png"" alt=""Screenshot_2024-03-14_at_6.58.10_PM.pngNaN"" /></p>
<p>I tried quadratic formula but it was very messy</p>","<p>I cannot find an easier way. You don&#39;t know the solution until you solve it. </p>
<p></p>
<p>Quadratic formula is fine. The difficulty is to find the square root of the discriminant $$-1-4(2+10i)=-9-40i$$. You may set it equal to $$(a+bi)^2=a^2-b^2+2abi$$, which gives $$ab=-20$$ and $$a^2-b^2=-9$$. Now you have a real quadratic equation. Solving it gives $$\sqrt{-9-40i} = a+bi = 4-5i$$.</p>"
"When and how to use &#34;Without loss of generality&#34; in proof: <p>My understanding: When you can have option to continue your proof by choosing an option from a list of option that will give you the same result. I guess this is the best I can put it, it came a couple of time in class and I want to learn how to use it.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4v5pzre7le%2Fbbe0a77d2822824a04bda6f87209a4df604871bab2cf0254818bad051fe27d59%2Fimage.png"" alt=""image.pngNaN"" width=""781"" height=""566"" /></p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4v5pzre7le%2F06f558acfa231e70114f6674dc45e0022a1a07ceae618b24dce3958490d9d8c5%2Fimage.png"" alt=""image.pngNaN"" width=""791"" height=""570"" /></p>","<p>Yes, you have the right idea.</p>
<p></p>
<p>Most of the time, we use WLOG when there is a lot of symmetry in our proofs.</p>
<p></p>
<p>For instance, if we know that one of $$\vec x$$ or $$\vec y$$ has a specific property, but we don&#39;t care which of $$\vec x$$ or $$\vec y$$ it is, then we will write &#34;WLOG, suppose $$\vec x$$ has [that property]&#34;. You know it is safe to use WLOG if a proof for if $$\vec y$$ had that property instead would be the exact same.</p>"
"Clarification on 0 is an eigenvalue of A iff A is not invertible: <p>In the forwards proof, it mentions that</p>
<p>$$\exists \vec{x} \in \mathbb{F}^n, \vec{x} \neq \vec{0},$$ such that $$A\vec{x} = 0\vec{x} = \vec{0} \Rightarrow$$ Null(A) $$\neq \{\vec{0}\}$$</p>
<p></p>
<p>I&#39;m confused on the part where it implies the Nullspace of A is not equal to the zero vector. Is it because there is more than one way for the matrix vector product to equal 0 (ie. more than one way to map to 0)? </p>","Yes. In particular, we know that $$\vec x \in \text{Null}(A)$$ because $$A \vec x = \vec 0$$, and we know that $$\vec x \neq \vec 0$$, so we can conclude that $$\text{Null}(A) \neq \{\vec 0\}$$."
"[pp Q5]: <p>Hi, can anyone explain a little more about why we could get det(A) = 0 through the det(A) = (-1)$$^{n}$$ det(A), is it simply because here we have det(A) = -det(A), which gives us det(A) can only be 0? And is it because we conduct n times ERO (mult.) that makes det(kA) = K$$^{n}$$ det(A)<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F3944c92a13fcf78fd461e40d03bf30d36056f373cceccabf01c800fe05a82a35%2F735e15b893bcf58466091053289cfeb.png"" alt=""735e15b893bcf58466091053289cfeb.pngNaN"" /></p>
<div>
<div></div>
</div>
<div>
<div></div>
</div>","<p>Yes, we know that $$n$$ is odd, so $$(-1)^n = -1$$. Then, $$\text{det}(A) = -\text{det}(A)$$, which implies that $$\text{det}(A) = 0$$ ($$0$$ is the only number which is both nonnegative and nonpositive).</p>
<p></p>
<p>---</p>
<p></p>
<p>To add on, you are right that we can conduct $$n$$ ERO by multiplying each row by $$k$$, hence</p>
<p>$$\text{det}(kA) = k^n \text{det}(A)$$</p>
Yes, we know that $$n$$ is odd, so $$(-1)^n = -1$$. Then, $$\text{det}(A) = -\text{det}(A)$$, which implies that $$\text{det}(A) = 0$$ ($$0$$ is the only number which is both nonnegative and nonpositive)."
"PP7 Q6: Hi, wondering if it is correct to conduct R1-&gt; R1&#43;R3-R2, R2-&gt; R2&#43;R1-R3, and R3-&gt; R3&#43;R2-R1 these 3 steps at the same time, which gives us$$\begin{bmatrix} 2a & 2p & 2u\\ 2b & 2q & 2v\\ 2c & 2r & 2w \end{bmatrix}$$. (however, I think I am doing it wrongly because<br />I can not deduce the det($$\begin{bmatrix} 2a & 2p & 2u\\ 2b & 2q & 2v\\ 2c & 2r & 2w \end{bmatrix}$$) into 2det$$\begin{bmatrix} a & p & u\\ b & q & v\\ c & r & w \end{bmatrix}$$ directly)<br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2Fd72169cfaeb2a0f3a43385900dac0efc5073829dbe042381a472557d053d77fe%2Fimage.png"" alt=""image.pngNaN"" />","<p>You need to be careful about doing multiple row operations at the same time if you are using rows which have been modified.</p>
<p></p>
<p>When you perform $$R_1 \to R_1 + R_3 - R_2$$ and $$R_2 \to R_2 + R_1 - R_3$$, it is unclear what the resulting matrix is, because the first operation depends on $$R_2$$, and the second row operation depends on $$R_1$$, so the order of row operation here matters.</p>
<p></p>
<p>Moreover, I believe what you are describing is using the pre-modified value of $$R_1$$ in the row operation $$R_2 \to R_2 + R_1 - R_3$$. This isn&#39;t permitted, and where your error comes from.</p>
<p></p>
<p>In short, when you are modifying many rows, you should only do it if the order in which row operations are performed do not change the outcome (e.g. $$R_1 \to \alpha R_1$$ and $$R_2 \to R_2 + R_3$$ can be performed together because they do not influence each other).</p>
Yeah, you cannot perform all these operations at the same time. This will (likely or definitely? Not sure.) lead to a different result (as you have seen here). <div><br /></div><div>Your way would result in bringing a 2 out from each row, so you would have $$8\cdot \det$$</div><div><br /></div><div>If you want some intuition for why this doesn’t work, I’d be happy to expand on my ideas for that </div>"
Coverage For Quiz 4: What will the coverage for quiz 4 be? ,"It&#39;s up to and including Section 7.6
See the announcement on LEARN. "
"Row scaling and determinanent: <p>How multiplying a 2 gives equality? Isn&#39;t the circled 2 an 1/2 because we multiply 1/2 to row 1? But wouldn&#39;t multiplying a 2 gives twice the determinant?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2Fae88208a581928d273c25a627258e2fcd17e88d278fcc0f32ffde5b89214bff4%2Fimage.png"" alt=""image.pngNaN"" /></p>","Thé effect of row scaling on the determinant is kind of like « undoing » what you did, so you multiply by the inverse. <div><br /></div><div>Say A is the original matrix and B is the one we got from performing a row scale operation: </div><div>$$\det(B) = c\det(A)$$</div><div><br /></div><div>So if we want to solve for A, the original matrix, we have to rearrange:</div><div><br /></div><div>$$\det(A) = \frac{1}{c}\det(B)$$</div>"
"wp8 q8 solution2: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fb5040f295f060e0865952e8cd861cbc59b6f5bea1ffa564e95386ee76f306afb%2F83FA189F-DC99-45B6-8B8A-2204704E90FA.png"" alt=""83FA189F-DC99-45B6-8B8A-2204704E90FA.pngNaN"" />I cannot fully understand how this idea generated, can anyone explain a little bit to me? Thanks!!","<p>We want to find a matrix $$A$$ that has no real eigenvalues, but $$A^3$$ has real eigenvalues. By Q4, we know that if $$\lambda$$ is an eigenvalue of $$A$$, then $$\lambda^3$$ is an eigenvalue of $$A^3$$. Thus, we want to choose $$\lambda$$ as some non-real value, but we also want to make it so $$\lambda^3$$ is real. Thus, we look to the roots of unity, and we try to manipulate the characteristic polynomial so that we can fit a third root of unity as an eigenvalue.</p>
<p></p>
<p>Here is another approach you could take. My first example of a matrix with no real eigenvalues is a rotation matrix (by a nontrivial rotation). Can you pick a rotation matrix so that $$A^3 = I$$? Why would such a rotation matrix have nonreal eigenvalues, and how does this imply that $$A^3$$ has real eigenvalues? You should try writing out the rotation matrix.</p>"
"Reducing complex matrices for eigenspaces: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa9q3bxirv%2F39063cf3544d5c75933a8ce407eb95aea2342a1b58b07bc1052fec6d42dbdbe6%2Fimage.png"" alt=""image.png"" />   </p>
<p>Row reducing with complex seems very time-consuming and error-prone for a 3x3 matrix in $$\mathbb{C}$$. Is there a faster way to do this? On a test, would we be expected to be doing all of them to find all eigenvectors?</p>",There isn&#39;t a faster way to do this.  Because it is time consuming we probably wouldn&#39;t ask for only a subset of such a question on a test. 
"Linear transformation is a function: <p>We have looked at the idea of a linear transformation being onto, onep-to-one and having a range. Could I think about linear transformation as just a function? If they are the same, is there a reason why we use the word transformation and not function.</p>
<p>If they are not the same, is what is the difference between them that makes a function a function, a linear transformation a linear transformation?</p>","<p>Yes, a linear transformation is a function.  It is a function between two vector spaces (a term we will define in chapter 10)  that preserves addition and scalar multiplication.  </p>
<p></p>
<p>Not all functions are between vector spaces whereas linear transformations are between vector spaces. </p>
<p></p>
<p>So linear transformations are a subset of functions. </p>
<p></p>
<p>(The words transformation and function are pretty much synonyms, but a transformation usually has some sort of geometric understanding connected to it.)</p>
<p></p>
<p>Maybe some of the other instructors will chime in here to add more. </p>"
"Characteristic polynomial over C: <p>So we include repeated lambda values when we are calculating Cn and C0? I&#39;m having a little bit of trouble understanding the last paragraph of this definition. </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fa858738cfec1dfb3ca5f9279e8ddd0b576f133828950577c63043466f1c0752a%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>When calculating the sum in (a) and the product in (b) you need to calculate the sum and the product of all the roots of the polynomial including repetition.  </p>
<p>For example,  you have may roots of the polynomial as 2, 2, and 3.  This means the eigenvalue 2 was repeated twice.  So the sum of the eigenvalues is 7 and the product of the eigenvalues is 12.  So $$c_2= 7$$ and  $$c_0=12$$.  <br /><br />$$(-1)^3(\lambda-2)^2(\lambda-3)= -\lambda^3+7\lambda^2-16\lambda+12$$ has roots 2, 2, and 3 and the coefficients match what we have above. </p>"
"Features of the Characteristic Polynomial: <p>Is this proposition only useful for checking if our calculations are correct? Cause unless the character polynomial is very short I don&#39;t see how we can rely on this.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2F634ab080d169dc327d32abcbdc0f62bc720089041b57a2bcc2d4e344cd50a03b%2FScreen_Shot_2024-03-16_at_12.31.18_PM.png"" alt=""Screen_Shot_2024-03-16_at_12.31.18_PM.pngNaN"" /></p>","Yes, it gives a bit of a check on your calculations. "
"theorem reference: On assessments, for any time when we are using a theorem or proposition, is it okay to just say &#34;from theorem in course notes&#34; or do we always have to give it the specific name of the theorem/proposition?",“from theorem in class” is fine
Adjugate and Determinant: Does the determinant of the adjugating function give you the same determinant as the original function?,"<p>$$\text{det}\left(\begin{bmatrix} a & b \\ c & d \end{bmatrix}\right) = ad - bc = da - (-c)(-b) = \text{det}\left(\begin{bmatrix} d & -b \\ -c & a\end{bmatrix}\right)$$</p>
<p></p>
<p>so yes, same determinant for $$2 \times 2$$.</p>
<p></p>
<p>In general, we know that</p>
<p></p>
<p>$$A \text{adj}(A) = \text{det}(A) I_n$$</p>
<p></p>
<p>Hence taking determinants on both sides:</p>
<p></p>
<p>$$\text{det}(A)\text{det}(\text{adj}(A)) = \text{det}(A \text{adj}(A)) = \text{det}(\text{det}(A) I_n) = \text{det}(A)^n$$</p>
<p></p>
<p>hence we can conclude that if $$\text{det}(A) \neq 0$$, then</p>
<p></p>
<p>$$\text{det}(\text{adj}(A)) = \text{det}(A)^{n-1}$$</p>
<p>$$\text{det}\left(\begin{bmatrix} a & b \\ c & d \end{bmatrix}\right) = ad - bc = da - (-c)(-b) = \text{det}\left(\begin{bmatrix} d & -b \\ -c & a\end{bmatrix}\right)$$</p>
<p></p>
<p>so yes, same determinant.</p>"
"Number of eigenvalues for nxn matrix: <p>I am confused for this question:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweeqkowg746%2F9340dd47b73c106b5675280cb712f30fe9ee22d5c9deeca3735e04c90ef1c395%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>Solution:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweeqkowg746%2F808fe830efab137e72f7d5ebdf229b6c281c2355421c88cb4fab42c921387145%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>What do the dimensions of a matrix have to do with the number of eigenvalue solutions? How do they relate to each other?</p>
<p></p>",Try using cofactor expansion on $$\text{det}(A-\lambda I)$$. What is the highest degree of $$\lambda$$?
Det = 1: Can a matrix whose rref isn&#39;t an identity matrix have Det = 1?,"No.
Yes. 
Yes. If you have an identity matrix I, then det(I) = 1 but if you have a matrix A such that det(A) = 1 it does not necessarily mean that A is the identity matrix.
Yes. If you have an identity matrix I, then det(I) = 1 but if you have a det(A) = 1 it does not necessarily mean that A is the identity matrix.
Yes. If you have an identity matrix I, then det(I) = 1 but if you have a det(1) it does not necessarily mean that you have the identity matrix.
If the RREF isn’t the identity matrix, the matrix is not invertible, and so the determinant must be 0. "
Is it necessary to memorize the formula for calculating the determinant: I am just wondering if it is worth our time to have to formula on how to calculate the determinant memorized for expansion along the row/ column.,You should know cofactor expansion and how to use it.
"Why is the following matrix invertible: <p>The inverse of </p>
<p>\begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 1 \end{bmatrix} </p>
<p>is</p>
<p>\begin{bmatrix} 1 &amp; -1 \\ -1 &amp; 2 \end{bmatrix} </p>
<p></p>
<p>But the first matrix clearly doesn&#39;t have it&#39;s RREF = $$I_2$$</p>",Why isn&#39;t the RREF of $$\begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix}$$ the identity? 
"Valid Eigenvector?: I was redoing some examples from the course notes, and I was wondering if this is valid. In my vector, the elements are flipped in comparison to the answer in the course notes, however when I check the eigenpair, it works.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8mmxg9o651%2F2e98e1ae2e77aeb7ad72a146c252804f8a3eb78643459ebb0bca61b5c1a9eedc%2FScreenshot_2024-03-16_at_5.55.47_PM.png"" alt=""Screenshot_2024-03-16_at_5.55.47_PM.pngNaN"" width=""580"" height=""61"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8mmxg9o651%2F4ed39b45bb717e71a65fc6f50a5b24065bc4e4631654be80478b93d87c19e730%2FScreenshot_2024-03-16_at_5.56.30_PM.png"" alt=""Screenshot_2024-03-16_at_5.56.30_PM.pngNaN"" width=""562"" height=""331"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8mmxg9o651%2Fda2ea2f3538b02b8d85a6645ea74385e9d63f40de424b40a3477546d30fdce64%2FIMG_91436B5B8617-1.jpeg"" alt=""IMG_91436B5B8617-1.jpegNaN"" width=""480"" height=""351"" />","Yes this is fine, any (non-zero) scalar multiple of the vector in the span of the eigenspace will also be a valid choice. Yours is just $$-1 \cdot$$ their vector so it’s fine!<div><br /></div><div>I probably would’ve solved it how you did, so I’m not sure why they chose to solve this way, but again it doesn’t matter</div>"
"Eigenvector: If we have a repeated eigenvalue, does it mean we also have a repeated eigenvector, where repeated ones count?","<p>In general, we don&#39;t tend to count eigenvectors because for each eigenvalue $$\lambda$$, if $$\vec v$$ satisfies</p>
<p></p>
<p>$$A \vec v = \lambda \vec v$$, then</p>
<p></p>
<p>$$A (c \vec v) = \lambda c \vec v$$ for all $$c \in \mathbb R$$</p>
<p></p>
<p>hence $$c \vec v$$ is also an eigenvector for that associated eigenvalue.</p>"
"Eigenspace and Characteristics Polynomial: Is it true that the difference between eigenspace and characteristics polynomial lies on the fact that eigenspace only uses nullspace, not determinant, whereas characteristics polynomial uses determinant. I want to ensure that I accurately distinguish between the two?","<p>There is a big difference between eigenspaces and characteristic polynomials.</p>
<p></p>
<p>The characteristic polynomial is a polynomial function that tells you about the eigenvalues of a specific $$n \times n$$ matrix $$A$$; specifically, that the roots of the characteristic polynomial are the eigenvalues. The characteristic polynomial is calculated through the use of the determinant.</p>
<p></p>
<p>An eigenspace for an eigenvalue $$\lambda$$ is the set of all associated eigenvectors for $$\lambda$$. To solve for the eigenspace, you would solve the homogenous system $$(A - \lambda I)\vec x = \vec 0$$.</p>"
"How do we get this E2?: <p>This question is from Sec002 Lec23. I think Null(A-2I) should be vector 0? How do we get this span?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwai8f51s2ag%2Fd276714323b215a8078e15e7c128bc91c9d7d7b30b55d018b4addabc1d2163ee%2F_____2024-03-16_190727.png"" alt="""" /></p>","Yes, I just checked the matrix displayed there and it has an RREF of the Identity matrix, so it should only have one homogenous solution ie $$\vec 0$$. <div><br /></div><div>However, it looks like this is a continuation of an example from a previous slide, maybe there was a transposition error with the matrix? Ie check all the entries— I can also take a look if you’d like <div><br /></div><div>Here’s a screenshot from my calculator reducing the matrix:</div><div><br /></div><div><img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqw8iusqnz5hk%2Fpvowpkcpbemr%2FIMG_2220_2024_03_16.png"" /><p></p><br /></div></div>
<p>There is a typo here.  The middle entry of the matrix should be -2. </p>
<p></p>
<p>We are finding eigenvectors of the matrix $$\begin{bmatrix}0 & -2 & -3 \\ -2 & 0 & -3 \\ -2 & 2 & 1 \end{bmatrix}$$.</p>"
"Connection between invertibility, determinant and eigenvector: If we do not have a unique solution(ie zero vector is the only solution) to Ax = 0 then A is not invertible. Furthermore, the determinant is zero. and we have an eigenvalue of 0. Anything else I could add? ",All of the invertibility criteria of Theorem 5.4.7.
"Eigenpairs for repeated eigenvalue: <p>If we had repeated eigenvalues like this question, what would be the final eigenpairs for lambda = 2? Does the course note suggest that we will end up having: $$\left( 2, \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix} \right) and \left(2, \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}\right)$$</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwawbisf44qe%2F4fc59d97b39ad3568889cb5bfbff5e148a796dc20816ec955574024e9c303c76%2Fimage.png"" alt=""image.png"" width=""716"" height=""161"" /></p>
<p> <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwawbisf44qe%2F5f6d82ba3ff4596815375d7cf7a4f906bd02a101d756517c4eb1207f5a32f674%2Fimage.png"" alt=""image.png"" width=""735"" height=""576"" /></p>
<p></p>
<div>
<div></div>
</div>","If you choose any values for $$t$$ and $$u$$, you will get an eigenvector corresponding to $$\lambda=2$$.  There are an infinite number of eigenpairs. "
"[007] Lecture 24 - Diagonalization: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkesz5s9tccz3qk%2Fc132f53342070bffb2c1fdc03b5e61be4f51592375f4d4bda079a8564adafd7d%2Fimage.png"" alt=""image.pngNaN"" width=""772"" height=""239"" /></p>
<p></p>
<p>How are we able to say that Q^-1 * P^-1 = (PQ)^-1? Wouldn&#39;t swapping the positions of Q^-1 and P^-1 not be equivalent because matrices are not commutative? Or is there a proposition in the textbook or elsewhere that I missed?<br /><br />I know that with transposes Q^T P^T = (PQ)^T but I am not sure if a result like that is even relevant to this problem.</p>","<p>Think about the inverse of the matrix $$PQ$$, if $$P^{-1}$$ and $$Q^{-1}$$ both exist.</p>
<p></p>
<p>Using associativity of matrix multiplication, we can evaluate</p>
<p></p>
<p>$$Q^{-1}P^{-1}PQ = I$$ and $$PQ Q^{-1}P^{-1} = I$$</p>
<p></p>
<p>so by the uniqueness of an inverse matrix, we can conclude that the matrix $$Q^{-1}P^{-1}$$ <em>is exactly</em> the inverse of $$PQ$$. That is, that</p>
<p></p>
<p>$$(PQ)^{-1} = Q^{-1}P^{-1}$$</p>
<p></p>
<p>(keep in mind that this is only true if $$P^{-1}$$ and $$Q^{-1}$$ both exist).</p>"
fixed point: Fixed points are vectors that have an eigenvalue of 1 but it might not necessarily be an eigenvector. Is that right? Can the zero vector be a fixed point ?,"Yes, the zero vector is always a fixed point even though it is not an eigenvector. This is the only case where a fixed point is not also an eigenvector. In other cases, the eigenvalue would be 1."
Eigenpair and eigenspace: What is the difference between eigenpair and eigenspace?,"<p>The eigenpair is just a way for us to group an eigenvalue and a corresponding eigenvector together</p>
<p></p>
<p>The eigenspace is a subspace that contains all the eigenvectors associated to a given eigenvalue (and it contains $$\vec 0$$, which is not an eigenvector but allows the eigenspace to be a subspace)</p>
<p></p>
<p>Does that answer your question? </p>"
"Notebook question 7.4.1: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb54jfba6ac%2Facf9e015bc83b01bf68341769cb610485c46000dd4b3b39d7ee2102507923f21%2F3ae237670aa1ee63f46b07f5a27a7d1.jpg"" alt=""3ae237670aa1ee63f46b07f5a27a7d1.jpgNaN"" /></p>
<p></p>
<p>I am wondering for the situation where λ=-1, we row reduces to[1 1\0 0][x1\x2], but why our final solution is [1\-1] instead of [-1\1]? Aren&#39;t we supposed to let t=x2 as x2 is a free variable?</p>","@854, not entirely sure why they chose to answer that way but both ways are valid! "
"Week 7 PP - Q8: <p>For this question, the idea is just that if n is even, the determinant is the product of the diagnonal entries. If n is odd, apply a negative, right?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Ff0c608c538e4b582d88a7a5ce63321ca538518ce2580e02db625f040c0d8243d%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fdaf6450af6fec527776a7aa6a6de0df4caa76f51222249918ebd2a66d6d7a26b%2Fimage.png"" alt=""image.pngNaN"" /></p>",Yes.
"tr(A) = tr(B): I can prove for the determinant by using the determinant of a product, but I am not too sure about how I can proceed with tr(A) = tr(BO since there is not really a way to seperate PAP^-1 like I did with the determinant.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4v5pzre7le%2Fa7852aeeb10ac13f56acfc4447ee2ec65f56317ff4a9a2912cb1ce2057d824aa%2Fimage.png"" alt=""image.pngNaN"" width=""981"" height=""313"" />","<p>I haven&#39;t actually gone through this, but have you proven exercise b) yet? If you have, then I think you could just use this proposition (7.3.4) to conclude that the traces must also be equal:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F29103c654591d425ef1535c176ebeba89a377f17dee23fc7153c1cce524dcbaa%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>As another hint, can you prove that</p>
<p></p>
<p>$$\text{tr}(AB) = \text{tr}(BA)$$</p>
<p></p>
<p>for two square matrices $$A, B$$?</p>"
"difference between finding eigenvalue and eigenvector: I struggle with knowing the difference in finding eigenvectors and eigenspaces, can somebody explain the difference?","An eigenspace is the set of all vectors which are eigenvectors for a specific eigenvalue $$\lambda$$. Finding an eigenspace requires giving the entire solution set to $$(A - \lambda I)\vec v = \vec 0$$, whereas finding an eigenvector requires finding just one solution to $$(A - \lambda I)\vec v = \vec 0$$."
"Diagonalization: Let A and B be two square matrices, if A is similar to B, can I state B is also similar to A?","<p>Yep! see the remark under the definition of similar matrices:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2Fe01da5742dd27d1193670a45f51ebe02e52a0e21a7e1a1f2dd119f2b33896e11%2Fimage.png"" alt=""image.pngNaN"" /></p>"
"eigenvalue and eigenvector: Is it true that we only use determinant to find eigenvalue, and finding eigenvector doesn&#39;t need eigenvector?","What I meant is that finding eigenvector doesn&#39;t require finding determinant?
<p>If you have an eigenvalue $$\lambda$$, and you want to find an eigenvector for $$\lambda$$, then you want to find a nonzero vector $$\vec v$$ so that</p>
<p></p>
<p>$$A \vec v = \lambda \vec v$$</p>
<p></p>
<p>To solve this, you can solve the system</p>
<p></p>
<p>$$(A - \lambda I)\vec v = \vec 0$$.</p>
<p></p>
<p>So you do not need to take any determinants.</p>"
Diagonalization and determinant: Why does &#34;A is not invertible&#34; imply &#34;0 is not an eigenvalue&#34;?,"Nevermind I figured it out.
<p>You have the wrong theorem.</p>
<p></p>
<p>The theorem should state</p>
<p></p>
<p>$$A$$ is invertible if and only if $$0$$ is not an eigenvalue.</p>"
nxn matrix has at most n eignenvalues: Could I just state that a nxn matrix have at most n eigenvalues without proof?,"<p>Seems fair enough to me.</p>
<p></p>
<p>If you want to be really rigorous about it, you can cite 7.3.4 to say that the characteristic polynomial is an nth degree polynomial, then you would have to backtrack to math 135 with Proposition CPN on page 185 to say that means there would be $$n$$ complex roots. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F72cbe7655be0e013040de1bd43958af0e482be421400a9d0972800ac42e4b029%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F811a0da32dafd93460ce53759ca8e598abe443179140269b58bc6acb4cbe6a8e%2Fimage.png"" alt=""image.png"" /></p>
Yes, you can simply state that the characteristic polynomial is of degree $$n$$ and has at most $$n$$ roots. "
"No two eigen values can correspond to the same eigenvalue: I am not sure how you can have two eigenvalues that correspond to the same eigenvector, however, in the solution to Q3 recommended it almost seemed like that is what they have done. I am not sure if my understanding is correct. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4v5pzre7le%2F37499951f9fe706d47c3bb445092079df5e1bfa37a8c58f2d812814726f9ad14%2Fimage.png"" alt=""image.pngNaN"" />","<p>$$A$$ and $$B$$ can be different matrices, but we assume that $$\vec v$$ is an eigenvector for both of them. So we state two different eigenvalues ($$\lambda$$ and $$\mu$$) that correspond with $$\vec v$$, but for different matrices. </p>
<p></p>
<p>I believe you are correct that a given eigenvector will only have 1 corresponding eigenvalue, but I am not 100% sure. </p>
<p></p>
<p>Does that answer your question? If not, could you be more specific about where in the solution they appear to have done what you suggested?</p>
For a given matrix $$A$$ an eigenvector will correspond to one eigenvalue since  $$A \vec{v}= \lambda \vec{v}$$.  However, as the student answer points out a vector may be be an eigenvector for two different matrices and then it may have a different eigenvalue for each matrix.
For a given matrix $$A$$ an eigenvector will correspond to one eigenvalue since  $$A \vec{v}= \lambda \vec{v}$$.  However, as the student answer points out a vector maybe be an eigenvector for two different matrices and then it may have a different eigenvalue for each matrix. "
"wp8 q9 solution2: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2F6ac6746ff4409fa0af2cfe77015f459c69e44d7233a584c6f4068a261b11eeeb%2Fimage.png"" alt=""image.pngNaN"" /><br />I&#39;d like to know how this equation is generated, thanks!!","The third roots of unity are the roots of $$z^3-1=(z-1)(z^2+z+1)$$.  The real third root is 1, and the other other two non-real third roots are roots of $$z^2+z+1$$. "
"[PP8] Q7: Hi, wondering if we have these already, do we still need to proceed with the contradiction method?<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2Fb7b72efcab6103046768363e2ef118d45e319f45d3dc47beee6d344bc99eb77e%2F1710706730125.png"" alt=""1710706730125.pngNaN"" /><br /><br />(the contradiction part is as followed here:)<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F8200215548552c93fe4b020e05dea82597843c19e7d7a2776382a087c5ea71f8%2F1710706774216.png"" alt=""1710706774216.png"" /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F8840330621852d183c88fa27e22773f3c125ecf512e21708d43fa304bb7d2bbd%2F1710706801771.png"" alt=""1710706801771.png"" />","I&#39;m not sure I understand your question.  They make a claim that $$\lambda_1=\lambda_3$$ and $$\lambda_2=\lambda_3$$ and then prove that claim.  By proving that claim, they have proved that $$\lambda_1=\lambda_2$$.  (The proof by contradiction is their proof of the claim.)"
"Week 8 - Q2: <p>I understand that det(A) = det(A^T) but why were we allowed to use it here because det(A &#43; B) != det(A) &#43; det(B)</p>
<p></p>
<p>To further clarify my question, I feel like solution asserts the idea that A = A^T INSIDE the det(...) part.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F2c9cda2493774016f59c1f0a43d8961246b75763e76b3818255eefccf524b609%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>I can sort of see where you&#39;re coming from here, and you are correct that $$\det(A+B)\neq \det(A) + \det(B)$$, however that is not what is going on here</p>
<p></p>
<p>We know that $$\det(A)=\det(A^T)$$, however that can&#39;t help us directly since we need $$\det(A-\lambda I)$$. Conveniently, we also have that $$(B+cD)^T=B^T+cD^T$$.</p>
<p></p>
<p>So we can use both of these facts at the same time:</p>
<p>$$\det(A-\lambda I) = \det((A-\lambda I)^T)$$ (I assume we agree up to here)</p>
<p>$$= \det(A^T-(\lambda I)^T)$$ (using the $$(B+cD)^T$$ property stated above)</p>
<p>$$=\det(A^T-\lambda I)$$      (since $$(\lambda I)^T$$ is just $$\lambda I$$ because $$I$$ is diagonal)</p>
<p></p>
<p>Reading this over, it seems like I repeated exactly what the solution says, so not sure if this is entirely helpful, but just as an extra note:</p>
<p></p>
<p>$$\det(A) = \det(A^T) \not\Rightarrow A=A^T$$. We are only allowed performing this operation inside the determinant. </p>"
"How to know the root ?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F4d90df5d6f3cd9f0a2207d6b10aaf8e2bbe240ceef30dc3cc16f5a725f2543df%2Fimage.png"" alt=""image.png"" />","See the student respond below.
Factoring:<div>(a - 1)(a - 3)</div><div><br /></div><div>So a root is a = 1.</div>"
"MP8: How to go from row reduce to general solution?: <p>Hi! I I get how to go to the RREF form, yet I don&#39;t know how it proceeds to the general solution.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwax6b6y44vy%2Ff3b186bfb201f935f54b526b319453596c2bc26bc4cb4c70969ed344d14d357e%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwax6b6y44vy%2F81fd624ca636f8a39bae7536a0d99c880e6e7648cc6758d29748d9c2df21a3b0%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>I understand how $$\begin{bmatrix} 1 \\ 0 \\0 \end{bmatrix}$$ would be part of the span, but where did the $$\begin{bmatrix} 0 \\ 1 \\0 \end{bmatrix}$$ come from?</p>
<p></p>
<p>Thanks!</p>","Picking up from the RREF form, we have that the nullity(A) = n - rank(A) = 2 which we also know is the number of parameters <div><br /></div><div>So we have 2 parameters, and one fixed value. Now, I’ll just take the matrix and put it back into equation form:</div><div><br /></div><div>Row 1: $$0x_1 + 0x_2 + 1x_3 = 0$$. From this we know that $$x_3 = 0$$</div><div><br /></div><div>Then, we have two rows of just 0’s, so it doesn’t matter what we make $$x_2$$ and $$x_1$$. So, we make them parameters! Let $$x_1 = t$$ and $$x_2 = s$$. So we write out the span as </div><div><br /></div><div>$$Span\{\begin{bmatrix}{t \\ 0 \\ 0}\end{bmatrix}, \begin{bmatrix}{0 \\ s \\ 0}\end{bmatrix}\}$$ which then simplifies to what they have as the span!</div><div><br /></div><div>Basically it’s a matter of how many parameters we have</div>
Picking up from the RREF form, we have that the nullity(A) = n - rank(A) = 2 which we also know is the number of parameters <div><br /></div><div>So we have 2 parameters, and one fixed value. Now, I’ll just take the matrix and put it back into equation form:</div><div><br /></div><div>Row 1: $$0x_1 + 0x_2 + 1x_3 = 0$$. From this we know that $$x_3 = 0$$</div><div><br /></div><div>Then, we have two rows of just 0’s, so it doesn’t matter what we make $$x_2$$ and $$x_1$$. So, we make them parameters! Let $$x_1 = t$$ and $$x_2 = s$$. So we write out the span as </div><div><br /></div><div>$$Span{\begin{bmatrix}{t \\ 0 \\ 0}\end{bmatrix}, \begin{bmatrix}{0 \\ s \\ 0}\end{bmatrix}}$$ which then simplifies to what they have as the span!</div><div><br /></div><div>Basically it’s a matter of how many parameters we have</div>"
"Q5 w7: <p>How did you know that det(A) = 0 at the last ?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F4d20f5d48d19354ce0ebad93ccbb688c08182aec66d4c71fe66c281723b3e6d7%2Fimage.png"" alt=""image.png"" /></p>","Let $$a \in \mathbb R$$ be arbitrary. If $$a = -a$$, then $$a = 0$$. (why?)"
eigenvalue problem: I&#39;d llike to know in what condition determinant of the matrix equals to the multiplication of all eigenvalues. Thanks!!,"See Corollary 7.3.7 in course notes. Determinant of a matrix equals the product of <em>all <strong>complex</strong> eigenvalues (with multiplicity)</em>.
See Corollary 3.7.3 in course notes. Determinant of a matrix equals the product of <em>all <strong>complex</strong> eigenvalues (with multiplicity)</em>. "
"how to know what row/col to expand by for determinants: for example, warmup 1g) says we clearly want to expand along the third column but how do we know that?",usually nice to choose the row or col with the most 0’s so we dont have a lot of calculations to do
"WP8 Q1C: Hello, I&#39;m confused by the solution to 1C. Specifically, I don&#39;t understand how they got from the second step to the third (with the third step involving the double summation). <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8j0ruc25io%2F1f9e47a8db804099974bace6409a0b294a0ad7249d240dd44dfdf9e75a611145%2FScreenshot_2024-03-17_at_11.24.58_PM.png"" alt=""Screenshot_2024-03-17_at_11.24.58_PM.png"" />",What is the $$ii$$-th entry of the matrix product $$AB$$? It is the dot product of the $$i$$-th row of $$A$$ with the $$i$$-th column of $$B$$.
"Cramers rule question: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F0382697ad44026c3c356c13c5d67ff10752dc6b2184282ba809685a6452af404%2Fimage.png"" alt=""image.png"" width=""629"" height=""341"" /></p>
<p>Is there any easy way of doing this, or are we just supposed to calculate every determinant?</p>","<p>Realistically, you would use an online calculator to compute this (if this is for practice for using Cramer&#39;s rule; better would be to directly use an online calculator to solve the system), but if I had to compute this by hand, I would calculate the determinants of all of the $$3 \times 3$$ and then $$4 \times 4$$ minors of $$A$$ to make this easier.</p>
<p></p>
<p>We will never ask you to compute the determinant of a $$5\times 5$$ matrix or larger, unless there is a pattern to the matrix.</p>"
"Do determinants being equal imply that the Eigenvalues are equal?: <p>For this question, why must the Eigenvalues be the same? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fa598fa6a4be8ee12d02fa7733d4e00a4eadda438b7dde53b6e34e563e33eeda3%2Fimage.png"" alt=""image.pngNaN"" /></p>","If the matrices have the same characteristic polynomial, then the roots of this polynomial are the eigenvalues of both matrices.  In other words, the matrices have the same eigenvalues. "
"WP8 Q7: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdqj252r1jl%2Fd0cea6bc3591ae1c41f9c5b3be1452f2918ddec7523ae7875416f29986825798%2FScreen_Shot_2024-03-18_at_10.52.13_AM.png"" alt=""Screen_Shot_2024-03-18_at_10.52.13_AM.pngNaN"" /></p>
<p></p>
<p>Once we get here can&#39;t we just state that if $$\vec{u} = c\vec{v}$$, then our eigenvectors are scalar multiples of each other, and knowing that eigenvalues map to an eigenvector and all of its scalar multiples, then is that a sufficient contradiction?</p>",I&#39;m not sure where you are deriving your contradiction. Could you elaborate further?
What does this mean?: Saw the following line in the mobius practice: A=(((A)*(t))*(e))*(x). What does it mean?,What question is this? Could you send a screenshot?
"Mobius week 8: I have been get the determinant for a while, however i keep getting something different. I got -x^3-x^2&#43;33x-63<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8wo1ykj3w%2Ff5f310f6a263fb501948d894f92d404b59eecde922a3db677f080da36dc0b5ff%2FCapture.PNG"" width=""1214"" height=""249"" alt="""" />",I am getting the solution that is written here. Can you show your work?
"Week 8 practice problem: why we can pick p as this matrix?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Ffvrtnqlxhmhc%2FIMG_0679_2024_03_18.jpeg"" /><p></p><p></p>","What question are you talking about? Your screenshot is 2 questions from week 7.
Notice that the columns of $$P$$ are just the eigenvectors we solved from the previous part. This is how we can construct $$P$$
Notice that the columns of $$P$$ are just the eigenvectors we solved from the previous part. This is how can can construct $$P$$"
Regrade update appear on LEARN: I requested A regrade for Q3 And it got approved in my mark has been changed on crowd mark But it appears to be the same on learn. I was wondering How long it would take for this mark update to appear on learn.,After all remark requests are handled.
"WP8 Q7- does this work?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fb8acfc74ab1531199933c42caec2dfb8355bc9ff8c2b7ffd1ac51d04ebc97005%2Fimage.png"" alt=""image.png"" /></p>
<p>I was wondering if we could just let another $$\lambda_x$$ so $$A(\vec{u}+\vec{v})=\lambda_x(\vec{u}+\vec{v})$$, and then we would also have that $$A(\vec{u}+\vec{v})=A\vec{u}+A\vec{v}=\lambda_1\vec{u}+\lambda_2\vec{v}$$, so we get that $$\lambda_1=\lambda_2=\lambda_x$$, but i&#39;m not sure since the solution is very long</p>","<p>How did you get $$\lambda_1 = \lambda_2 = \lambda_x$$ from</p>
<p></p>
<p>$$\lambda_x(\vec u + \vec v) = \lambda_1 \vec u + \lambda_2 \vec v$$?</p>
<p></p>
<p>I&#39;ll offer an intuitive reason why this result is true.</p>
<p></p>
<p>Consider the respective eigenspaces of $$\vec u, \vec v$$. Suppose they are different eigenspaces. Imagine &#34;joining&#34; these eigenspaces together to form a larger subspace. Since eigenspaces are subspaces, this must be closed under addition, hence $$\vec u + \vec v$$, which lives in this &#34;combined&#34; subspace must have the same eigenvalue as that of $$\vec u$$ and $$\vec v$$, if it is an eigenvector.</p>"
One dropped quiz still?: I was sick one quiz day and used the 48hr thing but i was just wondering if that quiz is now counted as my one dropped quiz or if its separate and my lowest quiz from the rest will still get dropped?,@643
"week 8 practice: can I write those vectors like [-1/2, 1]^T? Will this affect the following calculation?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fwngikvnnnhbp%2FIMG_0681_2024_03_18.jpeg"" /><p></p>","Writing vectors as horizontal vectors is simply a notational difference, and it is your choice which you choose.
That’s perfectly fine, any scalar multiple of that vector will also work for the span. <div><br /></div><div>Just be careful with all the $$\frac{-1}{2}$$ entries in the matrix because they make it easier to make mistakes </div><div><br /></div><div>The matrix $$D$$ will (probably?) look different, but it should be equivalent</div>"
"WP8 Q7 Why does this work?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwabn8bhm141%2F9dee7821422ef4c7539c22d45ff23bc26f82de7456b2609d95188dd67a9bb199%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>In the proof we made the assumption that λ_3 = λ_2, so wouldnt this mean that -(λ_2 - λ_3) / (λ_1 - λ_3) = -0/(λ_1 - λ_3) = 0, meaning u = the zero vector?</p>
<p></p>","We didn&#39;t make the assumption that $$\lambda_2 = \lambda_3$$; we are trying to show $$\lambda_2 = \lambda_3$$ and $$\lambda_1 = \lambda_3$$. By contradiction, we suppose that one of those is false; so without loss of generality, we take $$\lambda_1 \neq \lambda_3$$."
"A Result of Eigenvalues: <p>Hello,</p>
<p></p>
<p>I was wondering if the eigenvalues of an upper-triangular matrix and a lower-triangular matrix are their diagonal entries?</p>
<p></p>
<p>Is the above statement true? How do we prove it?</p>
<p></p>
<p>Thank you!</p>","<p>Yes I believe it is true, you can prove it by expanding along the corresponding row /column with almost all zeroes, I think you will get that the characteristic polynomial is of the form $$(a_1-\lambda)(a_2-\lambda)\dots(a_n-\lambda)$$, so the eigenvalues are the diagonal entries. Here is a picture of what I mean: (red is what you expand along)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F238e4b96fb055e19df640c1d5398285d806e18a41582ef2db1f66f9a6afdaf13%2Fimage.png"" alt=""image.png"" width=""285"" height=""192"" /></p>
<p>Also this may be useful (for the lower triangular, remember that $$\det(A)=\det(A^T)$$)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F46b72ba6ad01492b61f01d9ff7a618967a1bb7f5a04c6dff74a4c5dfb4fc167c%2Fimage.png"" alt=""image.png"" width=""360"" height=""165"" /></p>
<p>Yes I believe it is true, you can prove it by expanding along the corresponding row /column with almost all zeroes, I think you will get that the characteristic polynomial is of the form $$(a_1-\lambda)(a_2-\lambda)\dots(a_n-\lambda)$$, so the eigenvalues are the diagonal entries. Here is a picture of what I mean: (red is what you expand along)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F238e4b96fb055e19df640c1d5398285d806e18a41582ef2db1f66f9a6afdaf13%2Fimage.png"" alt=""image.png"" width=""285"" height=""192"" /></p>
<p>Also this may be useful (for the lower triangular, remember that $$\det(A)=\det(A^T)$$:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F46b72ba6ad01492b61f01d9ff7a618967a1bb7f5a04c6dff74a4c5dfb4fc167c%2Fimage.png"" alt=""image.png"" width=""360"" height=""165"" /></p>
<p>Yes I believe it is true, you can prove it by expanding along the corresponding row /column with almost all zeroes, I think you will get that the characteristic polynomial is of the form $$(a_1-\lambda)(a_2-\lambda)\dots(a_n-\lambda)$$, so the eigenvalues are the diagonal entries. Here is a picture of what I mean: (red is what you expand along)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F238e4b96fb055e19df640c1d5398285d806e18a41582ef2db1f66f9a6afdaf13%2Fimage.png"" alt=""image.png"" width=""285"" height=""192"" /></p>
<p>Also this may be useful:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F46b72ba6ad01492b61f01d9ff7a618967a1bb7f5a04c6dff74a4c5dfb4fc167c%2Fimage.png"" alt=""image.png"" width=""334"" height=""153"" /></p>
<p>Yes I believe it is true, you can prove it by expanding along the corresponding row /column with almost all zeroes, I think you will get that the characteristic polynomial is of the form $$(a_1-\lambda)(a_2-\lambda)\dots(a_n-\lambda)$$, so the eigenvalues are the diagonal entries. Here is a picture of what I mean: (red is what you expand along)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F238e4b96fb055e19df640c1d5398285d806e18a41582ef2db1f66f9a6afdaf13%2Fimage.png"" alt=""image.png"" width=""285"" height=""192"" /></p>"
when will WA4 be posted?: ,"It should be posted today. 
I don&#39;t see it on my LEARN account."
"WA2 Exemption: I realize I probably should&#39;ve done this earlier, but I wanted to follow up. I self-submitted an absence for the WA2 because I was sick, and I emailed my instructor about it.<br />I just wanted to make sure the exemption went through?",Did you instructor not write back?  You can also write to math 136@uwaterloo.ca.
"WA4 - Subspaces: <p>Hi, I&#39;ve been going through the notes and textbook trying to fully understand what a subspace is but I&#39;m not sure what it would mean here. When it says &#34;Let V be a subspace...&#34;, does that suggest that V = Span(v)? Or V = {v}? Or does it only suggest the definition:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fe9ba63576cf5e47a7161ba68ef9426aea5c637c52d2754fb5f3c67718da886ca%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F2e785b9a3a5826284a6abba6769379a2573a48ada3923153d3a9a041c674042d%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>The definition of a subspace is as follows:</p>
<p></p>
<p>$$V$$ is a subspace if and only if</p>
<p></p>
<p>1. $$0 \in V$$</p>
<p>2. For all $$\vec x, \vec y \in V$$, $$\vec x + \vec y \in V$$</p>
<p>3. For all $$c \in \mathbb R, \vec v \in V$$, $$c \vec v \in V$$</p>
<p></p>
<p>Which means that if $$V$$ is a subspace, then each of 1., 2., and 3. are satisfied.</p>
<p></p>
<p>An equivalent characterization of subspaces are the two criteria (a) and (b) you have posted.</p>
<p></p>
<p>It turns out that a subspace is always a span of some vectors (we will talk about this more later), but this is a useful way to think of subspaces.</p>"
"Standard basis: <p>For this question, is it possible to just put the columns into a matrix and solve for the RREF?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2F2624204a51f2e7394bb10bc68882a3c353a6b55128ad7592a6e0584cbd77a457%2Fimage.png"" alt=""image.pngNaN"" /></p>","Determining the RREF of such a matrix could be part of the solution, but how will that help you determine the values of $$a$$ so that this set will be a basis?"
WP9: when will the solution of weekly practice for week9 be posted,They are always posted on Thursday mornings and they are now there. 
WA3 marks: When can we expect the WA3 marks to be released?,"I&#39;m sure the team is working hard to get them marked! Soon probably.
I hope soon.  It seems that some of the TAs are holding things up. "
Q4: can we skip the proof and say using the proposition in the course note?,What proposition are you hoping to use?
"Question about how much work we need to show for Q1b: If we see an eigenvector by inspection, are we allowed to just state the eigenvector and show why it&#39;s an eigenvector, or do we need to find each of the eigenspaces then get a vector from that eigenspace to get full marks?","The point of this question is more about showing us that you can calculate the eigenspaces so I would do it that way, rather than by inspection.  On a test, seeing eigenvectors by inspection is very valid. "
"Typo in notes for finding basis: <p>Just making sure, is the set supposed to have $$\vec{v_3}$$?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F23eed3048085039053f3353c208c01acd45666d5cf445077dcf76005ae3e264d%2Fimage.png"" alt=""image.png"" width=""559"" height=""288"" /></p>","Yes, that&#39;s a typo. "
Quiz 4 Posted?: When can I expect the blank / printable version of quiz 4 to be uploaded to LEARN?,"<p>I&#39;ve passed this along to the person who looks after this.  I&#39;m guessing they will post it soon.</p>
<p></p>
<p>P.S.: It is available on LEARN now.</p>
I&#39;ve passed this along to the person who looks after this.  I&#39;m guessing they will post it soon. "
"Isn&#39;t the sign correct?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2Ff2a7d799c1b2ebc6b7af727bc67a28636b292ea2d3ff39466e294fdaead3704e%2FScreenshot_2024-03-22_at_11.35.57_AM.png"" width=""1692"" height=""520"" alt="""" />Oh nvm I messed up later, resolved",You&#39;re right. I just fixed your mark for you.
"Latex bonus: seems like I didn&#39;t earn latex bonus marks in this assignment (I used it), what should I do? Thank you. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwel0v1d7im%2F775d09ddebdebb3b8a9dcb866b11fdfaf4ec38397d9efc98bac10ebd5657e2d1%2Fimage.png"" alt=""image.pngNaN"" width=""1817"" height=""472"" />",I just added your typeset bonus.
"Finite Set Linear Dependence Propositions: <p>I&#39;m confused about the following two propositions:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8w0ryf67pe%2F4ff780653f61091a139559f178c5a421362b148a6f65183b39e116fd479b76fc%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>The underlying question is how can a set made from part of the span of a single vector be linearly dependent? I would imagine that in order for a set of vectors to be linearly dependent, there would have to be at least two vectors.</p>
<p></p>
<p>For a) specifically, would {s[1]: |s| &lt; 5} work as a counterexample?</p>
<p></p>
<p></p>","<p>Here, it is not specified that $$S$$ is a span of vectors.</p>
<p></p>
<p>For instance, $$S$$ could be $$\{(1, 1, 1), (2, 2, 2), (3, 3, 3)\}$$ which is just 3 points.</p>
<p></p>
<p>(a) is linearly dependent because you can choose any scalar in front of the $$0$$ vector, and choose all $$0$$s in front of the others, and you would have a nontrivial linear combination which equals $$0$$</p>
<p></p>
<p>(b) the backwards direction of (b) is given by (a)</p>
<p></p>
<p>For the forward direction, if $$x \neq 0$$, then $$a \vec x = 0$$ iff $$a = 0$$</p>"
"1d): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbue9f76p8%2Fe78bac336c9888475f463b848777e7e5061bbfcee1621bd5e889af6d53e8f6cc%2Fimage.png"" alt=""image.png"" /></p>
<p>Am I allow to simply use the fact that <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbue9f76p8%2Fc67838b71e133f0696d46232cc1bdf9c426222bb2a944f264bf669e601256034%2Fimage.png"" alt=""image.png"" /> (which is in an exercise in the course notes) or would I have to prove it via induction?</p>","Yes, you can use this fact.  You don&#39;t need to prove it. "
"8.6 in Textbook: Hello, I am reviewing section 8.6 and I&#39;m confused by this example from the textbook. The set B is shown to be a basis for Col(A), but how can this be the case if B only has 3 vectors? Does this not contradict the Spans F^n iff rank is n and Size of basis theorems? What am I missing?  <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8j0ruc25io%2F36dd60766f10ed61b152120c263cf4669a3e3fa724d6f35c38e69e50988ede60%2FScreenshot_2024-03-22_at_7.20.17_PM.png"" alt=""Screenshot_2024-03-22_at_7.20.17_PM.pngNaN"" width=""429"" height=""257"" />","Recall that a basis with vectors in $$\mathbb{F}^n$$ has size $$\leq n$$. That is, $$n$$ is the maximum it can be, but the size can be smaller than that<div><br /></div><div>You might be thinking of the theorem where a basis of $$\mathbb{F}^n$$ itself has to be of size $$n$$, but in this case we can conclude that Col(A) $$\neq$$ $$\mathbb{F}^4$$, in other words Col(A) does not span all of $$\mathbb{F}^4$$ so it doesn’t need to have 4 vectors in the basis </div>
Recall that a basis with vectors in $$\mathbb{F}^n$$ has size $$\leq n$$. That is, $$n$$ is the maximum it can be, but the size can be smaller than that<div><br /></div><div>You might be thinking of the theorem where a basis of $$\mathbb{F}^n$$ itself has to be of size $$n$$, but in this case we can conclude that Col(A) $$\neq$$ $$\mathbb{F}^n$$ </div>
Recall that a basis with vectors in $$\mathbb{F}^n$$ has size $$\leq n$$. That is, $$n$$ is the maximum it can be, but it doesn’t have to be the maximum. <div><br /></div><div>You might be thinking of the theorem where a basis of $$\mathbb{F}^n$$ itself has to be of size $$n$$, but in this case we can conclude that Col(A) $$\neq$$ $$\mathbb{F}^n$$ </div>"
"Missing a quiz, how does that change my grade?: I am not able to attend the last quiz for this term. Will one of my quiz marks still be dropped? Or will the four of the quizzes that I have already written be counted in the quiz grade.","I think if you can&#39;t attend a quiz and don&#39;t get accommodation for it, then its just a 0 mark quiz, so the lowest mark dropped would just be used for that quiz that &#34;you got 0 on&#34;, I think if you did get accommodation then it wouldn&#39;t use up the drop, but not totally sure"
"Latex bonus: I think I did not receive the typeset bonus for wa3, do I need to submit a regrade request?",Yes.
"Advice on taking Linear Algebra 2 next term?: Hi there,<br /><br />I just wanted some advice... I have a co-op term next term so I was wondering if it would be recommended to take linear algebra 2 online during co-op? The alternative would be having to take 2 math &#43; 2 cs &#43; stats and I&#39;m just not too sure if I&#39;ll be able to handle that much of a workload. I could also overload in a future term? How much of a workload is linear algebra 2 compared to linear algebra 1?",I have not taught MATH 235 but I am pretty certain that many students find it more difficult than MATH 136.  It deals with more abstract concepts.  
WA4q1b: Can we have an eigenvector that has element in terms of a and b?,"Yes. a, b, c are the given entries. Expressing the answer in terms of given parameters is good. (This is how Algebra is supposed to be.)"
Use of Exercise in Textbook: Are we allowed to use the results of exercises in the textbook without proof?,"Based off @932 I would say yes 
Yes"
"If A is diagonalizable over F, is A similar to D?: <p>If D = P^-1AP does A = PDP^-1?</p>
<p>Just wanted clarification, sorry if this is an obvious question</p>",Yep! No need to apologize :D <div><br /></div><div>Let’s start with $$D=P^{-1}AP$$<div><br /></div><div>Multiply the left side by $$P$$:</div><div>$$PD = PP^{-1}AP = AP$$</div><div><br /></div><div>Multiply the right side by $$P^{-1}$$:</div><div>$$PDP^{-1}=APP^{-1}= A$$</div><div><br /></div><div>So we arrive at the other side!</div></div>
"Subspaces: I&#39;m a bit confused on the difference between subspaces and spans, can someone clarify?<br /><br />Thanks!","A subspace is an abstract definition of this collection of vectors that:<div>a) contains $$\vec 0$$</div><div>b) is closed under addition</div><div>c) is closed under scalar multiplication</div><div>(Definition 8.1.1) </div><div><br /></div><div>We have lots of examples of this, and all of them *happen* to be spans (not a coincidence, I’ll get to that), but subspaces are that specific definition I mentioned above </div><div><br /></div><div>A span is anything of the form Span{$$\vec v_1, \ldots , \vec v_k$$}. It contains all the linear combinations of its spanning set ($$\vec v_1, \ldots, \vec v_k$$). A more concrete way of saying this is that any vector in the span is of the form $$\vec z = c_1\vec v_1 + \ldots + c_k\vec v_k$$ where $$c_1,\ldots , c_k \in \mathbb{F}$$. That’s what a span is by definition. </div><div><br /></div><div>Turns out, in 8.4 we learn that all subspaces can be represented by spans! Check out Theorem 8.4.1 to see this. Likewise, every span is a subspace, as defined in 8.1.2 part b)</div><div><br /></div><div>So spans and subspaces are characterized by different definitions, but are essentially the same</div><div><br /></div><div>Does that help?</div>
A subspace is an abstract definition of this collection of vectors that:<div>a) contains $$\vec 0$$</div><div>b) is closed under addition</div><div>c) is closed under scalar multiplication</div><div>(Definition 8.1.1) </div><div><br /></div><div>We have lots of examples of this, and all of them *happen* to be spans (not a coincidence, I’ll get to that), but subspaces are that specific definition I mentioned above </div><div><br /></div><div>A span is anything of the form Span{$$\vec v_1, \ldots , \vec v_k$$}. It contains all the linear combinations of its spanning set ($$\vec v_1, \ldots, \vec v_k$$). A more concrete way of saying this is that any vector in the span is of the form $$\vec z = c_1\vec v_1 + \ldots + c_k\vec v_k$$ where $$c_1,\ldots , c_k \in \mathbb{F}$$. That’s what a span is by definition. </div><div><br /></div><div>Turns out, in 8.4 we learn that all subspaces can be represented by spans! Check out Theorem 8.4.1 to see this. Likewise, every span is a subspace, not sure if this is explicitly in the course notes but it seems reasonable enough to prove. </div><div><br /></div><div>So spans and subspaces are characterized by different definitions, but are essentially the same</div><div><br /></div><div>Does that help?</div>"
"Q3: Hello, I was just wondering if I can solve this by setting the det(S) = 0. The solution is short but it seems to give the same answer as if I were to set the augmented matrix to 0.","<p>Yes. You need to add one sentence to justify your method. (Hint: invertibility criteria)</p>
<ul><li>S is the given set. If you want to compute determinant, you need to denote the matrix by a different letter.</li><li>That&#39;s not an agumented matrix. It&#39;s a matrix formed by columns vectors.</li></ul>
<p>Yes. You need to add one sentence to justify your method. (Hint: invertibility criteria)</p>
<ul><li>S is the given set. If you want to compute determinant, you need to denote the matrix by a different letter.</li><li>That&#39;s not agumented matrix. It&#39;s matrix formed by columns vectors.</li></ul>
Yes. You need to add one sentence to justify your method. (Hint: invertibility criteria)
You could do this, but I would suggest proving why exactly this makes sense. Not sure if the TAs would award full marks without connecting those dots explicitly. (Start with what $$\det(S) =0$$ implies for $$S$$ and go from there…)"
"Regrade Request Link: <p>Hi, </p>
<p></p>
<p>I want to ask for a regrade request but the link isn&#39;t working. </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedqw70l6xe%2F053a168c586186b9e0ea19069699ae6fc7083308fd181f3fe43883a6578454f2%2Fimage.png"" alt=""image.pngNaN"" width=""461"" height=""346"" /></p>
<p></p>
<p>If someone can update the website, that would be very helpful. Thanks! </p>","Sorry, it’s been fixed now."
"WA4 Q4: <p>What does the backwards slash &#39;\&#39; mean in this question? </p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2F2a49ca6e90336e917a4d7cbb57ab0cbf6709c8d5df90f71a8a2a3e4457632e12%2Fimage.png"" alt=""image.pngNaN"" width=""810"" height=""119"" /></p>","all the vectors in S1 and S2 without u
<p>The backslash (\setminus) denotes &#34;setminus&#34;, which is the operation of removing something from a set.</p>
<p></p>
<p>e.g.</p>
<p></p>
<p>$$\{\text{apple, orange, kiwi, watermelon}\} \setminus \{\text{apple}\} = \{\text{orange, kiwi, watermelon}\}$$</p>"
"Just checking: For Q5b, does the fact that $\vec{x}_1, \ldots, \vec{x}_k$ are linearly independent vectors still apply to (b) as well? Or is it just (a)?","Yep, applies to both
It applies to both.  The general rule is that any information given in the description that comes before the parts (called the  &#34;preamble&#34;) applies to all parts in a multi-part question. "
"Confirming matrix multiplication property: <p>Let A be a matrix, v be a vector, and c be a constant.</p>
<p></p>
<p><strong>Is it true that c(Av) = A(cv)?</strong></p>
<p></p>
<p>I think this is true <strong>if we consider v to be a 1D matrix (can we do that?)</strong>, because according to Proposition 4.3.11 (Properties of Multiplication of a Matrix by a Scalar):</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdh2s3927me%2F073888b0cd793ff37b55cc57ea3c2eee039d53d7842fd1cc1225fa6ebb4e630d%2Fimage.png"" alt=""image.png"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdh2s3927me%2Fbd2009726bd57536e6f6156f41d10517fbc886b2dacbf1fbc966f3b77ff31133%2Fimage.png"" alt=""image.png"" /></p>","Yep, it’s true!<div><br /></div><div>@559 </div>"
"When Will WA3 Solutions Be Posted?: <p>Hi. Do you know when the WA3 solutions will be posted? I just want to make sure I have a valid argument for my regrade request.</p>
<p></p>
<p>Apologies if this has been asked before. I haven&#39;t been able to find a previous post. Thanks!</p>","<p>I&#39;ll check on this.   Hopefully soon.</p>
<p></p>
<p>P.S.: Posted.</p>
I&#39;ll check on this.   Hopefully soon."
What is the coverage for quiz 5?: ,8.1-8.8.  An announcement should be made tomorrow about the quiz 5. 
"Set is linearly independent iff Matrix is not invertible?: Since from Pivots and Linear Independence we have that a set of vectors is linearly independent iff r = rank(A) = k, does this mean that if we have a set of n vectors in F^n, that a criteria for this set being linearly independent is that the nxn matrix made from the vectors in this set is invertible? In other words, is the set of n vectors in F^n linearly independent if and only if the matrix [v1 ... vn] is invertible? I vaguely remember this being discussed in lectures but I don&#39;t think it was written down formally. ",Yes! Good observation.
"Q4 WA4: For this question, I am confused about (S1 union S2) \ {u}<div><br /></div><div>Can we write it as the combination of vectors in S1 and S2 then get rid of set u?</div>","<p>I don&#39;t understand your question.</p>
<p></p>
<p>$$(S_1 \cup S_2) \setminus \{\vec u\}$$ denotes the set of vectors $$S_1 \cup S_2$$, with $$\vec u$$ removed. The definition is given in the question:</p>
<p></p>
<p>$$(S_1 \cup S_2) \setminus \{\vec u\} := \{\vec v_1, \ldots, \vec v_k, \vec w_1, \ldots, \vec w_\ell\}$$</p>"
Is it still necessary to cite simpler properties: Like properties of multiplication of a matrix by a scalar? ,"No, unless certain steps in your proof seem unclear or hard to follow."
"Showing RREF work: At this point in the course for this WA when finding an RREF, do we still need to show the full work (as in in showing what changes we are making each step)","You don&#39;t have to write out every step, but please don&#39;t write &#34;The RREF of this matrix is ...&#34;"
"Q4 WA4: Since it&#39;s been proven in class, can we say without proof that the subset of a matrix is linearly dependent, then its superset is also linearly dependent?","<p>What do you mean by the subset of a matrix? </p>
<p></p>
<p>We proved in class that if a set is linearly dependent, then any superset of that set will be linearly dependent.  You can use that fact. </p>"
"Notation for zero vector in n-dimensional vector space: For $$n\times 1$$ zero vector we can write $$\mathcal{O}_{n\times 1}$$, but is it acceptable to represent the same vector as $$\vec{0}_n$$ (similar to the zero square matrix)?","An instructor will have a more definitive answer for this, but I don’t recall us ever using a subscript to denote the length of a vector. In most (if not all) contexts, it would be pretty clear what the length of the vector is so I’m not sure if there would be an advantage of using $$\vec 0 _n$$ over just $$\vec 0$$ "
"How do we know the basis for Null(A) is LI?: How do we know that the free variable columns are LI? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fb9c3f584eba4b8c870bb600f8ee25cdbb433603f6ed6602a8d381f02c651fc06%2Fimage.png"" alt=""image.pngNaN"" />","Think about setting a linear combination of them equal to the zero vector and solving for the scalars.  In this system of equations, for each variable that is a free variable, the components in the vectors will all be 0 except for in one vector where it will be a 1.  Therefore, the linear combination will have to have all 0&#39;s as the scalars and so the only solution is the trivial solution. "
"Why is this true?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fa8eefc8aaf7dce02902327d9b80c1aff9adb77f6b884967b4d2bbae7fb965c58%2Fimage.png"" alt=""image.png"" />","<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fidt8cfyuv993c4%2Fd745c510f0d8d80390af6545784f2c1c07c215781c23898d81d3c331dd1c6b8f%2FScreenshot_2024-03-25_at_10.37.57_PM.png"" alt=""Screenshot_2024-03-25_at_10.37.57_PM.pngNaN"" />"
"Coordinate vector: Every vector X is a coordinate vector of X with respect to the standard basis  hence we say that it is the &#34;standard&#34; coordinate vector of X. We just never say that it is with respect to the standard coordinate, so from now on can I assume every vector written by itself(without subsrcipt , bracket)  is a coordinate vector with respect to the standard basis? Also could someone point out if my terminology is inaccurate. Greatly appreciated.","<p>Vectors are what we choose to make of them. All that vectors are are groups of numbers; it is what we choose to represent with them that matters.</p>
<p></p>
<p>The notation $$\vec x \in \mathbb F^n$$ implies that $$\vec x$$ is written with respect to the standard basis. We directly take the values of $$\vec x$$ to be what they mean. For instance, </p>
<p></p>
<p>$$\begin{bmatrix} 1 \\ -2 \\ 3\end{bmatrix}$$</p>
<p></p>
<p>is simply an object that contains three numbers: $$1, -2, 3$$.</p>
<p></p>
<p>However, if we write</p>
<p></p>
<p>$$[\vec x]_{\mathcal B} = \begin{bmatrix} 1 \\ -2 \\ 3 \end{bmatrix}$$</p>
<p></p>
<p>the vector $$[\vec x]_{\mathcal B}$$ is still the exact same vector; an object which contains three numbers: $$1, -2, 3$$. However, this notation represents something, which is that in the ordered basis $$\mathcal B = \{\vec b_1, \vec b_2, \vec b_3\}$$, we know that</p>
<p></p>
<p>$$\vec x = 1 \vec b_1 - 2 \vec b_2 + 3 \vec b_3$$</p>
<p></p>
<p>The two examples I gave you technically represent the same object, but they convey different meanings.</p>"
"Final Coverage: <div>
<div>
<div>
<div>
<div>
<div>
<div>
<div>
<div>
<div>
<div>
<div>
<p>Will the coverage for the final exam extend through the end of Chapter 9, or will it include up to Section 10.2? If it is up to 10.2, will there be no written problems for this chapter?</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>",The final will cover up to the end of chapter 9. 
"Regrade Request for Quiz 4: <p>Hi,</p>
<p></p>
<p>I went to the regrade request portal but there is no option for &#34;Quiz 4&#34;. How can I send a regrade request?</p>","<p>The portal for submitting a remark request for Quiz 4 is open now.</p>
<p></p>
<p>The remark request for WA 3 will close on Friday.</p>"
doubt about nullity and the null space: Why does having only the 0 vector in the null space of a matrix A imply that nullity(A)= 0?,"If Null$$(A)=\{\vec{0}\}$$, then this means that there cannot be any parameters in the solution set for the homogeneous equation. Since nullity$$(A)$$ is equal to the number of parameters in the solution set, this means that nullity$$(A)=0$$.
If Null$(A)=\{\vec{0}\}$, then this means that there cannot be any parameters in the solution set for the homogeneous equation. Since nullity$(A)$ is equal to the number of parameters in the solution set, this means that nullity$(A)=0$."
Question about linear independence: Are efficient spanning sets linearly independent? ,"I&#39;m guessing you got the phrase &#34;efficient spanning set&#34; from me.  This isn&#39;t a technical linear algebra term with a formal definition.  I use it to mean that every vector in the spanning set is needed to maintain the span.  If one of the vectors is a linear combination of the others, then we can see informally that anything that vector contributes to the span can be contributed by the other vectors and so that vector is redundant and can be removed from the spanning set and the span of the set won&#39;t change.  So essentially that means that any spanning set which has vectors in it that can be expressed as linear combinations of other vectors won&#39;t be an efficient spanning set.   So in other words an efficient spanning set won&#39;t have any vectors in it that are linear combinations of the others.  We do have a theorem about that and it says that such a set is linearly independent. "
"solution for WP 10: Given that the quiz is on Monday and it covers up to 8.8, Could the solution for WP 10 be posted? Many thanks.",The solutions are always posted on Thursday mornings (and they were just posted).
Can a coordinate vector be the zero vector?: ,"<p>The zero vector has the same representation in every basis (why is this the case?).</p>
<p></p>
<p>Moreover, it is clear that if $$[\vec v]_{\mathcal B} = \vec 0$$, then $$\vec v = \vec 0$$ (using that $$\mathcal B$$ is linearly independent).</p>"
"9.2.6 Proposition: <p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2Fcff9b423ec3775c1d58bd44d3bf7d06ff6adf4b1251f91e92845a1c09b89c00d%2Fimage.png"" alt=""image.png"" width=""473"" height=""266"" /></p>
<p></p>
<p>For the above proposition, I don&#39;t get why $$\begin{bmatrix} v_{i} \\ \end{bmatrix}_{B}$$ is an eigenvector of $$\begin{bmatrix} T \\ \end{bmatrix}_{B}$$. I may be getting a little too technical here, but by the definition of an eigenvector isn&#39;t $$\begin{bmatrix} v_{i} \\ \end{bmatrix}_{B}$$ an eigenvector of $$\begin{bmatrix} T(v_{i}) \\ \end{bmatrix}_{B}$$, which is only one column in $$\begin{bmatrix} T \\ \end{bmatrix}_{B}$$?</p>
<p></p>",They have proved that $$[T]_B[\vec v_i]_B= d_i[\vec v_i]_B$$.  This satisfies the definition of being an eigenvector. 
"Pivot column basis: <p>Is $$\{\vec{v}_1, \vec{v}_3, \vec{v}_4\}$$ the basis for V as column 1, 3, 4 are the pivot columns?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdhw3ida1r%2F771de757f823a90c76507bfcb992ef9fd616f87cc59b4920a8809064406ac9a1%2FScreenshot_2024-03-28_at_2.27.43_PM.png"" alt=""Screenshot_2024-03-28_at_2.27.43_PM.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdhw3ida1r%2F01f12b3d7b5c840752703903dcb88d511545c247401a9bf7e16324441103a0ea%2FScreenshot_2024-03-28_at_2.27.49_PM.png"" alt=""Screenshot_2024-03-28_at_2.27.49_PM.pngNaN"" /></p>","Yep, @921
I corrected this typo.  Is it still showing up wrong?"
Basis and LI: Do all basis need to be LI? Or only the ones that span $$\mathbb{F}^n$$,"Yes, a basis must be both a linearly independent set and a spanning set.
Isn’t a basis supposed to be a spanning set by definition?
Isn’t a basis supposed to be a spanking set by definition?"
Weekly practice: Can weekly practice solution for week 11 and 12  be posted earlier. To prepare for final,"The solutions for the week 11 practice problems will be posted on Thursday.  (It&#39;s good to do problems first without the temptation to look at the solutions.)  The week 11 practice problems cover to the end of chapter 9 which is far as the final covers.  There will be no week 12 practice problems posted.  However, there will be a final practice written assignment posted.  It is not to be handed in.  I believe it will be posted on Monday. "
"mobius week 9: <p>we can see that the only vector satisfies V is the zero vector $$\begin{bmatrix} 0\\ 0\\ 0\\ \end{bmatrix}$$ ,  c$$\begin{bmatrix} 0\\ 0\\ 0\\ \end{bmatrix}$$ = $$\begin{bmatrix} 0\\ 0\\ 0\\ \end{bmatrix}$$ .<br />Why the answer is d?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc65fwxp5ux%2Ff27d360f751d1964e8508742c7ad8414ddcc05699440f2056469373232b86df8%2Fimage.png"" alt=""image.png"" /></p>
<p></p>","<p>Each of $$x_1, x_2, x_3 \ge 0$$, so what happens when you take $$\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \in V$$ and scale it by $$-1$$?</p>
<p></p>
<p>e.g. is $$-\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \in V$$?</p>"
"Recommended Problems 1.c: <p>Hello,</p>
<p></p>
<p>In question 1 c) of the recommended problems for the written practice set, the solutions say that</p>
<p>S = {$${{\begin{bmatrix} x_{1}\\ x_{2}\\ x_{3} \\ \end{bmatrix}} \in \mathbb{R}^{3} : |x_{1}| + |x_{2}| - |x_{3}| = 0} $$} is not a subspace and they provide a counterexample.</p>
<p></p>
<p>The counterexample made sense, but I was just curious if there was a way to prove that the set doesn&#39;t have closure using general variables, instead of a counterexample (i.e. cx &#43; y), similar to what was done in part a)? If so, could someone explain what that might look like for this example?</p>
<p></p>
<p>Thanks!</p>
<p></p>","<p>In general, there is nothing wrong with a proof by counterexample (this is just good of a proof as any other proof). It demonstrates that something cannot be true because it does not account for this specific case.</p>
<p></p>
<p>However, perhaps you are wondering how they come up with certain counterexamples. For this one, the best way to work out a counterexample is just to try various numbers, or to think about this geometrically. The set $$S$$ in questions concerns all vectors in $$\mathbb R^3$$ which have the sum of magnitudes of the first two coordinates equal to the magnitude of the third coordinate.</p>"
"Take-home practice problem Lecture 28: <p>Find the basis for the subspace of $$\mathbb{R}^3$$</p>
<p>$$V = \left\{ \begin{bmatrix}x_1 \\ x_2 \\ x_3 \end{bmatrix} \right\}\in\mathbb{R}^3: x_1+x_2-x_3=0$$</p>
<p></p>
<p>The solution was:</p>
<p>$$\left\{\begin{bmatrix}1 \\ 0 \\ 1\end{bmatrix} \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix}\right\}$$ is a basis V</p>
<p><br />I&#39;m a little lost why the basis solution does not have 3 vectors in it. Doesn&#39;t the number of elements in a basis for a subspace have to have n vectors (n = 3 in this case)?</p>","<p>Subspaces do not necessarily have the same dimension. In this case, $$V$$ happens to be lower in dimension than $$\mathbb R^3$$. Some other cases in which this happen are:</p>
<p></p>
<p>- The zero subspace (i.e. $$\{\vec 0\}$$) which has dimension $$0$$</p>
<p>- Lines through the origin (e.g. $$\{s \vec x_1 : s \in \mathbb R\}$$ for $$x_1 \neq 0$$) which have a dimension of $$1$$</p>
<p>- Planes through the origin (e.g. $$\{s \vec x_1 + t \vec x_2 : s, t \in \mathbb R\}$$ for $$\{x_1, x_2\}$$ linearly independent) which have a dimension of $$2$$</p>"
"week 10 practice problem Q4: <p>where does V in this highlighted part come from?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2Fdeee3779d4c13768d01dc54ff3a4f42450cc81689c0c011861e2145ffa964ce3%2Fimage.png"" alt=""image.pngNaN"" /><br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2F25dce7933368db8fa4a7a945107862393720137c3b7a0e893d0dc364c3016a8c%2Fimage.png"" alt=""image.pngNaN"" /></p>",That looks like a typo and should say $$\mathbb F^n$$ instead of $$V$$. The $$\mathbb F^k$$ should also read $$\mathbb F^n$$.
"week 10 practice problem Q7: <p>can someone explain this? I cannot understand well.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2Fee646eca092c5db5c855b4d488c983310d198ed40b5ed09520d1a9b38d37ed44%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehero027lg%2F2a9e9a45b5cea2f9ea4c21e92d7c879e1c1ec002c049755ede31fe16f8e5959b%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>The matrix</p>
<p></p>
<p>$$A = \begin{bmatrix} \vec v \mid \vec e_1 \mid \cdots \mid \vec e_n\end{bmatrix}$$</p>
<p></p>
<p>is $$n \times (n + 1)$$. Since it contains the standard basis, its columns pace spans $$\mathbb F^n$$, so it must have rank $$n$$. This means that in its RREF, there are $$n$$ pivots, of which the first column must have one (because $$\vec v \neq \vec 0$$).</p>
<p></p>
<p>Then, take the submatrix of $$A$$ (note: $$A$$, not $$\text{RREF}(A)$$) by taking the $$n$$ columns which have pivots in them. This matrix is $$n \times n$$, and must be invertible because in its RREF, there is a pivot in each column.</p>"
"Question about basis: <p>For part a) of this question, the answer key claimed that Null(A) has no basis. But I thought Null(A) is a subspace and by Every subspace has a basis, it should have some sort of basis.</p>
<p>This is WP10 Q8 btw</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2Fd6c4cc0399706c9303e04a31611cb08dec0bc30470fb5de6aeb3e114fe01722c%2FScreen_Shot_2024-03-30_at_4.01.08_PM.png"" alt=""Screen_Shot_2024-03-30_at_4.01.08_PM.png"" /></p>","<p>You are right that the nullspace of every matrix is a subspace, and every <em>nontrivial </em>subspace (i.e. $$V \neq \{\vec 0\}$$) has a basis.</p>
<p></p>
<p>By convention, we take the emptyset $$\emptyset$$ to be a basis for the trivial subspace $$\{\vec 0\}$$. You can see the note under 8.2.6 for further reference.</p>"
"EP10 Q5b: <p>For EP10 Q5b, I&#39;m confused by this solution because I don&#39;t get how e_i = [v_i]_B</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2Fbe6b8ec2156350f0c8304b74cf319f9168000af9a8362559389087687831ce5a%2FScreen_Shot_2024-03-30_at_6.59.32_PM.png"" alt=""Screen_Shot_2024-03-30_at_6.59.32_PM.pngNaN"" /></p>","<p>Write out $$[\vec v_i]_{\mathcal B}$$.</p>
<p></p>
<p>We want to express $$\vec v_i$$ as a linear combination of the vectors in $$\mathcal B = \{\vec v_1, \ldots, \vec v_n\}$$, so we need to find $$c_1, c_2, \ldots, c_n$$ so that</p>
<p></p>
<p>$$\vec v_i = c_1 \vec v_1 + \cdots + c_n \vec v_n$$</p>
<p></p>
<p>Of course, we can take</p>
<p></p>
<p>$$c_j = \begin{cases} 0, &i \neq j \\ 1 & i = j\end{cases}$$</p>
<p></p>
<p>and these exactly are the coordinates that make $$\vec e_i$$.</p>"
Subspace Test vs. Definition of Subspace: When would it be better to use the Subspace Test (Proposition 8.1.4) instead of the formal definition (Definition 8.1.1) of a subspace for subspace verification? I see a lot of the practice problems for Week 9 make use of the formal definition as a means of verification,"<p>This is a matter of preference.</p>
<p></p>
<p>The subspace test can be faster since there are less things to check, but in essence, they are the same.</p>"
"48h On Quiz 5: Hello, I am feeling quite ill and was wondering how taking my 48h would affect my quiz grade. I know that the standard is best 4/5 quizzes, but if I was to take my 48h for  quiz 5 because im feeling unwell, would it then just take my best 3/4 quizzes or would it rather take my first four quiz marks, essentially dropping the fifth one.",@643
"General question: Will we be tested on how to row reduce this?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2Fc511e602f11492785ed8675e1ab178c1f7af0a8bb872b570b3f03b8fa9cd2306%2Fimage.png"" alt=""image.pngNaN"" />","Yes, you should know how to do row reduction with complex numbers.  However, the computations on tests/exams are meant to not be too intense. "
solution to q4: When will the solution to quiz4 be posted?,Hopefully soon.  I&#39;ve passed this along to the person who is in charge of posting them. 
"WP10 Q3a: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2Fab9e01eba31e175bc14871564b7e883352f181dd458a65b446475cc4c481bebb%2FScreenshot_2024-03-31_at_9.57.25_AM.png"" alt=""Screenshot_2024-03-31_at_9.57.25_AM.png"" /></p>
<p></p>
<p>Would it be ok to say that B spans R3 because rank = 3 by proposition Spans Fn iff rank(A) = 3?</p>","Yes, you should also mention that $$B$$ has 3 pivots, the diagonal entries."
"Mobius Practice 9: <p>I don&#39;t understand why the set below is not a subspace.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdlmdzkipo%2Ff06554879ea76c3d64b4502d87703837b397914f458d53d383f353cf37b802a6%2Fimage.png"" alt=""image.png"" /></p>
<p>To my understanding, the requirements on the set imply that the second and third components are zero and the first can be any element of the reals. This would mean that any linear combination of an V would still be in V and V is clearly not empty. Any clarification would be greatly appreciated.</p>
<p>Thanks!</p>","<p>Yes, I believe you are right that this is a subspace.</p>
<p></p>
<p>I think there were some issues with this mobius question, so we will try and get that fixed.</p>"
"MATH136W24BaumanLec18Post: What was the solution to this<br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Fc6e462ac4d25fa1679f06c16f8a47de0ed0f200dd706ae114a8a6f9691560b24%2FScreenshot_2024-03-31_at_11.27.45_AM.png"" alt=""Screenshot_2024-03-31_at_11.27.45_AM.pngNaN"" />",https://www.youtube.com/watch?v=fkj5yVdfC_Y
"EP10 Q10: <p>I&#39;m confused how this statement is correct because aren&#39;t subspaces a set of vectors and for a set of vectors to equal F^n we need more than n vectors? I thought it&#39;s only the span(V) that spans all F^n if V has n independent vectors.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2F349eac9a9abd5dc1118a025a2d15dc4470a1dccd70dba032916d3af89e502545%2FScreen_Shot_2024-03-31_at_12.09.37_PM.png"" alt=""Screen_Shot_2024-03-31_at_12.09.37_PM.pngNaN"" /></p>","<p>I&#39;m not sure exactly what you are asking.</p>
<p></p>
<p>A nontrivial subspace will contain an infinite number of vectors (since it is closed under scalar multiplication, you can scale any nonzero element by any real or complex number).</p>
<p></p>
<p>However, if $$V \subseteq \mathbb F^n$$ is a subspace, then $$V$$ is spanned by at most $$n$$ linearly independent vectors (as otherwise, if $$V$$ contains $$n + 1$$ or more linearly independent vectors, then $$\mathbb F^n$$ must also contain $$n + 1$$ linearly independent vectors, which means a basis for $$\mathbb F^n$$ must contain more than $$n$$ vectors).</p>
<p></p>
<p>Does this answer your question?</p>"
"R^2 subspace of R^3?: <p>can I say {[0,0]^T} is a subspace in R^3? Will the dimension of a subset for determining its subspace?</p>
<p>Thank you</p>","<p>$$\left\{\begin{bmatrix}0\\0\end{bmatrix}\right\}$$ is <em>not</em> even a subset of $$\mathbb{R}^3$$, so it cannot be a subspace of $$\mathbb{R}^3$$.</p>
<p></p>
<p>We don&#39;t say &#34;dimension of a subset&#34;. Dimension is defined to describe a (sub)space, which is the cardinality of a smallest spanning set (i.e. basis).</p>
<p>$$\left\{\begin{bmatrix}0\\0\end{bmatrix}\right\}$$ is <em>not</em> even a subset of $$\mathbb{R}^3$$, so it cannot be a subspace of $$\mathbb{R}^3$$.</p>
<p></p>
<p>We don&#39;t say &#34;dimension of a subset&#34;. Dimension is defined to describe a (sub)space, which is the cardinarlity of a smallest spanning set (i.e. basis).</p>"
"Can someone explain this please: <p><a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwatfhhwv48n%2F67eef4906a85ee964f666089c21647ed00daac67faa67a3c08e9713858b3b4ed%2FScreenshot_2024-03-31_134542.png"" target=""_blank"" rel=""noopener noreferrer"">Screenshot_2024-03-31_134542.png</a></p>
<p></p>
<p></p>","Sorry I meant explain the info about spanning sets; the linear independence part I get. Thanks !
<p>Here is another way to think about it.</p>
<p></p>
<p>Suppose you want to find a basis $$\mathcal B$$ for $$\mathbb F^n$$.</p>
<p></p>
<p>Put the vectors in $$\mathcal B$$ into a matrix $$A$$. Our goal is to show that $$A$$ has at least $$n$$ columns.</p>
<p></p>
<p>Since $$\mathcal B$$ must be spanning, we must have $$\text{Col}(A) = \mathbb F^n$$, so this implies that $$n = \dim(\mathbb F^n) = \dim(\text{Col}(A)) = \text{Rank}(A)$$ (using 8.7.8).</p>
<p></p>
<p>Then, using the rank bound, we know that $$A$$ has at least $$n$$ columns.</p>"
"non-subspace  basis: If V is not a subspace, can it have a basis? Thank you",No; bases are only defined for subspaces.
"w10pp Q10: <p>I understand that Span(B) = V (by definition of a basis), but I&#39;m confused why is Span(B) = F^n. Can someone please explain? Thanks.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2Ffe72d0cef5006ed4228f0d52de0db46f1704e22723aebea01e41650372ef6e75%2F19111711909195_.pic.jpg"" alt=""19111711909195_.pic.jpgNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F165aa5e7dc7d0df663a915a80c0f2d00835653bce8a4ab42015eae717f321f77%2FScreenshot_2024-03-31_at_14.18.50.png"" alt=""Screenshot_2024-03-31_at_14.18.50.pngNaN"" /></p>","Since $$\mathcal B$$ is a basis for $$\mathbb F^n$$, it must be spanning, hence $$\text{Span}(\mathcal B)=\mathbb F^n$$."
"Dimension of a Basis: Why isn&#39;t the dimension of a basis always equal to n, since for a spanning set to be a basis it has to have n vectors? I&#39;m not sure if I&#39;m making an assumption here that I can&#39;t make.","<p>We don&#39;t have &#34;dimension of a basis&#34;. Dimension is defined for a (sub)space. </p>
<p></p>
<p>&gt;&gt;for a spanning set to be a basis it has to have n vectors?</p>
<p>A basis for what space? If a set $$\mathcal{B}$$ spans $$\mathbb{F}^n$$ then $$\mathcal{B}$$ contains at least $$n$$ vectors. (why?) If $$\mathcal{B}$$ is also a basis for $$\mathbb{F}^n$$ then $$\mathcal{B}$$ contains exactly $$n$$ vectors. (why?)</p>
<p></p>
<p>Two $$4$$-dimensional subspaces of $$\mathbb{F}^{7}$$ <em>may not</em> be the same, but two $$7$$-dimensional subspaces of $$\mathbb{F}^7$$ <em>must be</em> the same. (why?)</p>"
"Q8a: <p>Why for ii it is ∅?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F726f13f94a236b9c3d0b3a2042e2f9fb9f5e25f2a9f2914cbb091be106ddd339%2FScreenshot_2024-03-31_at_7.25.33_PM.png"" alt=""Screenshot_2024-03-31_at_7.25.33_PM.pngNaN"" width=""622"" height=""275"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F22b487ce4e02ea07598dffce9552ebe0676598c9045ef569459841cbae9ef0c8%2FScreenshot_2024-03-31_at_7.25.49_PM.png"" alt=""Screenshot_2024-03-31_at_7.25.49_PM.pngNaN"" /></p>","By convention, we take a basis for the trivial subspace $$\{\vec 0\}$$ to be the emptyset $$\emptyset$$ (you can see the remark under 8.2.6)."
"Why is this true?: Why can&#39;t span(S) be a subspace <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2F59a1833c6b3e5e3db6977eb3fd1bf5799a741e414166c61d12bafc45b8ed9129%2Fimage.png"" alt=""image.pngNaN"" />","For both directions, if you put the vectors $$\vec v_1, \ldots, \vec v_n$$ into a matrix, you can show that that matrix is invertible."
"not all zero: To express not all zero, can I write C1 $$\neq$$ C2 $$\neq$$ ... $$\neq$$ Ck $$\neq$$ 0","This wouldn&#39;t be right since this is also saying that $$c_1,c_2,\dots$$ are not equal to each other, I would just write something like $$c_i \neq 0$$ for some $$i\in\{1,\dots,k\}$$, or &#34;it is not the case that $$c_1=\dots=c_k=0$$&#34;, or just write out in words that not all of $$c_1,\dots$$ are zero
This wouldn&#39;t be right since this is also saying that $$c_1,c_2,\dots$$ are not equal to each other, I would just write something like $$c_i \neq 0$$ for some $$i\in\{1,\dots,k\}$$, or just write out in words that not all of $$c_1,\dots$$ are zero
This wouldn&#39;t be right since this is also saying that $$c_1,c_2,\dots$$ are not equal to each other, I would just write something like $$c_i \neq 0$$ for $$i\in\{1,\dots,k\}$$, or just write out in words that each of $$c_1,\dots$$ are not zero"
"Q11a: <p>Can you help me to explain why my approach is wrong? Thank you!</p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2Fada76c721ec592dce9026162a011c0183989d993ad1b96a4ed922b42ba4dfbf4%2FIMG_4615.HEIC"">IMG_4615.HEIC</a></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwedxqjbh6yr%2F5f1438d8c1a23475ab74078bfc6a54b783348309876835b2cfd6eb37a6423248%2FScreenshot_2024-03-31_at_8.46.39_PM.png"" alt=""Screenshot_2024-03-31_at_8.46.39_PM.pngNaN"" /></p>","<p>You cant just combine the two equations into one - you lose information by doing that.</p>
<p></p>
<p>For instance, suppose we have the equations,</p>
<p></p>
<p>$$x_1+x_2 = 0$$, and </p>
<p>$$x_3+x_4 = 0$$</p>
<p></p>
<p>This is not the same as $$x_1+x_2=x_3+x_4\iff x_1+x_2-x_3-x_4=0$$. Note that if we took $$x_1=x_2=x_3=x_4=1$$, this would satisfy this equation, but not our original. We are &#34;losing&#34; information, since now we are only enforcing that $$x_1+x_2$$ and $$x_3+x_4$$ are equal to each other, when originally they need to be equal to each other, and also equal to 0.</p>
<p>You cant just combine the two equations into one - you lose information by doing that.</p>
<p></p>
<p>For instance, suppose we have the equations,</p>
<p></p>
<p>$$x_1+x_2 = 0$$, and </p>
<p>$$x_3+x_4 = 0$$</p>
<p></p>
<p>This is not the same as $$x_1+x_2=x_3+x_4\implies x_1+x_2-x_3-x_4=0$$. Note that if we took $$x_1=x_2=x_3=x_4=1$$, this would satisfy this equation, but not our original. We are &#34;losing&#34; information, since now we are only enforcing that $$x_1+x_2$$ and $$x_3+x_4$$ are equal to each other, when originally they need to be equal to each other, and also equal to 0.</p>
<p>You cant just combine the two equations into one - you lose information by doing that.</p>
<p></p>
<p>For instance, suppose we have the equations,</p>
<p></p>
<p>$$x_1+x_2 = 0$$, and </p>
<p>$$x_3+x_4 = 0$$</p>
<p></p>
<p>This is not the same as $$x_1+x_2=x_3+x_4\implies x_1+x_2-x_3-x_4=0$$. Note that if we took $$x_1=x_2=x_3=x_4=1$$, this would satisfy this equation, but not our original. We are &#34;losing&#34; information</p>"
"Not understanding the solution, why is it true?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Ff233d00afd117f1275b45a538a8e4bd5ac7c58a94a53b5f03bf6b9ff6e9cbf50%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fba5aaeef3967d665b2451a927978e15ea5b51bbcc86ff1f54bbe06c346d81a3e%2Fimage.png"" alt=""image.png"" /></p>","I think you might be confused by the wording. We know B is both a basis for S1 and S2. With that we can make that statement true.<div><br /></div><div>So S1 = SpanB</div><div>And S2 = SpanB</div>
I think you might be confused by the wording. We know B is both a basis for S1 and S2. With that we can make that statement true.<div><br /></div><div>So S1 = SpanB</div><div>And S2 = SpanB</div>"
"WP 10 Q9 clarification: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fa56571c6e32a3666f94c677c30da9d8460ba217e23ed737e109f68decdeb46b1%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>Is this supposed to be Q6 warmup, or Q5 recommended? There is no Q6 recommended in week 2, and while Q6 warmup is sort of related, Q5 recommended seems much more related</p>
<p></p>
<p>Q6 warmup:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2F2100a2493c00c0a20503265560faf913c2add5f52dccaa27d9a36a610b9b3826%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Q5 recommended: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fd698f056b56818c7e8a32bf174a7f915d74aab3e2e884ec65519c9780517dbf9%2Fimage.png"" alt=""image.png"" /></p>",That should say Q5 recommended.
Is A^-1  == inverse of A: Title,"yes, also this is math not cs you don&#39;t have to put =="
What is the fastest way to calc the inverse of matrixes greater than 2x2: The adjugate method or the row reduce the augmented matrix?,"I don&#39;t think the adjugate is very practical unless you can memorize the adjugates, I use the super augmented matrix method, but I&#39;m sure there&#39;s a better way
I do not recommend calculating inverses using the adjugate method.  Using the super augmented matrix method is usually the best way. "
"Performing RREF: I saw in some posts that we don&#39;t have to show all EROs in WAs. But do we still have to show every single ERO in quizzes and tests? Or if it&#39;s very straightforward that the RREF will become the identity matrix, can we just write e.g.<br />$$\begin{bmatrix} 1&2&4\\ 0&3&5 \\ 0&0&6 \end{bmatrix} \xrightarrow{RREF} \begin{bmatrix} 1&0&0\\ 0&1&0 \\ 0&0&1 \end{bmatrix}$$<br />Or should we write out each ERO?","I think you should show your steps.  But in the example you gave above you could simply say that the matrix is invertible and therefore, it&#39;s RREF is the identity matrix. "
"WPP 10 Q8 (C): <p>when finding the basis for Col(A), the rank of the matrix is 2 which is not equal to 3, so that means the set is linearly independent, so how can it have a basis?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fktn8lt6g2yi515%2F5ae2d42af2c4d8bc4b507e6fd255a2b202752994daeb3f378b330b01c42359ce%2FScreenshot_2024-04-01_at_12.52.48_PM.png"" alt=""Screenshot_2024-04-01_at_12.52.48_PM.pngNaN"" /></p>","<p>&gt;&gt;the rank of the matrix is 2 which is not equal to 3, so that means the set is linearly independent</p>
<p>You meant the three columns are <em>linearly dependent</em>?</p>
<p></p>
<p>Here, you are asked to find basis for the span</p>
<p>$$\mathrm{Span}\left\{
\begin{bmatrix}1\\-1\\1 \end{bmatrix},
\begin{bmatrix}2\\-2\\3 \end{bmatrix},
\begin{bmatrix}0\\0\\-1 \end{bmatrix}
\right\}$$. Every span is a subspace hence has a basis. In this case, the basis <em>cannot be</em> formed by <em>all three vectors as they are linearly dependent</em>,  but <em>how about two vectors or one vector</em>?</p>"
Unable to attend quiz: <md>If I declare a short_term absence today will i be exempted from the quiz today?</md>,"Yes
The weight of the quiz will be added to the final exam.  Your quiz grade will be calculated as the best 3 out of the remaining 4 quizzes."
"Subspace Test: <p>I&#39;m a bit confused to how to prove/disprove whether a set is a subspace or not. For example, I cannot find a counterexample for this: </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F87257bbad7a788daa56b988399aa8617c0b634f9c5cc47e2c760d233ed49daba%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>This question is a bit subtle. One should be careful about what whole space is given. Here, you are checking subspace of &#34;$$\mathbb{R}^3$$&#34;! This means the scalar multiplication inherits from that of $$\mathbb{R}^3$$, which is $$\mathbb{R}$$.</p>
<p>The set $$V$$ is defined in $$\mathbb{Q}$$, so scalar multiples like $$\sqrt{2} \begin{bmatrix}x_1\\x_2\\x_3 \end{bmatrix}$$ are <em>not</em> in $$V$$. (not closed)</p>"
"Example 8.7.6 and 8.7.7: Will the result of these examples apply to lines and planes NOT through the origin?<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Fd21e3f8574c3ccf205793a204d99a0fda90e691dff76333a46d17d8fc18315fd%2FScreenshot_2024-04-01_at_1.31.13_PM.png"" alt=""Screenshot_2024-04-01_at_1.31.13_PM.pngNaN"" />","No, because lines and planes that do not go through the origin are not subspaces.  (They do not contain the zero vector.)
No, because lines and planes through the origin are not subspaces.  (They do not contain the zero vector.)"
"WPP W10 Q9: <p>Hi, i don&#39;t quite understand the step of multiplying vi on both sides, could someone explain a bit?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcluq3qmxm%2F95934d0e4ca55e0784a916e36bd56fde35b6f751ef35761e4212601749bc84c7%2Fimage.png"" alt=""image.pngNaN"" /></p>","All the other terms are zero since we are given that they are mutually orthogonal, leaving only the $$c_1\vec{v}_i\cdot\vec{v}_i$$ term
All the other terms cancel since we are given that they are mutually orthogonal, leaving only the $$c_1\vec{v}_i\cdot\vec{v}_i$$ term"
[plane as subspace of R3]: Wondering if we have a scalar equation like x&#43;y&#43;z = 1. Is it a subspace of R3? (I think it is not a subspace since it did not go through the origin)<br /><br />Thanks in advance,"Not a sub space because the 0 vector isn’t in the set. Exactly what you said.
Not a sub space because the 0 vector isn’t in the set. Exactly what you said."
"Questiona bout change basis matrix: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2Fb8110b2ec98e5b64fd48e58e21605a37c639ad180fca0251a382b5159b219a29%2FScreenshot_2024-04-01_at_5.42.48_PM.png"" alt=""Screenshot_2024-04-01_at_5.42.48_PM.pngNaN"" />If x is a matrix instead of vector, can the change-basis matrix still changes a matrix from coordinates B to coordinates C? Why? Thank you","Yes, the matrix just changes vectors to the other base, a matrix is an essentially a collection of vectors, so it can just change each vector. (If you do the matrix multiplication, you can see that it does just this)"
coverage: What is the coverage for math136 final,"Everything we learnt.
Everything up to the end of chapter 9. "
"A Problem about Dimension of Subspaces: <p>Hello! I was trying to figure out how can we give a formal proof to the following statement.</p>
<p></p>
<p>If $$V$$ and $$W$$ are subspaces of $$\mathbb{F}^n$$ such that $$W \subseteq V$$, then $$\dim{(W)} \leq \dim{(V)}$$.</p>
<p></p>
<p>Thank you!</p>","Here is a rough idea, Consider the basis of W, and suppose for a contradiction that dim(W)&gt;dim(V). But since W is a subset of V, we can express each vector in the basis for W in terms of vectors in the basis of V (but there are less vectors in V than W).
Here is a rough idea, Consider the basis of W, and suppose for a contradiction that dim(W)&gt;dim(V). But since W is a subset of V, we can express each vector in the basis for W in terms of vectors of V (but there are less vectors in V than W).
Consider a basis for $$W$$.  Every vector in $$W$$ is also in $$V$$ and so we have a linearly independent set in $$V$$ (since a basis for $$W$$ is linearly independent).  Now think about growing this set into a basis for $$V$$. "
"Additional practice problems or past papers for final: <p>hello I wonder will we be given past papers or additional practices for final exam?</p>
<p>Thanks!</p>","I am sure there are several past finals or midterms in the MathSoc exambank, and I &#39;guess&#39; we will be given a sample final similar to the midterm.
I&#39;m pretty sure a sample final exam will be posted. "
"Some general questions: <md>Hi sorry to bother, can I get some insights into these questions that I have?

1. We know that the inner product corresponds to the work done (energy) in physics, which is conserved in any coordinate/reference systems by relativity. So I naturally thought the inner product should be the same in different coordinate systems. But obviously, it has different values with respect to different bases. Is it because every time we change the bases, we also have to multiply by something (the Gram matrix) in order to get the same value for inner products?

2. How to derive the rank-nullity theorem for an infinite dimensional vector space? I've heard that it's V/Ker f = Im f, but I don't know how to derive it. Does it have any relationships with the finite-dimensional one? 

3. What are some uses of complex eigenpairs, perhaps in physics? I suppose it's not only scaling the vectors but also rotating them at the same time, since it's being multiplied by ki (rotation by pi/2). 

4. Why are we interested in singular matrices? I thought matrices that don't have inverses are somewhat less interesting. 

5. What is the intuition behind the Jordan Canonical Form, and how does it simplify the understanding of linear operators?

Thanks in advance.</md>","<p>These are some pretty big questions and a lot of them are beyond the scope of the course.  I might suggest that you ask some of the questions to your instructor in office hours because it would take quite a lot of work to type out answers here on Piazza.  I don&#39;t want to discourage your questions, but I think you might get better answers this way. </p>
<p></p>
<p>Maybe some of the other instructors will respond below. </p>"
"Final Study Schedule: Feeling a bit lost about how to approach studying for finals. Do you have any recommended schedule, and/or topic and practice problems you recommend focusing on?","<p>I recommend focusing on material covered since the midterm (but don&#39;t totally ignore pre-midterm material since it is the foundation.)  Do as many of the practice problems as you can.  </p>
<p></p>
<p>Be careful not to over focus on one of the weeks and run out of time.  Maybe start by doing 5 warm-up problems and 5 recommended problems from each week and once you have finished that go back and do 5 more from each week.   This has the advantage of not doing all questions on one topic but rather switching from topic to topic which is what will happen on the exam. </p>"
"Proof of Proposition 9.1.6: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F24b4fb35c5ba360d57c4c63e25ce392d81355a93fc18baeaf41e3dd8594c9038%2FScreenshot_2024-04-02_at_1.55.32_PM.png"" alt=""Screenshot_2024-04-02_at_1.55.32_PM.pngNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fd049839864c2c5df8675b01edc94ad6c1e69b7c131d015911610318d747a538a%2FScreenshot_2024-04-02_at_1.55.51_PM.png"" alt=""Screenshot_2024-04-02_at_1.55.51_PM.pngNaN"" /></p>
<p></p>
<p></p>
<p>Could someone please explain how exactly using the definition of Matrix Multiplication we are able to do so, I know it&#39;s a very basic doubt but I am getting very confused regarding why it is this way, most likely because of the symbols.</p>","When you calculate the product $$AB$$, the ith column of the product is $$A\vec{b}_i$$ where $$\vec{b}_i$$ is the ith column of $$B$$.  So $$AB = [A\vec b_i \cdots A \vec b_n]$$.  Use this idea in reverse and we can pull out the $$_C[I]_B$$ and then later the $$[T]_B$$."
"Show that if A and B are similar they have the same characteristic polynomial: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweeqkowg746%2Fb8ea448cd2a40f5d47371e4d3c42e0b8ae3627baa53cc1fe7e4bfe666bc3d5ff%2Fimage.png"" alt=""image.png"" /></p>
<p>For this question if i prove det(A)=det(B) then  by default they will hbave the same characteristic polymonials right?</p>","<p>Can you post more details? How does $$\det(A)=\det(B)$$ imply the same chararcteristic polynomial?</p>
<p></p>
<p>An approachable process could be </p>
<ul><li>Show that for any $$\lambda$$, $$A-\lambda I_n$$ and $$B- \lambda I_n$$ are similar using the similarlity of $$A, B$$.</li><li>Throw determinant?</li></ul>
Can you post more details? How does $$\det(A)=\det(B)$$ imply the same chararcteristic polynomial?
<md>No. Consider
$$
\left(\begin{array}{cc} 
0 & 0\\
0 & 0
\end{array}\right) $$ and 
$$
\left(\begin{array}{cc} 
1& 0\\ 
0 & 0
\end{array}\right)
$$.</md>"
"regrade request: If I have a problem with my grade for WA4, can I send a regrade request via email to math136?",You can find the form for submitting a remark request on Learn.
"Latex bonus: Hi, I used latex to do this assignment, but I didn&#39;t receive the bonus point. Could please help me to add this point? thank you.",You can submit a regrade request through the portal from Learn.
"Definition of Range: <p>I want to know, should the vector x in F^m? (because range shows the output). Thanks!</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwabr0j5p14l%2Fb1ab5dbcf6f54f65d3964dce99ab9304fb1e19954a6086ccee6fce19dbbe879f%2FScreenshot_2024-04-02_at_6.14.38_PM.png"" alt="""" /></p>","<md>x is the input, which is from F^n.</md>"
cramer&#39;s rule on exam: Is cramer&#39;s rule on the final exam?,"I believe so, but my prof said it likely won&#39;t be a full computation and might just be asking us to get a part of the solution, like only the 2nd component of the resulting vector for example.
It could be a possible topic.  But any question would aim to limit the amount of computation. "
Latex bonus WA4: I used Latex for WA4 and I didn&#39;t get the bonus added on crowdmark like I usually do. I was wondering if I could just get my mark updated or will I have to submit a regrade request just to get the bonus.,<md>You should submit a remark request (you can find the page for this on Learn).</md>
"Example 5.6.3 Rotation Matrix: <p>When trying to find a rotation matrix, I get thoroughly confused, as in this example.<br />Also, how does showing R(x) = Ax for some matrix A show that it&#39;s an LT? Is that the converse of some theorem?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Ff7324be5591884519b0578143aa2512b41f0517bfc5a554615c4c00f47b0d15d%2FScreenshot_2024-04-03_at_10.18.09_PM.png"" alt=""Screenshot_2024-04-03_at_10.18.09_PM.pngNaN"" /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Fa950e2beeb68e6746d402499b97829be0faf6e704c8ae4debecb77e7102c1db3%2FScreenshot_2024-04-03_at_10.17.12_PM.png"" alt=""Screenshot_2024-04-03_at_10.17.12_PM.pngNaN"" /></p>","There’s a theorem that states that all functions that is determined by a matrix is linear.
<p>While I think it is good to understand where the rotation standard matrix comes from, at this point it&#39;s just good to memorize it.  (It&#39;s not difficult to memorize.) </p>
<p></p>
<p>See Theorem 5.1.4 for why a transformation determined by a matrix is linear. </p>"
"refl transformation: <p>How do we find the linear transformation expression for refl and do we need to memorize it for the exam?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2Feabdf43d273e73dfe5287a8ba3f0d95df27114dd489a1de2d1e347a4bfa037c6%2FScreen_Shot_2024-04-03_at_11.18.09_PM.png"" alt=""Screen_Shot_2024-04-03_at_11.18.09_PM.pngNaN"" /></p>","<md>You can find the derivation for the reflection transformation in the textbook, but you won't need to memorize it for the exam.

For this question, your basis $$\mathcal B$$ should consist of 3 eigenvectors, so which vectors remain along the same axis after the reflection?</md>"
"Example 8.8.9 (Page 223 of course notes, section 8.8): <p>Hi for this question: </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fktn8lt6g2yi515%2F05f411ef82fbe576989756b4aecec204ec558f073241fb10f49acc1c2bea1a2a%2FScreenshot_2024-04-04_at_2.14.56_AM.png"" alt=""Screenshot_2024-04-04_at_2.14.56_AM.png"" /></p>
<p>why can&#39;t we take a=-2 and b = 1 and so why can&#39;t the vector [-2 1] ^ T be the coordinates of v with respect of B?</p>",-2(2) &#43;1(6) = 2
"Quiz 5 answers: <md>I was wondering if we will get our results before the final exam. If not could we get the solutions and the plain version on to learn?

Thank you</md>",The  questions have already been posted on LEARN.  The solutions will be posted soon.  I&#39;m quite certain you will get the results before the final exam.
"WA4 Official Solutions: Where can I find the official solutions for WA4? It does not appear on Learn. I am trying to send a regrade request, but I would like to first verify with the official solutions.",I&#39;ve passed along this request.  Hopefully it doesn&#39;t take too long to get them posted. 
"Is the converse of injectivity necessarily true?: If x=y, does T(x)=T(y)?<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F4427d84228d693f019f3dfa0ad55a4f6bd8dc1949d73c698208020150163b84c%2FScreenshot_2024-04-04_at_11.40.24_AM.png"" alt=""Screenshot_2024-04-04_at_11.40.24_AM.pngNaN"" />",If $$\vec{x} = \vec{y}$$ then their images under T are the also equal.
basis: What is the criteria to be a basis?,This definition can be directly found in the course notes. 
WA5: Will the WA 5 answers be posted?,Yes.  On Tuesday. 
"WP 11 Q1: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2Faa422f748454a1c02e7d26a36f33f7ecdd31b3cc6bb25dc4c6e94995181ece6d%2Fimage.png"" alt=""image.png"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2F4b9c0688ad824a9f9d8f5e377f1acb6a098c9e086198f203da4cb66e0b686b0c%2Fimage.png"" alt=""image.png"" /></p>
<p>I was wondering whats the process of getting those vectors</p>
<p></p>",They are a basis of eigenvectors.  Remember that an eigenvector of a linear transformation $$T$$ is a vector $$\vec{x}$$ such that $$T(\vec{x})= \lambda \vec{x}$$.  So we can use our geometrical knowledge of the transformation to choose eigenvectors which are linearly independent. 
"quiz 5 mc d: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2F0f096028ebadff9e54cdfd7455b554cd0d055c8d292237c6459db207c35d842f%2FScreenshot_2024-04-05_at_12.42.48_AM.png"" alt=""Screenshot_2024-04-05_at_12.42.48_AM.pngNaN"" />How can this be true? Consider $$V= \{[1,0]^T,[0,1]^T\} \in R^2$$. $$dim(V) = 2$$ However, $$\{[1,0]^T,[0,1]^T\} \neq R^2$$. Only $$span\{[1,0]^T,[0,1]^T\} = R^2$$.","<p>I think you have gotten subspaces and bases mixed up, $$V$$ is a subspace, $$\{[1~0]^T,[0~1]^T\}$$ is the base for the subspace, so it is true that $$V=\mathbb{R}^2$$ since $$V$$ is the span of its base,</p>
<p></p>
<p>$$\dim(V)=2$$ means that the base of $$V$$ has two vectors in it, not that $$V$$ has two vectors in it ($$V$$ would have infinitely many vectors in this case)</p>
I think you have gotten subspaces and bases mixed up, $$V$$ is a subspace, $$\{[1~0]^T,[0~1]^T\}$$ is the base for the subspace, so it is true that $$V=\mathbb{R}^2$$ since $$V$$ is the span of its base
I think you have gotten subspaces and bases mixed up, $$V$$ is a subspace, $$\{[1~0]^T,[0~1]^T\}$$ is the base for the subspace, so it is true that $$V=\mathbb{R}^2$$ since $$V$$ is indeed the span of the base"
"Final difficulty: Hi, I just want to ask does the final is more like the quizzes (which is focusing on testing some definitions) or assignment type questions? Thanks.",I would say it&#39;s a mix of both.  The posted sample final will give you a sense of the format and style of the final exam. 
PP10 Q10: Can someone please explain how using the super augmented matrix gives C[I]B?,"<md>Suppose you have two $$n \times n$$ matrices $$A, B$$, and you are looking at the super augmented matrix $$[A \mid B]$$.

Suppose $$B = [B_1 \mid \cdots \mid B_n]$$ where $$B_i \in \mathbb F^n$$ for each $$1 \le i \le n$$.

Then, the super augmented matrix can equivalently be written as $$n$$ systems of equations:

$$[A \mid B_1], [A \mid B_2], \ldots, [A\mid B_n]$$

Putting them together into the super augmented matrix is simply a way to solve all of these systems at the same time.

For this problem, we are trying to determine $$\vphantom{}_{\mathcal C}[\mathcal I]_{\mathcal B} = \begin{bmatrix} [\vec v_1]_{\mathcal w} & [\vec v_2]_{\mathcal w}\end{bmatrix}$$

We know that $$\underbrace{[\vec w_1 \mid \vec w_2]}_{\in M_{2 \times 2}(\mathbb R)}[\vec v_1]_{\mathcal w} = \vec v_1$$, so we can take $$A = [\vec w_1 \mid \vec w_2]$$, and put all of the columns of $$B$$ as the basis vectors in $$\mathcal B$$.</md>"
What is the Deadline for WA4 Regrade Submission?: ,"<p>I found this on the Regrade Request page (content --&gt; assessments)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F4a28f82388e972aaf6177bf2f12b173eb3b4669c865ae53781a25e267c9fc92a%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>Since the results were posted on Tuesday, you can still submit the request during the next couple days</p>
<p>I found this on the Regrade Request page (content --&gt; assessments)</p>
<p></p>"
"Are there office hours before the final and if so, where are they posted?: ","<p>This will depend on your instructor. My instructor told us his scheduled office hours during lecture today.</p>
<p></p>
<p>I took a quick peek on learn and saw that Professor Bauman is hosting office hours next week! You might be able to find other instructors posting this info on learn. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F45c7e8a5b189e138afa253e6521fb92937d1a2a55fdbefcf76a806a421721f7a%2Fimage.png"" alt=""image.png"" /></p>
<p>This will depend on your instructor. My instructor told us his scheduled office hours during lecture today.</p>
<p></p>
<p>I took a quick peek on learn and saw that Professor Bauman is hosting offices hours next week! You might be able to find other instructors posting this info on learn. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F45c7e8a5b189e138afa253e6521fb92937d1a2a55fdbefcf76a806a421721f7a%2Fimage.png"" alt=""image.png"" /></p>"
Wrong grade updated on Learn: The WA4 grade posted on Learn does not include the LaTeX bonus mark. Will it be updated later?,"<md>I have the same problem with remarked grades.</md>
The LEARN gradebook will be synced with Crowdmark closer to the final exam. "
"Solution eigenvectors: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwatgzzdt48x%2F8d09e10b39e998686d4f4f2547515ec1740cb709dec462e9855475ee2cf8dc25%2FCapture.PNG"" width=""439"" height=""165"" alt="""" /></p>
<p>Is the general solution not t [-1 1] </p>",That is a scalar multiple of $$[1\ 1]^\top$$ (multiply by $$-1$$).
"WP11 Q2: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2F7b523cd33c23c6f0ef56c64cf9e8408a30b7fe28fda82948de93de764f9081b2%2Fimage.jpeg"" alt=""image.jpegNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fdbcebda916c294278092754dc64162fa9d595a5d29429a7b5cd8441dbe267f21%2Fimage.jpeg"" alt=""image.jpegNaN"" /></p>
<p>I wonder how we can get the algebraic multiplicity is 2. Thanks.</p>","The algebraic multiplicity of an eigenvalue $$\lambda$$ is the degree of $$\lambda$$ as a root in the characteristic polynomial.<div><br /></div><div>This characteristic polynomial is $$C_A(x) = (\lambda - x)^2$$, so the algebraic multiplicity of $$\lambda$$ is 2.</div>
The algebraic multiplicity of an eigenvalue $$\lambda$$ is the degree of $$\lambda$$ as a root in the characteristic polynomial.<div><br /></div><div>This characteristic polynomial is $$C_A(\lambda) = (\lambda - \mu)^2$$, so the algebraic multiplicity of $$\mu$$ is 2.</div>"
Column space and row space clarification: Does the column space include all columns of the matrix? If two columns are equivalent to each other then are both columns included in the column space? ,"The column space is the set of all linear combinations of the columns, so the columns are included in the columns space (consider taking just $$1$$ copy of each column).<div><br /></div><div>For writing the column space as a span of vectors, it doesn’t matter if you include redundant vectors (or linearly dependent ones), unless the question asks to find a basis for the column space.</div><div><br /></div><div>On the exam, it will be clearly stated if we want you to find a basis or not.</div>"
"WP11 Q8: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Fa8d9a156b1a2ae9f636ef8247c9804664f99683a00c1861e7994b77a98ef1c78%2Fimage.jpeg"" alt=""IMG_0326.jpgNaN"" /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Ffe673fc546ca736789a8f69f769b1e74763ae46ec328ffe1fa4312e59f828afb%2Fimage.jpeg"" alt=""IMG_0325.jpgNaN"" /><br />For this question, why is the matrix that diagonalizes A and B the same? I suppose it should be two invertible matrices P1, P2 filled with the same eigenvectors vi in different order, however, the answer says that P1 = P2. Is there anything wrong with my thinking process? Thank you.","If a matrix $$A$$ is diagonalizable, then the matrix which diagonalizes $$A$$ is going to be the matrix whose columns are all eigenvectors. i.e.<div><br /></div><div>$$A=PDP^{-1}$$</div><div><br /></div><div>where $$D$$ is diagonal with entries full of the eigenvalues, and the columns of $$P$$ are the corresponding eigenvectors of $$A$$.</div><div><br /></div><div>Does this answer your question?</div>"
"Question about transformations: Are we expected to memorize the standard matrix of transformations like reflection, rotation, projection etc?","No, but you may find them useful for thinking about examples or counter examples."
"Clarification on Proof: I&#39;m going over the proof for Eigenvector Basis Criterion for Diagonalizability. I noticed that after extracting the ith column of the diagonal matrix, the next line was = diei. However by column extraction, the product of the diagonal matrix and ei is di. Therefore, is it the case that diei = di? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8j0ruc25io%2F3075bb1d4410d8400134d87265a69b6408a01606cc8879a1489b5c289f08003b%2FScreenshot_2024-04-06_at_3.17.03_PM.png"" alt=""Screenshot_2024-04-06_at_3.17.03_PM.pngNaN"" />","yes, since it is a diagonal matrix, the only nonzero entry is the $$i$$-th entry."
"Score: Hi, so I used latex to do these assignments and I received latex bonus marks on crowdmark, but in learn it seems like I didn&#39;t get any bonus marks of WA3 and WA4. What should I do? Thank you. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwel0v1d7im%2F7932d2af46dee1bf57000a01867088365ce0d4ba4f47787b37795692386be4bb%2Fimage.png"" alt=""image.pngNaN"" />","here is my crowdmark. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwel0v1d7im%2Fa2e778f9a587d2578d0ec5bb8a20ef8c9f5fd103db92fb48e534c68729e670c9%2Fimage.png"" alt=""image.pngNaN"" />
LEARN will be synched with Crowdmark closer to the final exam. "
"Quiz 5 Mark Release: <p>Hi! I&#39;m just wondering when the quiz marks will be released for quiz 5 since i&#39;m a bit anxious to receive it.</p>
<p></p>
<p>Thanks!</p>",I passed this question along to the person in charge of marking but I would predict that you will get Quiz 5 marks back on Monday or Tuesday. 
"Choice of Basis and Reflection: <p>Hi, I&#39;m confused about if we have to apply the operation that we learned about refl here and how does the reflection do so &#34;through the origin&#34;? Furthermore why is it that the eigenvectors of refl make sure that its diagonalizable? Why were those two vectors chosen other than linear independence? Could we have chosen [1 0 0]T instead of [1 0 1]T? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2F6a30506c5945a149f699ec626022dead454fe8f6108460ada36cd669318b4267%2FScreenshot_2024-04-06_at_5.33.43_PM.png"" alt=""Screenshot_2024-04-06_at_5.33.43_PM.pngNaN"" width=""590"" height=""91"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2F9a9937a17ce1b065f02b34886b55cf6f5f6622ea5ba474bb0e21a2e9e19adb01%2FScreenshot_2024-04-06_at_5.33.50_PM.png"" alt=""Screenshot_2024-04-06_at_5.33.50_PM.pngNaN"" width=""579"" height=""145"" /></p>
<p></p>
<p>I&#39;d really appreciate any help in filling my gaps in understanding, thank you!</p>
<p></p>
<p></p>","<p>We are reflecting the vectors through a plane through the origin.  If we reflect vectors through a plane that doesn&#39;t go through the origin, then $$T(\vec 0) \neq \vec 0$$ and the transformation will not be linear. </p>
<p></p>
<p> We have proved that a linear transformation is diagonalizable if and only if we can find a basis of eigenvectors.  </p>
<p></p>
<p>The equation of the plane will be $$x-z=0$$.  To get two eigenvectors with eigenvalue 1, they chose two linearly independent vectors in the plane.  The vector $$[1,0,0]^T$$ does not lie in the plane because it does not satisfy the equation of the plane. </p>"
"Midterm Regrade: Hi, just wondering when the midterm regrade will be updated on LEARN, as my regraded mark has not updated on LEARN yet.",All grades on LEARN will be resynched with Crowdmark closer to the final exam date. 
"Final Exam Breakdown: Hi, I am wondering if you could share what percentage of the final will be more computational based and what percentage will be more theoretical based? I think this will help me organize my studying. Thanks!",That information is not being made public because it&#39;s actually difficult to calculate because the boundaries between these categories is often blurry.  Focus on learning all of the material. 
WA5 Q1: Would it be possible to get an example of what w3 or w4 would be? I&#39;m not really seeing the pattern with the vectors. How is wn = vn &#43; v1? ,"$$w_3=v_3+v_4$$. In general, $$w_i=v_i+v_{i+1}$$ with the exception $$w_n=v_n+v_1$$."
"Office hour tomorrow: Hi since tomorrow is a make up day for Good Friday, I was wondering whether or not the office hour tomorrow will be the same as that of a regular Monday.",I would suggest checking with your individual instructor as it likely varies. I know my instructor usually has office hours on Friday but he is hosting them at a different time than usual tomorrow. 
Diagonalization: Why is being diagonalizable sometimes A = PD(P)^-1 and sometimes PA(P)^-1 = D? Are they used interchangeably?,"Yeah pretty much. The first is to express the matrix A and the second to express the diagonal matrix $$D$$. Given the first equation $$A = PDP^{-1}$$, you can multiply both sides by $$P$$ and $$P^{-1}$$ to get $$D = P^{-1}AP.$$ 
<p>Yeah pretty much. The first is to express the matrix A and the second to express the diagonal matrix D. Given the first equation A = PD(P)^-1, you can multiply both sides by P and P^-1 to get D = (P)^-1AP. </p>
<p></p>
<p>Side note: D = (P)^-1AP not PA(P)^-1 </p>"
"mobius: <p>Since T is over R, why eigenvalue is over C?<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc65fwxp5ux%2Fb6c41fdbdfe72c352c27db9582e1f132eda9abd84aa76a506b29fb61a94da01c%2F75b39e0368b2be171b373306414d9e6.png"" alt=""75b39e0368b2be171b373306414d9e6.png"" width=""315"" height=""128"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc65fwxp5ux%2F0b685c0a7096c9f488c6d98bbcdd9897834ffd58256efea3f01bbbc5de854a3c%2F3a70b0cf6cf1b4effcd1a4b53206b55.png"" alt=""3a70b0cf6cf1b4effcd1a4b53206b55.png"" width=""519"" height=""286"" /></p>
<p></p>",This question is not clearly posed.  When finding eigenvalues we should make clear whether we are looking for eigenvalues over $$\mathbb{C}$$ or over $$\mathbb{R}$$.
Midterm PDF: Is there a pdf copy of the midterm? I would like to use it as extra practice for the final.,"Here you go.  <a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fidt8cfyuv993c4%2F59b7b3994a16a9b732d53b5bc22980c21eb4abfac0e3e96930d3a65a84c5ff34%2FMATH_136_W24-midterm.pdf"">MATH_136_W24-midterm.pdf</a>"
"what is a8 = g8 means: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl7s7g2ty49x44n%2F51c001dd413f8f95fbff738d6912ad3c35adee8be1d0e8b6ff0d3adc5ed540a4%2FScreenshot_2024-04-07_142453.png"" width=""1263"" height=""427"" alt="""" />","<p>This is the algebraic and geometric multiplicities. The geometric multiplicity is the number of vectors you find in the eigenspace (in this example, 2). The algebraic multiplicity is the multiplicity/exponent of the eigenvalue in the characteristic polynomial.</p>
<p></p>
<p>So for example $$(2-\lambda )^3$$ would tell you the Eigenvalue 2 has a algebraic multiplicity of 3.</p>
<p></p>
<p>In order for a matrix/linear operator to be diagonalizable we need the algebraic multiplicity to equal the geometric multiplicity for every Eigenvalue.</p>
<p>This is the algebraic and geometric multiplicities. The algebraic multiplicity is the number of vectors you find in the eigenspace (in this example, 2). The geometric multiplicity is the multiplicity/exponent of the eigenvalue in the characteristic polynomial.</p>
<p></p>
<p>So for example $$(2-\lambda )^3$$ would tell you the Eigenvalue 2 has a geometric multiplicity of 3.</p>
<p></p>
<p>In order for a matrix/linear operator to be diagonalizable we need the algebraic multiplicity to equal the geometric multiplicity for every Eigenvalue.</p>
<md>The student answer has swapped the definitions:

the geometric multiplicity is the dimension of the eigenspace, while the algebraic multiplicity is the degree of the eigenvalue as a root in the characteristic polynomial.

You can find these definitions in the course notes.</md>"
"Standard matrix of linear transformation: For the final will we be required to memorize the standard matrix of transformations such as reflection, projection. I remember reflection was derived in the course notes but we were sort of just used it as it was given in class.","iirc we aren&#39;t required to memorize them
I think it is good to memorize the rotation transformation matrix.   It is not difficult to remember.  You do not need to memorize the others. "
"Finding the Basis: How are we able to calculate the basis for E3 and E-3? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8wau5si1h%2F622198131fa116a3d5b9e23a3583f84fea217e41b2fab4f112362ada479b1532%2FScreenshot_2024-04-07_at_2.41.35_PM.png"" alt=""Screenshot_2024-04-07_at_2.41.35_PM.pngNaN"" />","<md>Recall that the eigenspace for $$\lambda$$ is given by $$\text{Null}(A - \lambda I)$$, so a basis for the eigenspace is a basis of the nullspace of $$A - \lambda I$$.</md>"
"WP11 Q1: <p>I&#39;m unsure of three things, firstly how [1 0 1]T and [0 1 0]T are obtained, secondly how the set of those two vectors along with the normal vector n = [1 0 -1]T results in a basis, and thirdly how we know each of these vectors are eigenvectors of the transformation.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8j0ruc25io%2F2b8108c8a08e2c14efb3660742905532df16f18654d512906994a2bd6b3b2c53%2FScreenshot_2024-04-07_at_2.47.21_PM.png"" alt=""Screenshot_2024-04-07_at_2.47.21_PM.pngNaN"" width=""530"" height=""130"" /></p>","<md>Since we are given a normal vector $$\vec n$$, we are interested in finding the plane which is orthogonal to this normal vector. Note that every vector which is orthogonal to $$\vec n$$ will be on this plane, so we can simply solve the system

$$\vec n^\top \vec x = 0$$

i.e. solve the system

$$\begin{bmatrix} 1 & 0 & -1\end{bmatrix} \vec x =  0$$

for $$\vec x \in \mathbb R^3$$.

This is equivalent to solving a homogenous system, hence why we know that it is a basis.

Lastly, if we think about how the transformation should act on these three vectors, we can deduce that $$T(\vec n)  = -\vec n$$ (since the normal vector should be reflected across the plane), and $$T(\vec u) = \vec u, T(\vec v) = \vec v$$ where $$\vec u, \vec v$$ are the basis vectors of the plane.</md>
<md>Since we are given a normal vector $$\vec n$$, we are interested in finding the plane which is orthogonal to this normal vector. Note that every vector which is orthogonal to $$\vec n$$ will be on this plane, so we can simply solve the system

$$\vec n^\top \vec x = 0$$

i.e. solve the system

$$\begin{bmatrix} 1 & 0 & -1\end{bmatrix} \vec x =  0$$

for $$\vec x \in \mathbb R^3$$.</md>"
"Unmarked Test: My quiz 2 was unmarked even though I wrote it. I emailed the math136 email multiple times and contacted TAs during other quizzes and I was still offered no solution. Can someone help, please? ",Did you try speaking to your instructor about this?
"Citing propositions on the final exam: <p>For the large amount of propositions that we learned, some of them have intuitive names like System Rank Theorem, but a lot of them have really long names that are hard to remember exactly, like &#34;Composition of Linear Transformations Is Linear&#34;</p>
<p></p>
<p>What are the requirements when we cite the usage of a proposition from the textbook?</p>",You don&#39;t need to cite things by name.   You won&#39;t be penalized for not using the names of the theorems. 
Diagonalizability Test: Could anyone explain the importance of the h(x) constant polynomial in this theorem please? Preferably with an example but anything helps. Thank you,"If $$h(x)$$ is not a constant polynomial, then that means the factorization of the characteristic polynomial into irreducible factors will not only include linear factors.  Therefore, there will not be $$n$$ real roots of this polynomial.  So there won&#39;t be $$n$$ eigenvalues even with repetition.  Since we don&#39;t have $$n$$ eigenvalues then it will be impossible for the geometric multiplicities of these eigenvalues to add up to $$n$$ and so it will be impossible to find a basis of eigenvectors. "
"invertible: Is it when all the eigen values are 0, then it is not invertible or is it when one of the eigen value is 0 then not invertible?",There only needs to be one eigenvalue that is 0<div><br /></div><div>Recall that $$A$$ invertible $$\Leftrightarrow$$ Null(A)={$$\vec 0$$}</div><div><br /></div><div>So having $$0$$ as an eigenvalue means that there is an eigenvector (non zero by definition) where $$A \vec x = 0\cdot \vec x = 0$$ so the Null(A) contains a vector other than the zero vector!</div><div><br /></div><div>Hope that helps!</div>
"Complex Eigenvalues (wpp 11, q2c): <p>Can someone please help show the steps for finding the complex eigenvalues for c)? Thank you.</p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2Fd491fdd6ac85c36a7759eb17054a403d087a33cfa6989f980b12ef19d036fe4b%2FScreenshot_2024-04-07_at_5.39.46_PM.png"" alt=""Screenshot_2024-04-07_at_5.39.46_PM.pngNaN"" width=""392"" height=""159"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2F817a4ab9f7f7b779ad1e47cec92a8e7f32f3f82ac70b1cb9bf37ae76846a6e2f%2FScreenshot_2024-04-07_at_5.40.06_PM.png"" alt=""Screenshot_2024-04-07_at_5.40.06_PM.pngNaN"" width=""320"" height=""112"" /></p>","There you go! Let me know if you have trouble seeing the picture <div><br /></div><div><img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqw8iusqnz5hk%2Fxnthjflsjsfy%2FIMG_2447_2024_04_07.jpeg"" /><p></p><br /></div>"
"Proposition 9.4.8 Geometric and Algebraic Multiplicities: <md>On the first line of the proof, it says “By definition, if $$\lambda_i$$ is an eigenvalue of $$A$$, then there is a non-trivial solution to $$A\vec{v}=\lambda_i\vec{v}$$.”<br /><br />Which definition tells us that?</md>","This is the definition of eigenvalues and eigenvectors (7.1.5)
Eigenvectors must be non-zero by definition. "
"Regarding Special Transformations: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fb78463ae3cb1169b7a2236fc0ef8c9c3a2c50efdf1213299bdb610c804818972%2FScreenshot_2024-04-07_at_7.22.14_PM.png"" alt=""Screenshot_2024-04-07_at_7.22.14_PM.pngNaN"" /></p>
<p></p>
<p></p>
<p>Would these type of questions not be asked then? Since the solution to this question involves the formula for the reflection matrix. Asking because I saw on piazza that we don&#39;t need to memorize the standard matrix for such transformations. </p>","If we gave you this question on the exam, we would also give you the standard matrix of the reflection transformation. "
"similar linear operator matrix: <p>If $$[T]_B$$ and $$[T]_C$$ are similar, we have $$[T]_C ={}_{C}[I]_B [T]_B {}_{B}[I]_C$$.</p>
<p>since the change base matrix can also change matrix, $$[T]_C =[T]_C {}_{B}[I]_C$$</p>
<p>$$([T]_C)^{-1}[T]_C =([T]_C)^{-1}[T]_C {}_{B}[I]_C$$</p>
<p>Does that means $$I_n= {}_{B}[I]_C$$? But $$I_n$$ will change nothing, how can it is a change base matrix?</p>
<p>Thank you</p>",The second line that you wrote above is not correct. 
"Quiz 5 Question 3c): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2Fd3a156c08eb85166968106e5177cb23135b495f3fa8d29406a16fe7f712e5dfe%2Fimage.png"" alt=""image.png"" /></p>
<p>Can someone explain why this is true? I&#39;m struggling to see the relationship</p>","<p>W here is defined as {$$\vec x \in \mathbb{R}^3 : \vec v \cdot \vec x = \vec 0 $$ for all $$\vec v \in S$$}. Recalling what the dot product looks like:</p>
<p></p>
<p>$$\vec v \cdot \vec x = v_1x_1 + v_2x_2 + v_3x_3 = 0$$</p>
<p></p>
<p>Now, we&#39;re given that this must hold for all $$\vec v \in S$$ so we have this:</p>
<p>1st vector in S:      $$ x_1 + x_2 + x_3 = 0$$</p>
<p>2nd:                      $$ 2x_1 + 2x_2  = 0$$</p>
<p>3rd:                       $$ 3 x_3 = 0$$</p>
<p>4th:                       $$ 3x_1 + 3x_2 + 4x_3 = 0$$</p>
<p></p>
<p>I just replaced each $$v_i$$ with the corresponding component of the vectors from S, I can elaborate more if this is unclear.</p>
<p></p>
<p>So now we can stick this into an augmented matrix! Also notice how it is just the transpose of $$A$$.</p>"
"homogenous non-linear systems: I had two questions: (1) Can there be a homogenous non-linear system? If yes, what&#39;s a good example of them? (2) Can a homogenous non-linear system be a subspace?","<p>Yes, there can be a homogenous non-linear system.  For example  $$x^2=0, y^2=0$$ is a homogeneous non-linear system. </p>
<p></p>
<p>For your second question, I assume you are asking whether the solution set to a homogeneous non-linear system could be a subspace.  The answer is yes.  Consider my example above.  The solution set to this system consists of only the zero vector in $$\mathbb{R}^2$$.  And the set which consists of only the zero vector is a subspace. </p>"
"Quiz 5 solution: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2Fb2f450adf6192cbb3179f461a7ff98b6c45b214159789386361f9cee4fbe58ca%2FScreenshot_2024-04-08_at_9.49.04_AM.png"" alt=""Screenshot_2024-04-08_at_9.49.04_AM.pngNaN"" /></p>
<p>should this be $$\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}$$ instead</p>","Yes, the zero vector should have 4 components. "
"Q2d Factoring: <p>I&#39;m confused about how the answer of eigenvalues to this matrix is 2 (alg mult. 2) and 8. When factoring I keep ending up with 2, 4 and 6... Is there something im missing here?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2Fc8a8739e7134d51099eb32c0d3db026b55ab586095e6c83fa93cd0184ddb9329%2FScreenshot_2024-04-08_at_10.13.14_AM.png"" alt=""Screenshot_2024-04-08_at_10.13.14_AM.png"" width=""200"" height=""100"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2Ff39ea81044d411bc54824f2e3676eb0a2db2b97ac0b809e4a80d274513a83f10%2FScreenshot_2024-04-08_at_10.13.29_AM.png"" alt=""Screenshot_2024-04-08_at_10.13.29_AM.pngNaN"" width=""454"" height=""37"" /></p>",You must be making an arithmetic mistake because I checked the solution with mathematical software and the eigenvalues are definitely only 8 and 2. 
"Final coverage: For 5.6 special matrices, are we only responsible for rotation by theta that kind of problem for the final? If not, what else should we also look at. Thank you!",You could be asked questions about the other types of transformations but if you need the standard matrix for these transformations we would give them to you.
Question about Subspaces: is Fn considered a subspace of Fn?,"Yes, by the subspace test."
Class scores distribution for Q5?: Just wonder if it’s going to be posted or not?,I sent an inquiry about this to the person in charge of marking.  We&#39;ll see what they say. 
"Proving linear independence by taking determinant: <p>Hello,</p>
<p></p>
<p>I just wanted to ask if it&#39;s ok to use the determinant when proving linear independence of a span. When we put the span vectors into a matrix &#39;A&#39; as columns we should get the RREF being the identity if the vectors are LI, right? So by our invertibility criteria this is equivalent to det(A) != 0</p>
<p></p>
<p>Im not sure if this is obvious or completely wrong so I am just asking to be sure. If it is true do we need to explain this logic on the final, or is it implied?</p>
<p></p>
<p>Thanks!</p>","<p>Just realized this is proved in the Practice problems:<br /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwca7ky8m6lh%2Ff3eb3a08f6129bbe86e92a24f5fb0be89e472623c5f2a79bde057ddf6f7a6f40%2Fimage.png"" alt=""image.pngNaN"" width=""1022"" height=""68"" /></p>
<p></p>"
"Question about invertability: <p>Is it true that?</p>
<p>If A, B is invertible,</p>
<p>$$(A^{-1}B^{-1})$$ is also invertible?</p>",It is true.   Can you prove it? 
"WPP11 Q1: <p>Here is the question and solution respectively:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2Fadbf42f62870cae0431a86c52c31ece82ce7aa24037664f3e96799308fce0063%2Fimage.png"" alt=""image.pngNaN"" width=""712"" height=""100"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2Fe2c9977ee33f1f01c3f670e36c1a875f3f3459e489a1d8e46018fafd25fce6fa%2Fimage.png"" alt=""image.pngNaN"" width=""714"" height=""175"" /></p>
<p>I&#39;m confused as to how the two LI vectors in the plane are determined since there is no given equation for the plane. What does the fact that it&#39;s through the origin tell us?</p>",You have the normal to the plane and you know that it goes through the origin so the equation of the plane is $$x-z=0$$. 
"Week 4 practice problems Q1c: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F348b61fe1734b5450326a9613db926336551da8a64d46ea184766ba7b729b435%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>isnt the answer to this wrong?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2Ff48073abe4b03a0635ec8fb2e480173f852df4d8016d14cee9568b3ca3c16f4f%2Fimage.png"" alt=""image.png"" />shouldnt it be -3, 20, -3?</p>","<p><span style=""text-decoration:line-through"">Yes, you are correct.  I&#39;ll update the solutions.</span></p>
<p><span style=""text-decoration:line-through""></span></p>
<p>Actually you are incorrect.  I think there is a problem with your pdf viewer.  It&#39;s not showing the negative sign in front of the 1 in the second vector.  <br /></p>
Yes, you are correct.  I&#39;ll update the solutions. "
"Quiz 1 - MC 1b: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F3b4441a924beba508e90d0c3ba2ba7e3c3f116bdf15684392ee4e75dce880c63%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Faa65d76f591165bc17dabbf94fb5784ccc53fa4d079122b7c921fa55ab0c0437%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>Can anyone clarify the idea behind this solution?</p>","If $$Ax=b$$, then $$A(cx)=cAx=cb$$. "
math136@uwaterloo.ca Not Responding Absence Request: What should I do if both math136@ueaterloo.ca and my instructor not responding my absence of Quiz 5 email? My grade was not exempted on Learn. ,"Actually I see that grades are being exempted on LEARN.  I would be patient and weight or try contacting your instructor again. 
Your grade will not be exempted on LEARN.  The weight of the quiz needs to be transferred to the final exam and I&#39;m quite certain this transfer will not happen on LEARN.  This transfer will be done manually before submitting your final exam. "
Invertibility and linear dependence/independence: What is the connection/theorem relating invertibility and linear independence and dependence,"<p>Not sure if it&#39;s explicitly stated in a theorem anywhere (maybe it is!) but as far as the logic is concerned lets consider a set of n linearly independent vectors.</p>
<p></p>
<p>By pivots and linear independence we know that we can put these vectors in a matrix and row reduce. The fact that we have specifically n vectors is important, it means that these vectors create a square matrix. Since the vectors are Linearly independent, you would get that every column gets a pivot (therefore every column vector is worth keeping in the set).</p>
<p></p>
<p>But then, you have a matrix where the RREF is the identity. This is a statement in our invertibility criteria. If a matrix has an RREF which is the identity matrix, it is invertible. So the matrix constructed from linearly independent vectors is invertible.</p>
<p></p>
<p>You also get a bunch of other things from the invertiblilty criteria for free, like the determinant of the matrix not being zero, the corresponding linear transformation being one-to-one/onto, etc.</p>
<p></p>
<p>The reverse here is also true. An invertible matrix means the vectors which make its columns are linearly idependent.</p>"
"SHowing a proposition from class: <p>How would we do the following question? Do we write the two matrices as n x n matrices with arbitrary length and do a ton of algebra?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2F6e46486075ee17bacfac5bab3bf47c9355fa064c68ec526ac905a62396c78372%2FScreen_Shot_2024-04-08_at_4.54.07_PM.png"" alt=""Screen_Shot_2024-04-08_at_4.54.07_PM.pngNaN"" /></p>",you must show that det($$A -\lambda I$$) = det($$B- \lambda I$$). A hint would be that $$I = PP^{-1}$$ where P is the invertible matrix such that A = $$PBP^{-1}$$.
"7.6 Exercise Proof: can anyone give suggestions as to what theorems/ideas to use to prove these?<br />specifically (b), but discussions for all 4 would be helpful<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F75c927796f79389c291d949663e028e2d99df1212f79033da543894864e23b12%2FScreenshot_2024-04-08_at_5.43.49_PM.png"" alt=""Screenshot_2024-04-08_at_5.43.49_PM.pngNaN"" />","for b, you must show that det($$A -\lambda I$$) = det($$B- \lambda I$$). A hint would be that $$I = PP^{-1}$$ where P is the invertible matrix such that A = $$PBP^{-1}$$.
for b, you must show that det(A -$$\lambda I$$) = det(B- $$\lambda I$$). A hint would be that I = $$PP^{-1}$$"
"Basis for F^n: <p>Hello, could anyone clarify the main ideas for when to tell a basis is a spanning set for Fn?</p>
<p></p>
<p>I find all the theorems to be very closely related but also different - but here&#39;s what I&#39;m thinking as of right now:</p>
<p></p>
<p>1. Let S be subspace. Span(S) = F^n iff rank(A) = n</p>
<p>2. This is the same idea as V = F^n iff dim(V) = n. Since V is obtained by taking the pivot columns of A, dim(V) = n = rank(A).</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fa20f037e4e88387b93a1917b8c93bcb34ce7932b0da63abb03ab4fabb0287d5e%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>3. We also note that if V is basis for F^n =&gt; f the basis has n vectors? (not an iff statement)</p>
<p>4. But if we say V is linearly independent and has n vectors =&gt; V is basis for F^n?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fc5539ca0a6eb197a6e820f34b0cdc2396afe1fc544efe63b1a5b4c9296eda652%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>Essentially, I just want to clarify if these ideas can be clarified/distinguished concisely by anyone? Thanks!</p>","<md>It looks like you may be confusing the idea of a subspace and its basis.

I'm not exactly sure what some of your questions are asking since you didn't define some objects, so I will try to give an overview.

We always say that a set of vectors $$B$$ is a basis for a subspace $$W$$. This means 2 things:
1. $$B$$ is linearly independent.
2. $$\text{Span}(B) = W$$

From here, we define the dimension of a subspace $$\dim(W)$$ to be the number of vectors in any basis $$B$$ for $$W$$. Note that this definition is consistent because every basis has the same number of vectors in any basis. By convention, we take $$\emptyset$$ to be a basis for the trivial subspace $$\{\vec 0\}$$.

Conversely, any set satisfying 1. and 2. is a basis for $$W$$.

We have some bounds: for a subspace $$W$$ of dimension $$d$$, if $$B \subseteq W$$ is linearly independent, then there are at most $$d$$ vectors in $$B$$. This means that if $$B$$ has exactly $$d$$ vectors, and is linearly independent, then $$B$$ must be a basis for $$W$$.

Also, if $$B \subseteq W$$ is spanning, i.e. $$\text{Span}(B) = W$$, then $$B$$ has at least $$d$$ vectors in $$B$$. This means that if $$B$$ has exactly $$d$$ vectors, and is spanning, then $$B$$ must be a basis for $$W$$.

From these two ideas, we can gather two different ways of thinking of a basis $$B$$ for a subspace $$W$$:
1. $$B$$ is a maximal linearly independent set
2. $$B$$ is a minimal spanning set

Here, the term maximal is used to mean that you can't add any more vectors to the set which preserve the linear independence of the set, and minimal is used to mean that there are no spanning sets with less vectors.

Does this help?</md>"
"diagnonizible: In the test if they ask if A is diagonalizable. Is it fine to say &#34;Yes, by diagonalizability test&#34; or Do we have to provide more details?","<md>Yes, as long as you demonstrate that the algebraic multiplicity equals the geometric multiplicity for each eigenvalue, or, conversely, that there is an eigenvalue whose algebraic multiplicity differs from its geometric multiplicity.</md>"
"What definition is this?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2Ffa8e512db64a258a826e1884056fa6f24479421ffbf25220256bfd27072871ff%2Fimage.png"" alt=""image.png"" />","<md>$$\vec x \in W \iff \vec v \cdot \vec x = 0, \forall\ \vec v \in S \iff \vec v^\top \vec x = 0, \forall \vec v \in S, \iff A^\top \vec x = \vec 0$$

The last step is seen by putting together the 4 equations $$\vec v^\top \vec x = 0, \forall\ \vec v \in S$$ into the matrix.</md>"
"Quiz 2 Question 1b): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F1770a80be49dccb523e0ff3a2fa1928c71dc67a526e9af53128110837f776e75%2Fimage.png"" alt=""image.png"" width=""754"" height=""85"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F62fa5c5e0d44ba804f451f2410f919034d10f7e61aef11f49939de94c0c9990e%2Fimage.png"" alt=""image.png"" width=""772"" height=""133"" /></p>
<p></p>
<p>How does c(Ax) = cb imply Ax = cb is consistent?</p>","<p>I was also wondering this: @1151</p>
<p></p>
<p>It&#39;s just the idea that we hope there&#39;s some solution, v such that Av = cb.</p>
<p></p>
<p>In this case, we plug v = cx to get A(cx) = c(Ax) = cb, as desired.</p>
<p></p>
<p></p>"
"WP9 Q7: Would this be a valid proof?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8j0ruc25io%2Fdb71c6f0a0da556974b14433abaacbce5eb69ef01bec0b10a6397b4459d63893%2FScreenshot_2024-04-08_at_10.02.48_PM.png"" alt=""Screenshot_2024-04-08_at_10.02.48_PM.pngNaN"" width=""644"" height=""227"" />","<md>You have a good idea, but the questions asks to show that any subset is linearly independent.

You showed that this result is true for $$\{\vec v_2, \ldots, \vec v_k\}$$, and your proof idea does generalize, but you should still be careful with how you notate this.

A better way would be to write ""Suppose $$W \subseteq S$$ where $$W = \{\vec w_1, \ldots, \vec w_\ell\}$$ for some $$1 \le \ell \le k$$"", or something along the lines of this.

You may also need to separately check the case where $$W = \emptyset$$.</md>
<md>You have a good idea, but the questions asks to show that any subset is linearly independent.

You showed that this result is true for $$\{\vec v_2, \ldots, \vec v_k\}$$, and your proof idea does generalize, but you should still be careful with how you notate this.

A better way would be to write ""Suppose $$W \subseteq S$$ where $$W = \{\vec w_1, \ldots, \vec w_\ell\}$$ for some $$1 \le \ell \le k$$"", or something along the lines of this.</md>"
"Find Eigenvalue for 3 by 3 matrix: I was doing a practice a problem in find eigenvalues in week11 problem. When I was working on a 3 by 3 matrix, it is very time consuming, when there is no and 1 in that matrix. Anyone have any suggestion for shortcuts. Thank you.  ","<md>For 3 by 3 matrices, the best shortcut I have is to memorize the determinant of a 2x2 matrix, where

$$\det\left(\begin{bmatrix} a & b \\ c & d\end{bmatrix}\right) = ad - bc$$

There are other tricks you can look up online such as this (https://demonstrations.wolfram.com/33DeterminantsUsingDiagonals/), although I find this to be an unnecessary thing to memorize and I have never used this.</md>"
"Quiz 5 Question 1 (e): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl3a6u2rficd4fp%2Ff869cd665978d332ca5ecee6b96ab7e022d477eb044b797565a33f3c188e4860%2FScreenshot_2024-04-08_at_10.32.19_PM.png"" alt=""Screenshot_2024-04-08_at_10.32.19_PM.pngNaN"" width=""789"" height=""136"" /></p>
<p></p>
<p>For this question from quiz 5, I put false, thinking of the following counter-example:</p>
<p>if B = {$$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$, $$\begin{bmatrix} 0 \\ 1 \end{bmatrix}$$} and C = {$$\begin{bmatrix} 0 \\ 1 \end{bmatrix}$$, $$\begin{bmatrix} 1 \\ 0 \end{bmatrix}$$}, since they are ordered bases, they are different but both spans $$F^2$$. </p>
<p>For a vector $$\begin{bmatrix} 7 \\ 7 \end{bmatrix}$$ as an example, the coordinate vector for both bases would be $$\begin{bmatrix} 7 \\ 7 \end{bmatrix}$$. <br /><br />I am having trouble understanding why my counter-example doesn&#39;t work and therefore it&#39;s a true statement. Is there any logical flaw in my understanding or my counter-example? I would appreciate any explanations. </p>","<p>It doesn&#39;t satisfy that $$[\vec{x}]_\mathcal{B} = [\vec{x}]_\mathcal{C}$$ for every $$\vec{x}$$. For instance, if $$\vec{x}=[1~0]^T$$, $$[\vec{x}]_\mathcal{B}=[1~0]^T$$, but $$[\vec{x}]_\mathcal{C}=[0~1]^T$$</p>
<p></p>
<p>Also I think you didn&#39;t actually mean this in your wording, but note that the counter-example not working doesn&#39;t mean its a true statement.</p>
<p></p>
<p>You can show the statement is true, since you can do something similar to column extraction with the two sets. If $$[\vec{u_1}]_\mathcal{B}=[1~\dots~0]^T$$, and this is the same for $$\mathcal{C}$$, then $$\vec{u}_1=\vec{v}_1$$, you can repeat the same with all the other vectors</p>
<p>It doesn&#39;t satisfy that $$[\vec{x}]_\mathcal{B} = [\vec{x}]_\mathcal{C}$$ for every $$\vec{x}$$. For instance, if $$\vec{x}=[1~0]^T$$, $$[\vec{x}]_\mathcal{B}=[1~0]^T$$, but $$[\vec{x}]_\mathcal{C}=[0~1]^T$$</p>
<p></p>
<p>Also I think you didn&#39;t actually mean this in your wording, but note that the counter-example not working doesn&#39;t mean its a true statement</p>
It doesn&#39;t satisfy that $$[\vec{x}]_\mathcal{B} = [\vec{x}]_\mathcal{C}$$ for every $$\vec{x}$$. For instance, if $$\vec{x}=[1~0]^T$$, $$[\vec{x}]_\mathcal{B}=[1~0]^T$$, but $$[\vec{x}]_\mathcal{C}=[0~1]^T$$"
Determining normal vector of Plane: Hi I am wondering when finding a normal vector for a plane does it matter which direction vector you take first in the cross product (or are there more than one normal vectors for a plane depending on the order you pick for the cross product of the direction vectors).,"<p>Taking the cross product of the direction vectors should always give you a valid normal vector, so it doesn&#39;t matter. ($$\vec{a}\times\vec{b} = -(\vec{b}\times\vec{a})$$)</p>
<p></p>
<p>There are infinitely many normal vectors in $$\mathbb{R}^3$$ (scalar multiples of each other), but you just need to have a normal vector</p>
<p>Taking the cross product of the direction vectors should always give you a valid normal vector, so it doesn&#39;t matter.</p>
<p></p>
<p>There are infinitely many normal vectors in $$\mathbb{R}^3$$ (scalar multiples of each other), but you just need to have a normal vector</p>
<p>Taking the cross product of the direction vectors should always give you a valid normal vector, so it doesn&#39;t matter.</p>
<p></p>
<p>There are infinitely many normal vectors (scalar multiples of each other), but you just need to have a normal vector</p>
<p>Taking the cross product of the direction vectors should always give you a valid normal vector, so it doesn&#39;t matter.</p>
<p></p>
<p>There are infinitely many normal vectors, but you just need to have a normal vector</p>"
"Quiz 4 Question 3c): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2Faad927ef7e9bfe11a7ea52ee6f351ffa9bc5e1e259e83243770b1a1717f75b16%2Fimage.png"" alt=""image.png"" width=""548"" height=""160"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2Fe0d02f756584a13e931fb4e7d313af0b6f28fc06d3e2bdba433286f9ad64edf1%2Fimage.png"" alt=""image.png"" width=""678"" height=""124"" /></p>
<p>How did they get [1 i]^T?</p>
<p>When I solved for (A-iI)x = 0, I got [1 1]^T as an eigenvector.</p>","How did you get $$[1~1]^T$$? I think it should row reduce to $$\begin{bmatrix}1&-i\\0&0\end{bmatrix}$$
<md>Do you mean for part c) or for your chosen value of $$a$$? For $$a = -1$$, and $$\lambda = i$$, the eigenspace is

$$\text{Null}\left(\begin{bmatrix} -i & -1 \\ 1 & -i\end{bmatrix}\right) = \text{Null}\left(\begin{bmatrix} -i & -1 \\ 0 & 0\end{bmatrix}\right) = \text{Span}\left\{\begin{bmatrix} 1 \\ i\end{bmatrix}\right\}$$</md>"
"WP 9 Q4: <p>Could I argue as proof that:</p>
<p><br />Since $$\{\vec{v}_1,...,\vec{v}_n\}$$ is already linear independent and $$\vec{x}$$ cannot be a linear combination of $$\{\vec{v}_1,...,\vec{v}_n\}$$ (since it doesn&#39;t belong to its span), then $$\forall\vec{x} \in \{\vec{v}_1,...,\vec{v}_n, \vec{v}\}, \vec{x} \text{ cannot be a linear combination of any other vectors.}
$$ (none of the vectors can be a linear combination of any other vector(s))</p>
<p><br />Then, by linear dependence check, $$\{\vec{v}_1,...,\vec{v}_n, \vec{v}\}$$ cannot be linearly dependent, thus making it linearly independent.<br /><br />And if this proof is valid, what might be a more mathematically notated way to show this?</p>","I think you have the right idea but I&#39;m not sure if the proof is complete, you have argued that $$\vec{x}$$ isn&#39;t a linear combination of the other vectors, but how do you know that for instance $$\vec{v}_1$$ is still not a linear combination of the other vectors, since we now have included $$\vec{x}$$ into the other vectors? I think after showing that you can&#39;t express any $$\vec{v}_i$$ in terms of the other vectors, the proof is complete. 
I think you have the right idea but I&#39;m not sure if the proof is complete, you have argued that $$\vec{x}$$ isn&#39;t a linear combination of the other vectors, but how do you know that for instance $$\vec{v}_1$$ is still not a linear combination of the other vectors, since we now have included $$\vec{x}$$ into the other vectors?"
"Does the characteristic polynomial C_A(\lambda) always have cn = (-1)^n?: &#34;Always&#34; meaning for distinct, non-distinct eigenvalues, as well as over R and over C?",Yes
Linear Transformation is Invertible: Could somebody explain what it means when it is said that a linear transformation is invertible? Does it only mean that the linear transformation is onto and one-to-one?,"Yes, this gives that an inverse function exists
Yes"
"To pass this course, do we need to pass the final or pass the weighted exam?: ",No. 
"Week 9 practice problems Q 2: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F5926ecc84e9d4f4b941a6cebe54ac4c5d641dd1149384cacb4284dde12703654%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>wouldnt V U W = span {[1 0]^T, [0 1]^T} so [1 1]^T would be an element of this set as its equal to [1 0]^T &#43; [0 1]^T?</p>","No, the union is not equal to the span you have indicated. "
"linearly dependence &#43; consistency: <div dir=""auto"">
<div>
<div>
<div>
<div>
<div>
<div dir=""auto"">
<div>are linearly independent systems always consistent and vice versa?</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>","Remember, only a set or list of vectors can be classified as linearly independent. Moreover, only a system of equations can be classified as consistent. It is very important to remember the type of object you are working with. "
"Practice problems 9 Q4: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F18a41c95ce9f4664b11da4375299126d6403bcb8ce05f73894528a68b68f996a%2Fimage.png"" alt=""image.png"" />But isnt the trivial solution also a solution? so we would have 0/0 for all the terms?","In the second line of the proof, it is assumed that we are working with a non-trivial solution. "
"Theorem 8.5.6: In the theorem Basis From a Spanning Set or Linearly Independent Set, the first statement is that If Span(S) = F𝑛, then there exists a subset B of S which is a basis for F𝑛. If S is linearly independent, then by Dimension is Well-Defined, is B simply S itself?",Yes. 
"Span the subspace?: <p>If V is a subspace, can we do span(V)? Will span(V) equal to V because subspace is closed under addition and multiplication?</p>
<p>Thank you</p>","<p>If $$V$$ is a subspace, then $$\text{Span}(V) = V$$.  </p>
<p></p>
<p>(There is going to be a lot of redundancy in this spanning set though.)</p>
I believe so as V is a set of vectors. And span(V) = V by closure. Although it&#39;s closure under addition of 2 vectors, it would generalize to any finite linear combination of vectors."
"W11P Q2: geometric multiplicity, nullspace, and invertibility: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8ul9ciu7gn%2Fb5925668ad3639ad0665f7d2472e47c26d14e65bac7ee5c1d14319551c04ac91%2FScreenshot_%28208%29.png"" width=""664"" height=""50"" alt="""" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8ul9ciu7gn%2Fcf145f607b909bdc6f98a34e30d93111d96c43375db19c26677a63aafb1d3ee8%2FScreenshot_%28207%29.png"" width=""716"" height=""149"" alt="""" /></p>
<p>I don&#39;t understand the logic of the second sentence?</p>","There is only one eigenvalue.  It&#39;s value is $$\lambda$$ and it&#39;s algebraic multiplicity is 2.  If the matrix is diagonalizable, then the geometric multiplicity of $$\lambda$$ must be 2. "
"Inverse: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2F0c6e1df8890e60fc7f16e17090aa3454eaf72e9ca6c59469e24c686e102d7320%2Fimage.png"" alt=""image.png"" /></p>
<p>Without proof can we write:AA = I_n is A invertible and an inverse of itself?</p>
<p>Or do we have to prove it</p>
<p>$$det(A^2) \neq 0$$, $$det(A) \neq 0$$</p>","Are you considering the special case where $$A^2=I_n$$?  If that is true, you can immediately say that $$A$$ is invertible and its inverse is itself. "
"0 dimension: <p>What is dim(V)? If V is the trivial subspace that only contains a vector 0. Will the dimension be 0?</p>
<p>Thank you</p>",Yes.  $$\dim (\{ \vec{0} \} )=0$$
"Transpose wpp11 q7: <p>I&#39;m confused about this line. How is the transpose distributive and how is the last implication on the yellow line valid?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8gwt19q556%2F33f12f5e64d1fd2708127264bc36302bd3fbfe5971afe05d0959b265ca1bb477%2FScreenshot_2024-04-09_at_12.41.33_PM.png"" alt=""Screenshot_2024-04-09_at_12.41.33_PM.pngNaN"" width=""449"" height=""311"" /></p>","<p>$$(AB)^T=B^TA^T$$.  See properties of transpose. </p>
<p></p>
<p>If you have that $$CD= I_n$$,  then $$D^{-1} = C$$. </p>"
"Eigenvalues and eigenspaces over real: <p>If an eigenvalue question asked for the eigenvalues and eigenspaces of a matrix A in Real, and we had an characteristic like this, where some of the roots only exist as complex numbers, what do we say? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2F97ae359a21806def03dbfe499624c4c7bc5505c82ffcb0bd8cee863d58985714%2FScreen_Shot_2024-04-09_at_12.49.45_PM.png"" alt=""Screen_Shot_2024-04-09_at_12.49.45_PM.pngNaN"" width=""571"" height=""36"" /></p>
<p>Would we say the only real eigenvalue is 0 and only calculate the eigenspace of lambda zero?</p>
<p></p>",The answer to your second question is yes and yes.  
"Week 9 - PP Q5: <p>If a set of vectors are linearly dependent, that doesn&#39;t necessarily mean that the sum of the scalars is not 0 right? </p>
<p></p>
<p>In that case, any hints to continue this proof (I used contrapositive rather than direct proof as shown in solutions)</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fecc02db1b5f8da20ebf51eb504b12018c2d26ee8926273f0227a3c4eb79f153b%2Fimage.png"" alt=""image.pngNaN"" /></p>",You&#39;re close! Try: Case 1: $$\sum c_i=0$$ and Case 2: $$\sum c_i\neq 0$$. 
"Week 9 - ppq5: <p>Is my proof correct? Can I answer it by refernecing Q4a? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2F148ffe691ffc79239fb3d4bffc7bc2000ea9a8cd6fd734df2a747cfe0297d00c%2Fimage.png"" alt=""image.pngNaN"" /></p>",Looks good! 
"Quiz 5 Q1b - Adding Vector to Every Element in Spanning Set: <p>Hi!</p>
<p></p>
<p>I&#39;m having difficulty seeing why the question below is false.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcm6iy84zh%2Fe01cc5aa62d945589a1f400d80fb993e3fd50351525243910aaa8838552b3111%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p></p>
<p>Initially I had thought that because $$\vec{x} \in V$$, $$\vec{x}$$ can be written as a linear combination of $$\vec{v_1} ... \vec{v_m}$$, and that <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcm6iy84zh%2F860d41475c67493971f3c9fabee47c0ced10864fdaefab023d9014d6a162e359%2Fimage.png"" alt=""image.png"" /> can be split to become $$Span(\vec{x}, \vec{v_1} ... \vec{x_m}) = Span(\vec{v_1} ... \vec{x_m}) = V$$.</p>
<p></p>
<p></p>
<p>I am further confused by the counter-example:<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcm6iy84zh%2Fee2cb9cffd6e9e1033917c535d26da5ed8c2df233d7a29fcd9a218b7dfb17551%2Fimage.png"" alt=""image.png"" />. Doesn&#39;t this set not take into account that $$\vec{x}$$ is added to every vector in the original set? Should it not be $$Span({\vec{0}}, \vec{e_2} - \vec{e_1}, \vec{e_3} - \vec{e_1})$$ which can be split to become $$Span({\vec{e_1}}, \vec{e_2}, \vec{e_3})$$?</p>
<p></p>
<p>I would appreciate the clarification/if anyone could point out where my misunderstanding is. Thank you!</p>","<p>Consider $$V = \text{Span}\{\vec{v}_1\}$$ where $$\vec{v}_1\neq 0 $$. Now, take $$\vec{x}=-\vec{v}_1$$.</p>
<p></p>
<p></p>
<p>Also, I am not sure what you mean by splitting the spans, I think they made a typo with the counter-example but the counter-example still works when you write it correctly like you did</p>
<p>Consider $$V = \text{Span}\{\vec{v}_1\}$$ where $$\vec{v}_1\neq 0 $$. Now, take $$\vec{x}=-\vec{v}_1$$.</p>
<p></p>
<p></p>
<p>Also, I am not sure what you mean by splitting the spans. </p>
Consider $$V = \text{Span}\{\vec{v}_1\}$$ where $$\vec{v}_1\neq 0 $$. Now, take $$\vec{x}=-\vec{v}_1$$
Consider $$V = \text{Span}\{\vec{v}_1\}$$. Now, take $$\vec{x}=-\vec{v}_1$$
Good catch on the typo in the solutions.  I fixed the typo. "
"A plane is a span of two vectors if it passes through the origin,: and a span of three vectors if it does not. <br /><br />Is this correct?","<p>A plane passing through the origin is a span of two <strong>linearly independent</strong> vectors.</p>
<p></p>
<p>A plane not passing through the origin would not be the span of any vectors, since it is no longer a subspace (does not contain $$\vec{0}$$)</p>
<p></p>
<p>I think you got confused by that a plane not passing through the origin is equal to a plane passing through the origin plus another vector</p>
<p>A plane passing through the origin is a span of two <strong>linearly independent</strong> vectors.</p>
<p></p>
<p>A plane not passing through the origin would not be the span of any vectors, since it is no longer a subspace (does not contain $$\vec{0}$$)</p>
<p>A plane passing through the origin is a span of two <strong>linearly independent</strong> vectors.</p>
<p></p>
<p>A plane not passing through the origin would not be the span of any vectors</p>
If a plane does not go through the origin it is not a subspace.  It is not closed under addition and therefore, does not have a spanning set. "
"Final Content: Will the distribution of chapters in the final be uniform, or will be there a greater emphasis on post midterm content? Just so we have a better idea of what to cover more thoroughly ","<p>I think most of the topics are built ontop of previous topics, so I would expect for essentially everything to be tested. (They can&#39;t test you on eigenvalues, determinants, without testing you on matrix multiplication for example)</p>
<p></p>
<p>Also this may be related @1120</p>
You can expect that more emphasis will be given to material that was covered since the midterm.  But the material in this course really builds on itself so you can&#39;t forget the pre-midterm material. "
"non vector space example typo?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fa80e47842aa0d346174639f3331a315a68fc25da3a20d54a51c242cb52e4f41f%2Fimage.png"" alt=""image.png"" width=""518"" height=""587"" /></p>
<p>Is <strong>V4</strong> in these two examples supposed to be <strong>V3</strong>?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fe7f69940f63ac60cc5653b0d3a3cc7cd4c46a42e81290b9c865c45596bf460a4%2Fimage.png"" alt=""image.png"" width=""613"" height=""110"" /></p>",Thanks for catching this.  I will updated the typo.
"Does this statement go both ways?: <a href=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl66pr4f64oe3ti%2Fef2a2a7036a402a5586dd6a3ee4ec23b066482ebe83321f8d16e6e96a21549bd%2FScreenshot_2024-04-09_at_3.30.02_PM.png"" target=""_blank"" rel=""noopener noreferrer"">Screenshot_2024-04-09_at_3.30.02_PM.png</a><br /><br />might be a dumb question but just making sure",Do you mean is it iff? This is a definition not a theorem/proposition so it is iff
"WP11 Q1: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4vly2vo7m4%2Ff6b5e1e10c215b3689d079fad91ad9c4dcffdf12e4d577a0484ba5952ae28e1a%2FScreenshot_2024-04-09_at_3.54.21_PM.png"" alt=""Screenshot_2024-04-09_at_3.54.21_PM.pngNaN"" /></p>
<p></p>
<p>Why do similar matrices have the same characteristic polynomial?</p>",This is an exercise given on pg 185 of the course notes. 
Transformation: Can a linear transformation always be represented as a matrix vector multiplication?,Yes
Determinant for 3x3: Are we allowed to use quicker ways to calculate the determinant of a 3x3 matrix on the final?,"<md>Yes</md>
I’m pretty sure the sentiment is that « if it works, it works ». I’ll try to find a piazza post on this but it might take a while. <div><br /></div><div>The exception to this is when they say to use a specific method to do something, ie use Cramer’s Rule to solve the system </div>"
Inner products on final?: Are we getting tested on standard inner products on the final?,<md>No</md>
"q3 practice final: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwatgzzdt48x%2F2616d54392e8f167d8ff844ba338d75101bdd1d03c861437f7bf0a241cfbcff0%2FIMG_7005.jpg"" alt=""IMG_7005.jpgNaN"" width=""287"" height=""383"" /></p>
<p>Was wondering if I could solve it this way but not sure if I made an error or you cannot solve it like this</p>","<md>If you are computing $$\det(A - \lambda I)$$, you cannot row reduce $$A$$, then compute $$\det(\text{RREF}(A) - \lambda I)$$; you would have to compute $$\det(\text{RREF}(A - \lambda I))$$, and track which row operations you perform (e.g. if there are column swaps or scaling, they would change the determinant).</md>"
"What are some common standard matrices of transformations we should know?: I&#39;m talking about reflection about some line, rotation by some degrees, projection, or translations in as standard matrices, will we be expected to memorize any of them?",<md>You don't have to memorize these standard matrices.</md>
"Formula Sheet: Would formula sheet be provided in the finals?
Do I have to explicitly write the name of theorem everytime i use it?","no formula sheet
<md>No formula sheet is provided. You can say ""by property"" or ""by thoerem proven from class"" if you don't remember its name.</md>
<md>No formula sheet is provided.</md>"
"Question about Linear Independence: <p>If we had a matrix A made with [v_1 v_2 ... v_6] where the REF turns into:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2Fbf3101cf80ecc0c320d3571d1e854080c055c72aff4a40135a6a790d45a997e2%2FScreen_Shot_2024-04-09_at_4.48.21_PM.png"" alt=""Screen_Shot_2024-04-09_at_4.48.21_PM.pngNaN"" width=""517"" height=""138"" /></p>
<p>I understand {v_1 v_2 v_4 v_6} would be linearly independent but Would {v_1 v_3 v_5 v_6} also be LI?</p>","<md>Yes: imagine if you swapped the columns of your basis (i.e. swap column 2, 3, and swap column 4, 5).</md>"
"Week 10 - PP Q5b: <p>Can anyone further clarify why ei = [vi]_B? Thanks.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fd3ce2825362b65b903f2ce3c054552e6223ef5af34985019f67ade864d7fef58%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2F0406ac94d662b641afb52f64750984913671b20bb5af16907df53a6cb78dd979%2Fimage.png"" alt=""image.pngNaN"" /></p>","We want to write $$\vec v_i$$ as a linear combination of $$\vec v_1,\ldots,\vec v_n$$, i.e.<div><br /></div><div>$$\vec v_i = c_1\vec v_1+\cdots +c_n\vec v_n$$.</div><div><br /></div><div>What values of $$c_1,\ldots,c_n$$ should we choose?</div>"
"Chapter 9: <p></p>
<p></p>
<p>Could someone please explain how to solve this <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F6e4485903f0c9c52663c4136c9c48aa1594ceb53aaea4e15dc65b52b046a72ba%2FScreenshot_2024-04-09_at_6.07.36_PM.png"" alt=""Screenshot_2024-04-09_at_6.07.36_PM.pngNaN"" /></p>","<md>**Proposition 5.5.2** directly tells us that $$[T_A]_\varepsilon = A$$. In the other hand, since $$\mathcal B$$  is the ordered basis consisting of the columns of $$P$$, you can easily show that the change of basis $$_\varepsilon [I] _\mathcal{B}$$ is the matrix $$P$$. From here, you can use the **Hypothesis** and the **Proposition 9.1.7** to solve the rest of the question</md>"
"Explanation: <p></p>
<p></p>
<p>Could someone explain this result? Is it true for all k that det(A)^k = det(A^k). Why?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8dqa8gc4mm%2F6466f883f32b984169e799bff43f9a6f4f03ffa80965d493fd4084f5b1816767%2Fimage.png"" alt=""image.png"" /></p>","<md>Yes, you can use the multiplicative property of the determinant and induct on $$k$$ to show that $$\det(A)^k = \det(A^k)$$</md>"
"Chapter 9 - Eigenvector Basis Criterion for Diagonalizability: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F0b13a4d9a2923f051c24cbc2fda26059cf00a77cac8a5164f1982784c8feb4c2%2FScreenshot_2024-04-09_at_6.41.35_PM.png"" alt=""Screenshot_2024-04-09_at_6.41.35_PM.pngNaN"" /></p>
<p>Could someone explain how the last line comes about, how does T(vi) = Lambdai(vi) imply that [T]b = diag(lambda1, ... , lambdan)?</p>","<md><img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwerfxazc1nt%2Fnrwcvvkhldnp%2FIMG_8131_2024_04_09.jpeg"" /></md>
<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwerfxazc1nt%2Fnrwcvvkhldnp%2FIMG_8131_2024_04_09.jpeg"" /><p></p>"
subspace vs linear transformation: Do we use the same methods to prove each of these?<br />They seem pretty similar. What are the differences in proving something is a linear transformation vs a subspace?,"<md>A linear transformation is a function, whereas a subspace is a set. They ways for proving them are pretty similar, but they are fundamentally different objects.</md>"
"Dimension of Eigenspace and Nullity: <p>I&#39;m having trouble connecting everything together, could anyone help explain why the dimension of eigenspaces/geometric multiplicity is the nullity of the matrix $$(A-\lambda I)$$.</p>
<p></p>
<p>Thanks</p>","Since nullity is how many free variables are in the null space, you can think of nullity as the dimension of the nullspace. The eigenspace is the nullspace of $$(A-\lambda I)$$, so the geometric multiplicity is therefore the nullity of $$(A-\lambda I)$$"
"Quiz 5 Q1e: <p>Can someone please explain the definition of change-of-basis matrix step?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwaij6bmq2cc%2Fa4a75f5d3570caeb8edbeed971195a2cced3c0addebd15961518509f8ea8c174%2FScreenshot_2024-04-09_at_9.42.14_PM.png"" alt=""Screenshot_2024-04-09_at_9.42.14_PM.pngNaN"" width=""447"" height=""145"" /></p>","<p>$$_\mathcal{B}[P]_\mathcal{E}$$ represents changing a vector in terms of $$\mathcal{E}$$ into a vector in terms of the vectors of $$\mathcal{B}$$. In this case, $$\vec{u}_1,\dots,\vec{u}_n$$ <em>is<strong> </strong></em>$$\mathcal{B}$$, so the representation would be $$I_n$$.</p>
<p></p>
<p>For instance, to rewrite $$\vec{u}_3$$, it would clearly just become $$\vec{e}_3$$ since we are taking the third vector from $$\mathcal{B}$$</p>
<p>$$_\mathcal{B}[P]_\mathcal{E}$$ represents a vector in terms of $$\mathcal{E}$$ in terms of the vectors of $$\mathcal{B}$$. In this case, $$\vec{u}_1,\dots,\vec{u}_n$$ <em>is<strong> </strong></em>$$\mathcal{B}$$, so the representation would be $$I_n$$.</p>
<p></p>
<p>For instance, to rewrite $$\vec{u}_3$$, it would clearly just become $$\vec{e}_3$$ since we are taking the third vector from $$\mathcal{B}$$</p>"
"WA5 Q1: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl6at4vly2vo7m4%2F8e1651f62ea2abdb7b2ace5f0f34974bccf8e3eb78d340525f4eb656fbcd6c32%2FScreenshot_2024-04-09_at_10.11.29_PM.png"" alt=""Screenshot_2024-04-09_at_10.11.29_PM.pngNaN"" /></p>
<p>Why $$C_{n-1}$$ is free not $$C_{n}$$? And Why $$C_{i}$$ can be represented by $$C_{2}$$</p>","<p>&gt;&gt;Why $$C_{n-1}$$ is free not $$C_n$$? And Why $$C_i$$ can be represented by $$C_2$$.</p>
<p></p>
<p>The answer is correct, but not straightforward... </p>
<p></p>
<p>Following the standard process, $$c_n$$ is the free parameter and other $$c_i$$ can be directly determined by $$c_n$$ using the RREF. (Then, you realize this is the same as the solution.) Just stick with the standard method, if it makes you comfortable.</p>
<p></p>
<p></p>
<p>&gt;&gt;Why $$C_{n-1}$$ is free not $$C_n$$? And Why $$C_i$$ can be represented by $$C_2$$.</p>
<p></p>
<p>The answer is correct, but not straightforward... </p>
<p></p>
<p>Following the standard process, $$c_n$$ is the free parameter and other $$c_i$$ can be directly determined by $$c_n$$ using the RREF. (Then, you realized this is the same as the solution.) Just stick with the standard method, if it makes you comfortable.</p>
<p></p>
<p></p>"
"Proof of Exercise in Ch. 9.1: I created a proof but I&#39;m unsure about some steps. I labelled them 1, 2, 3 for reference. <br /><br />So In step 2, it&#39;s pretty obvious by defn that the matrix to the right of A is P, and that is by definition of change of bases matrix. <br />And I know I can prove that P^-1 shows up to the left of A just by inverting the change of matrix. <br /><br />However, in step 2, I wrote out the change of matrix B[I]E in a different form [[e1]B ... [en]B]. To be honest, by looking at this matrix, I don&#39;t immediately see how it corresponds to P^-1. The standard basis vectors with B-components corresponding to P^-1 is a bit hard to think about.<br /><br />On an exam, I would probably ignore writing it out and just invert  B[I]E to be (E[I]B)^-1 = P^-1 which I knew how to solve. But just as a discussion, how would I know [[e1]B ... [en]B] = P^-1 with the given information?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F62362a03733ca193b76faad20b9d05d74cf1cdfa1a385b414d9f38aa1aa5215c%2FIMG_853802E248F4-1.jpeg"" alt=""IMG_853802E248F4-1.jpegNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2F183d0cb2d6d942c6d08e2c32c07025d3857758e8934c7612fb3ecef3b2748e76%2FScreenshot_2024-04-09_at_10.18.05_PM.png"" alt=""Screenshot_2024-04-09_at_10.18.05_PM.pngNaN"" />","<p>This is just what the change of basis matrix is. $$P$$ is the matrix changing from $$\mathcal{B}$$ to $$\mathcal{E}$$, so $$P^{-1}$$ is changing back from $$\mathcal{E}$$ to $$\mathcal{B}$$.</p>
<p></p>
<p>In the matrix you wrote out, you can see that it makes sense that this is changing from $$\mathcal{E}$$ to $$\mathcal{B}$$ if you think about what happens when you multiply some vector $$[\vec{v}]_\mathcal{E}$$ by it. Each of the entries of the vector, $$v_1,\dots,v_n$$ will be &#34;transformed&#34; into their respective representations in terms of the vectors of $$\mathcal{B}$$.</p>"
"standard matrix of composite transformation: <p>Is this the only transformation in chapter 5 that is a matrix matrix(instead of a vector) transformation?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb17ooi55le%2Fab02d2f8afd65fba9fa31e514395fcf7d534fd63cc1db2606796eb22e08c0ea6%2Fimage.png"" alt=""image.pngNaN"" /></p>","<md>This is still a transformation of vectors to vectors:

$$T_2 \circ T_1 : \mathbb F^n \to \mathbb F^p$$.</md>"
"Stuck on exercise 9.2: could someone give me pseudo instructions for the pwoof of this exercise<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwclm3shhvy%2Fbef66071eb21abd1b56b7caa40d68719e2f6baafcb570066fae2e1963e494f62%2FScreenshot_2024-04-09_at_11.26.09_PM.png"" alt=""Screenshot_2024-04-09_at_11.26.09_PM.pngNaN"" />","<md>Think about what the eigenvectors should be.

As a hint, what is $$\text{proj}_{\vec w}(\vec w)$$, and if $$\vec v$$ is a nonzero vector which satisfies $$\vec v \cdot \vec w = 0$$, then what is $$\text{proj}_{\vec w}(\vec v)$$?

To determine that these are eigenvectors, you should use

$$\text{proj}_{\vec w}(\vec u) = \dfrac{\vec u \cdot \vec w}{\|\vec w\|^2} \vec w$$</md>
<md>Think about what the eigenvectors should be.

As a hint, what is $$\text{proj}_{\vec w}(\vec w)$$, and if $$\vec v$$ is a nonzero vector which satisfies $$\vec v \cdot \vec w = 0$$, then what is $$\text{proj}_{\vec w}(\vec v)$$?

To determine that these are eigenvectors, you should use

$$\text{proj}_{\vec w}(\vec u) = \dfrac{\vec u \cdot \vec w}{\|\vec w\|^2} \vec u$$</md>
Using the hint, it is easier to solve the equation $$[proj_\vec{w}]_\mathcal{B} [\vec{x}]_\mathcal{B} = \lambda[\vec{x}]_\mathcal{B}$$, to find the eigenpairs in terms of the basis $$\mathcal{B}$$"
"diagonalizable for non-squared matrix?: <p>Can a non-squared matrix be diagonalizable? Or we only talks about diagonalizability for squared matrix?</p>
<p>Thanks</p>","<p>&gt;&gt;Or we only talks about diagonalizability for squared matrix?</p>
<p>Yes.</p>
<p></p>
<p>&gt;&gt;Can a non-squared matrix be diagonalizable?</p>
<p>For non-squared matrices, we have other &#34;standard&#34; forms. For instance, one can do <a href=""https://en.wikipedia.org/wiki/Singular_value_decomposition"" target=""_blank"" rel=""noopener noreferrer""><em>singluar value decomposition</em></a>. </p>"
"Diagonalizability test proof course note 239: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2Fd35bfa4426ff3053e557c91e1271c9604022afe6f85af1490472a51b9182543c%2FScreenshot_2024-04-10_at_12.33.56_AM.png"" alt=""Screenshot_2024-04-10_at_12.33.56_AM.pngNaN"" />Could someone explain why $$[T(\vec{v_i})]_B = [d_i\vec{v_i}]_B$$ iff $$T(\vec{v_i}) =d_i\vec{v_i}$$? Thank you",It is true in general that $$[x]_B=[y]_B$$ if and only if $$x=y$$. Can you see why? Try using the definition of a coordinate vector. 
"Rank: <p>If AB = C and C is full rank, does it also mean A is also full rank?</p>
<p>If so, how would you prove it?</p>","Are you talking about square matrices? If so, yes. Using the determinant, $$AB$$ is invertible if and only if both $$A$$ and $$B$$ are invertible. "
"Special linear transformation: Do we need to memorize the standard matrix for all the special linear transformation including projection, perpendicular, rotation and reflection?",@1203
"[Q5 regrade]: Hi, I submitted a regrade request yesterday for Quiz 5, wondering when is the deadline for requesting a re-mark.","If you already submitted a regrade request why would you care about when the deadline is?
The policy is 6 days after the assessment was returned. "
"PP 11, Q2: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8dqa8gc4mm%2F5244f1aa7159b419770f7f26737d4d1f37fcfbb3a3f6f39da9a04048c92c9d72%2Fimage.png"" alt=""image.png"" /></p>
<p>For questions like these, I was wondering if row reducing A first(only by adding scalar multiples) and then taking the det(A-$$\lambda$$I) was the same as doing the direct det(A-$$\lambda$$I) </p>
<p>My intuition tells me it doesn&#39;t since det(A&#43;B) != det(A) &#43; det(B). I was simply wondering because the way I have been doing them, I find the computation and algebra to be very long and so I was wondering if there was a more efficient way of doing these.</p>
<p></p>
<p>Thanks!</p>","No I don&#39;t think you can do this. I think however, you could first do $$A-\lambda I$$, and then perform row operations on that, but that may not even be worthwhile"
"Valid Proof?: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc62io9y5uf%2F7962c599c8fc392fce573623ea08837215503a7fddd7e6f11ea8244ecee80df0%2FScreenshot_2024-04-10_at_12.37.49_PM.png"" alt=""Screenshot_2024-04-10_at_12.37.49_PM.png"" />","I did something similar - if the given statement is true for every basis it is also true for the standard basis which makes the latter half of the statement suggest that the columns of A are linearly independent. By LI this implies that we have n pivot columns so Rank(A) = n which implies that A is invertible.
This looks good to me. "
"Understanding inverse of 2 X 2 Matrix PP 5 Q6: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwemr3flutc%2F9b4964c21e39f333f8aa64d53f719b0359d76d18d64bbbfc3b4bf21b98c4b878%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>Doing it on my own, I used the proposition for finding the inverse of a 2 x 2 matrix and got this answer: $$\frac{1}{cos^2\theta + sin^2\theta}\begin{bmatrix} cos \theta & sin \theta \\ -sin \theta& cos \theta \end{bmatrix}$$ which is similar to the solution but without the stuff outside the matrix. I am also confused ad the why we let theta equal zero</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwemr3flutc%2Ffd9cd349443371a9c07f2d87231aa0296a5c67ac5918563ce84e204b5c0eba96%2Fimage.png"" alt=""image.pngNaN"" /></p>","<md>You should remember some trig identities:

$$\cos(\theta) = \cos(-\theta)$$ because cosine is even

$$\sin(-\theta) = -\sin(\theta)$$ because sine is odd

$$\cos^2(\theta) + \sin^2(\theta) = 1$$ (pythagorean identity).</md>
cos^2(x) &#43; sin^2(x) = 1. So the 1/det(A) = 1."
"WP10 - Q4: Can I just use linearity of matrix-vector multiplication for this question? Thanks! <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2F4e04471f62fbc42421485064fa827b2fbc1d2151a078099e2ed3f2b5b1c1617c%2Fimage.png"" alt=""image.pngNaN"" />","Yea I think that&#39;s right, though I think you made a typo in the second line, and forgot the $$c$$
Yea I think that&#39;s right, though I think you made a typo in the next line, and forgot the $$c$$
Yea I think thats right"
"Missing ERO&#39;s in between: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwemr3flutc%2Fc1ad85e892b943240e4e6890f0767096aa741a1eb81bb3d629062c1f06ae4665%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>I was trying to see which ero&#39;s were used to get to this point, I used the following then get stuck $$R_1\leftrightarrow R_3, R_2 \rightarrow R_2 - R_1, R_3 \rightarrow R_3 - aR_1$$</p>
<p></p>",Then they did $$R_3 \rightarrow R_3+R_2$$ followed by $$R_3 \rightarrow -R_3$$.  (Then they factored.)
Content before midterm: What is percentage of the final which is based on the content before midterm? 30%?,That number wasn&#39;t made public.  It&#39;s difficult calculate because we keep using the material from before the midterm throughout the whole course. 
Solutions to Practice Final: When will solutions to the practice final be posted?,They&#39;ve been posted on Learn as of 9 am today.
"Diagonalizability test: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2Fd91623c9a820fce6e9ef8508fa506e94f18ba60798e565fa2c3f84bdc7ccacc7%2F9.4_Diagonalizability_Test.jpeg"" alt=""9.4_Diagonalizability_Test.jpegNaN"" />What does &#39;completely factors&#39; means?</p>
<p>Thanks  </p>",It means that you can factor it all the way to linear factors with a possible constant out front. 
Practice Final: I&#39;m just wondering if the difficulty of the final is similar to that of the posted Practice final. Can we assume to have similar type proof questions?,"<md>I would say the final is similar in difficulty and style to the practice final.

Of course, the specific content that is asked may be different still.</md>"
"PP 11 recommended problem Q2: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2Fb3d2ebcc11846f9502d7625085565257f875b552aa6a8d7b7553caceec473d3c%2F19381712771880_.pic.jpg"" alt=""19381712771880_.pic.jpgNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F60a6b256e3d979161f0a05ad25272250bb0e5608cd5a065205ce74e25db9de9e%2FScreenshot_2024-04-10_at_14.00.17.png"" alt=""Screenshot_2024-04-10_at_14.00.17.pngNaN"" />Can someone explain why is the algebraic multiply of lambda equals to 2?","<p>Algebraic multiplicities are the powers of the linear factors in the factorization of the characteristic polynomial.</p>
<p></p>
<p>For example, if one has $$C_\lambda(A)= (\lambda-1)^2(\lambda+5)^3 (\lambda - 4)$$, then the algebraic multiplicity of $$\lambda=1$$ is $$2$$; algebraic multiplicity of $$\lambda=-5$$ is $$3$$, etc..</p>
<p></p>
<p>Speaking of your question, can you work out the characteristic polynomial and factor it into linear pieces? Then you will see the algebraic multiplicity. </p>
<p>Algebraic multiplicities are the powers of the linear factors in the factorization of the characteristic polynomial.</p>
<p></p>
<p>For example, if one has $$C_\lambda(A)= (\lambda-1)^2(\lambda+5)^3 (\lambda - 4)$$, then the algebraic multiplicity of $$\lambda=1$$ is $$2$$; algebraic multiplicity of $$\lambda=-5$$ is $$3$$, etc..</p>
<p></p>
<p>Speaking of your question, can you work out the characteristic polynomial and factor it into linear pieces? Then you will see algebraic multiplicity. </p>"
"Why is this false?: <p>The solution says there can only be either 1 or 3 real roots, is this under the assumption that there are 3 eigenvalues? I remember seeing matrices in 3x3 with 2 real roots. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fec62d76312933ab12d1e6e469bbe4cb778d16177e93fed207a490c34c8996dbb%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2F161afe49a2b36abe4da068b4e05ffd0cdbb1af8555e82403d369b739f9c20c98%2Fimage.png"" alt=""image.pngNaN"" /></p>","That’s impossible. Since a complex root must come in pairs, it must be either 1 or 3. 1 of them could be repeated though.
That’s impossible. Since a complex root must come in pairs, it must be either 1 or 3
This question is not worded appropriately. The matrix $$\text{diag}(2,2,4)$$ has exactly 2 real eigenvalues. "
"WP10 warmup q10: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2F0e8e9022dc7730d934f1713269dd684c566f6ef896a43d09024ced3be7cba2a8%2FScreenshot_2024-04-10_at_2.48.27_PM.png"" width=""1330"" height=""290"" alt="""" /> <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2Fdfe31ab96ec62b292046ab45863f403e4ad9d49dc63e13e5be99fe180c23ac4a%2FScreenshot_2024-04-10_at_2.51.22_PM.png"" alt=""Screenshot_2024-04-10_at_2.51.22_PM.pngNaN"" />Im confused as to why this approach works to find these matrices. Would appreciate a breakdown.","<p>Given $$\mathcal{B}=\{\vec{v_1},\vec{v_2}\}$$, the matrix formed by basis vectors $$[\vec{v_1}\ \ \vec{v_2}]$$ is exactly $$_{\mathcal{E}}[I]_{\mathcal{B}}$$. (Why? check course notes or lecture notes)</p>
<p></p>
<p>Same holds for other ordered basis $$\mathcal{C}=\{\vec{w_1}, \vec{w_2}\}$$.</p>
<p></p>
<p>The row-reduction algorithm is turning the LHS into an identity matrix and meanwhile (left)multiplying the inverse of LHS to the RHS. </p>
<p></p>
<p>Therefore by a theorem, $$[\vec{w_1} \ \ \vec{w_2}]^{-1} [\vec{v_1}\ \ \vec{v_2}]$$ $$=\ _{\mathcal{E}}[I]_{\mathcal{C}}^{-1}$$ $$_{\mathcal{E}}[I]_{\mathcal{B}} =\  _{\mathcal{C}}[I]_{\mathcal{B}}$$.</p>
<p></p>"
"Real or Complex field: If a question does not specify if it is in a real or complex field and it makes a difference which field it&#39;s in, what do we assume?","if it makes a difference, you should state what field you&#39;re working in to use certain theorems. (i.e. Complex numbers if you want to use n distinct eigenvalues =&gt; diagonalizable). <br /><br />But I would assume that if it matters, the question should state AEMnxn(F) or T: R^2 -&gt; R^2 or something like that to let you know."
"9.1 expression: <p>Are they the same and why</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flm71gjhlsad1v9%2F8537df2a099c6e51ad26cfb6a961443baa8b4845fbd4faa78c5d35ba7dd75f65%2FIMG_23B69623F898-1.jpeg"" alt=""IMG_23B69623F898-1.jpeg"" /></p>",See @1139
is it true that 0 vector is a member of every spanning set: ,"Yes<div><br /></div>
The zero vector is not a member of every spanning set.  However, the zero vector is always in the span of any set of vectors.  (You just choose the linear combination with all scalars equal to 0.)"
"quiz 5: Hi, in this question why W is equal to A^T?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fpkoarihpfrpy%2Fb5ebb051f63ffa5ad23d99e206e4d5df_2024_04_10.jpeg"" /><p></p>",See @1140
Practise final Q11: What does this sign mean in context (<b>⟂)</b>,I think perpendicular
"[Q7][Sample Midterm] Is this a valid solution?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcbcphb86sf%2F288964fdec7466f3495087e79a701430868dda4cfe3fd6a3a966cde4a185862d%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p>A is invertible where $$A^{-1} = A$$<br />Follows from Invertibilty criteria that $$Null(A) = \{ \vec{0}\}$$ and $$Col(A) = \mathbb{F^N} $$ which implies that $$\vec{0}\in Col(A)$$<br /><br />is this enough?</p>","<p>I would say that is sufficient for the ($$\subseteq$$) direction.</p>
<p></p>
<p>I agree with you that the ($$\supseteq$$) is kind of implied by this proof, however I would include it just to be safe (and it&#39;s really straightforward)</p>
<p></p>
<p>So just add one line like &#34;since Col(A) and Null(A) are spans, $$\vec 0 \in Col(A) \cap Null(A) \Rightarrow \{\vec 0\}\subseteq Col(A) \cap Null(A)$$ &#34;.</p>
<p></p>
<p>Then it&#39;s a watertight proof! </p>"
"practice exam T/F d): <p>Why is the answer to this false?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2F074f8a71d32923ebfeabdc615048a81420d1828c5c673b30a744e150d052f9d2%2FScreen_Shot_2024-04-10_at_4.06.36_PM.png"" alt=""Screen_Shot_2024-04-10_at_4.06.36_PM.pngNaN"" /></p>","There should be a justification a page down.
There should be a justification a page down. "
"linearly dependence question: <p>does a set of linearly independent vectors have to equal to dimension they belong to? </p>
<p></p>
<p>for example if you have a set of vectors in R^n S that are linearly independent does this imply that the span(S) = R^n?</p>","<p>I&#39;m not entirely certain what your question is asking. A set of linear independent vectors is a set, dimension is the number of vectors in the basis of a subspace. </p>
<p></p>
<p>If you have a set of vectors in $$\mathbb{R}^n$$ that are linearly independent, Span(S) $$\subseteq \mathbb{R}^n$$, and Span(S) = $$\mathbb{R}^n$$ iff dim(Span(S)) = n.</p>
<p></p>
<p>Does that answer your question? Feel free to elaborate further if I misinterpreted what you are asking</p>
<p>I&#39;m not entirely certain what your question is asking. A set of linear independent vectors is a set, dimension is the number of vectors in the basis of a set. </p>
<p></p>
<p>If you have a set of vectors in $$\mathbb{R}^n$$ that are linearly independent, Span(S) $$\subseteq \mathbb{R}^n$$, and Span(S) = $$\mathbb{R}^n$$ iff dim(Span(S)) = n.</p>
<p></p>
<p>Does that answer your question? Feel free to elaborate further if I misinterpreted what you are asking</p>
<p>I&#39;m not entirely certain what your question is asking. A set of linear independent vectors is a set, dimension is the number of vectors in the basis of a set. </p>
<p></p>
<p>If you have a set of vectors in $$\mathbb{R}^n$$ that are linearly independent, Span(S) $$\subseteq \mathbb{R}^n$$, and Span(S) = $$\mathbb{R}^n$$ iff dim(S) = n.</p>
<p></p>
<p>Does that answer your question? Feel free to elaborate further if I misinterpreted what you are asking</p>
If a set of vectors is linearly independent, it doesn&#39;t have to be a spanning set for the vector space it is from.  For, example if I choose one non-zero vector from $$\mathbb{R}^2$$ it will be a linearly independent set, but it will not be a spanning set for $$\mathbb{R}^2$$."
"Justification vs. Computation / Statement: <p>I am taking up my answers to the practice final, and I noticed that for Q5 b) I simply stated the answer and did not justify. </p>
<p></p>
<p>I recognize that I probably should have elaborated a little on my answer because of the 3 mark allocation, but I was wondering if most computational / &#34;determine the following...&#34; type questions are mostly going to be graded for correctness or is process also important?</p>","Notice that it says this in the instructions &#34;Unless the question states otherwise, you must justify all of your answers. Your<br />arguments must be logical, clear and easy to understand.&#34;"
"Practice final Q5b: <p>Can someone explain why dim(Span(S)) = rank(A) in all cases?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2Fc27af5bf3ebf10829165d142e940f2c4eeeda94002886263ff40969416e8ec2c%2FScreenshot_2024-04-10_at_16.49.22.png"" alt=""Screenshot_2024-04-10_at_16.49.22.pngNaN"" /></p>","The rank(A) is the number of pivots, and by the LI and pivots theorem, every pivot column corresponds to a basis vector. So essentially there are rank(A) many basis vectors, implying that the dim(Span(S)) = rank(A)"
"Practice Final Q4 d: <p>For this question, I&#39;m confused how we just multiplied the area of the paralellogram made with [T_2]_E by 3 to get the answer</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2Fb21229e16c63bea26798e8cc7e6680f8bb4564c5d8172b70f5e81ddcdbfa9479%2FScreen_Shot_2024-04-10_at_4.53.00_PM.png"" alt=""Screen_Shot_2024-04-10_at_4.53.00_PM.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwam6q3i52z9%2Fabe1f7dc88f1d8d72d6f47c17f7de47408027a03f63e6cc20fbe85c47fae773a%2FScreen_Shot_2024-04-10_at_4.53.21_PM.png"" alt=""Screen_Shot_2024-04-10_at_4.53.21_PM.png"" /></p>
<p></p>
<p></p>","<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F8f170b76f5c2b0ad94e20ec3a8e2ca5fed14ea1e94b35a1c4b2f3debfbcddb8f%2FScreenshot_2024-04-10_at_16.55.58.png"" alt=""Screenshot_2024-04-10_at_16.55.58.pngNaN"" />"
Final Exam Practice Q3: Can someone who did this question properly show me their steps for finding the characteristic polynomial and the eigenvalues. I&#39;ve been looking at my solution for a while and I can&#39;t figure out where I went wrong. Thanks.,"<p>I expand along the first column.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa7mpvvkep%2F098d71751006ea5c97a74851895afa1d71881e192d7fec73e8dd2d02f0e33b86%2F19421712782771_.pic.jpg"" alt=""19421712782771_.pic.jpgNaN"" /></p>"
"Are there more solutions to this question?: <p>When I did the question I thought just the standard basis would diagonalize the matrix, is that not right?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8zcl4pgki%2Fecf0d974b4ca78ed523aace721796334d8500044e0382a3fa9afd910b39dab52%2Fimage.png"" alt=""image.pngNaN"" /></p>",How did you get that the standard basis would diagonalize the matrix?
Complex numbers on final: I was wondering how much we should focus on calculating the determinants and eigenspaces with complex numbers for practice? The ones on the practice problems are very time consuming to do. ,"According to other similar questions, you won&#39;t be asked to do very large computations with complex numbers, but you are expected to <strong>know</strong> how to do them. I would guess that the final won&#39;t be as bad as some of the ones in the practice problems"
"Confusion about the B-matrix of T: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8j0ruc25io%2Fe34ba38b2bf2b423f843d0dce7f7a58ab82998713414cb08f32840b32487fd05%2FScreenshot_2024-04-10_at_5.27.28_PM.png"" alt=""Screenshot_2024-04-10_at_5.27.28_PM.pngNaN"" width=""712"" height=""77"" /></p>
<p>The backwards direction of the proof for Eigenvector Basis Criterion for Diagonalizability seems to suggest that the diagonal entries of $$[T]_{\beta }$$ are the eigenvalues of T. Is this the case, or am I reading it wrong? Because if this were true, then the B-matrix of T would be equal to $$D = P^{-1}[T]_{\beta }P$$ who&#39;s diagonal entries are the eigenvectors of T. I know that these two matrices are similar, but I&#39;m not quite sure if they&#39;re supposed to be equal...</p>","<p>I think it should be $$P^{-1}[T]_\mathcal{E}P=D$$. $$[T]_\mathcal{B}$$ is indeed the diagonal matrix with the eigenvalues of $$T$$.</p>
<p></p>
<p>Consider $$[T]_\mathcal{B}\vec{e}_i$$. This is extracting the $$i$$th column of $$[T]_\mathcal{B}$$, which we would expect to be $$\lambda_i \vec{e}_i$$.</p>
<p></p>
<p>Indeed, $$[T]_\mathcal{B}\vec{e}_i = ~_\mathcal{B}[I]_\mathcal{E}~[T]_\mathcal{E}~_\mathcal{E}[I]_\mathcal{B}~\vec{e}_i=~_\mathcal{B}[I]_\mathcal{E}~[T]_\mathcal{E}~\vec{v}_i=~_\mathcal{B}[I]_\mathcal{E}~\lambda_i\vec{v}_i=\lambda_i\vec{e}_i$$</p>
<p>I think it should be $$P^{-1}[T]_\mathcal{E}P=D$$. $$[T]_\mathcal{B}$$ is indeed the diagonal matrix with the eigenvalues of $$T$$.</p>
<p></p>
<p>Consider $$[T]_\mathcal{B}\vec{e}_i$$. This is extracting the $$i$$th column of $$[T]_\mathcal{B}$$, which we would expect to be $$\lambda_i \vec{e}_i$$.</p>
<p></p>
<p>Indeed, $$[T]_\mathcal{B}\vec{e}_i = ~_\mathcal{E}[I]_\mathcal{B}~[T]_\mathcal{E}~_\mathcal{E}[I]_\mathcal{B}~\vec{e}_i=~_\mathcal{E}[I]_\mathcal{B}~[T]_\mathcal{E}~\vec{v}_i=~_\mathcal{E}[I]_\mathcal{B}~\lambda_i\vec{v}_i=\lambda_i\vec{e}_i$$</p>
<p>I think it should be $$P^{-1}[T]_\mathcal{E}P=D$$. $$[T]_\mathcal{B}$$ is the diagonal matrix with the eigen values of $$T$$.</p>
<p></p>
<p>Consider $$[T]_\mathcal{B}\vec{e}_i$$. This is extracting the $$i$$th column of $$[T]_\mathcal{B}$$, which we would expect to be $$\lambda_i \vec{e}_i$$.</p>
<p></p>
<p>Indeed, $$[T]_\mathcal{B}\vec{e}_i = ~_\mathcal{E}[I]_\mathcal{B}~[T]_\mathcal{E}~_\mathcal{E}[I]_\mathcal{B}~\vec{e}_i=~_\mathcal{E}[I]_\mathcal{B}~[T]_\mathcal{E}~\vec{v}_i=~_\mathcal{E}[I]_\mathcal{B}~\lambda_i\vec{v}_i=\lambda_i\vec{e}_i$$</p>
I think it should be $$P^{-1}[T]_\mathcal{E}P=D$$
I think it should be $$P^{-1}TP=D$$"
"Is this right?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdl9os1tne%2F8c2ed06b3dd30aa4678699c5ff9c75717fcf9bfcce7a07f3f0bb5119aa1ba93a%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdl9os1tne%2F20796eba0fd89cf3a1ffd04752233dfebc29239ec971a4d1fe4eefb32f86d652%2Fimage.png"" alt=""image.pngNaN"" />Can the span of { [1 0], [0 8] } be a correct answer? these are also LI?<br />Or is there something else I am looking for?</p>","I&#39;m pretty sure that Span{[1 8]} <b>⊆ </b>{[1 0], [0 8]} but Span {[1 0], [0 8]} <b>⊄ </b>Span {[1 8]} therefore the two spans are not equal. This is because there are linear combinations of [1 0], [0 8] which can&#39;t be obtained with just [1 8]. Another way to think about it is that a Basis for a line will always be of dimension 1, therefore the basis will always consist of a just a single vector.
I&#39;m pretty sure that Span{[1 8]} <b>⊆ </b>{[1 0], [0 8]} but Span {[1 0], [0 8]} <b>⊄ </b>Span {[1 8]} therefore the two spans are not equal. This is because there are linear combinations of [1 0], [0 8] which can&#39;t be obtained with just [1 8]. Another way to think about it is that the Basis for a line will always be of dimension 1, therefore a basis will always consist of a just a single vector.
I&#39;m pretty sure that Span{[1 8]} <b>⊆ </b>{[1 0], [0 8]} but Span {[1 0], [0 8]} <b>⊄ </b>Span {[1 8]} therefore the two spans are not equal. This is because there are linear combinations of [1 0], [0 8] which can&#39;t be obtained with just [1 8]. Another way to think about it is that the Basis for a line will always be of dimension 1, therefore a basis for a line will always consist of a just a single vector.
I&#39;m pretty sure that Span{[1 8]} <b>⊆ </b>{[1 0], [0 8]} but Span { 1 0], [0 8]} <b>⊄ </b>Span {[1 8]} therefore the two spans are not equal. This is because there are linear combinations of [1 0], [0 8] which can&#39;t be obtained with just [1 8]. Another way to think about it is that the Basis for a line will always be of dimension 1, therefore a basis for a line will always consist of a just a single vector.
I&#39;m pretty sure that Span {[1 8]} <b>⊆ </b>{ [1 0], [0 8] } but Span { [1 0], [0 8] } <b>⊄ </b>{[1 8]} therefore the two spans are not equal. This is because there are linear combinations of [1 0], [0 8] which can&#39;t be obtained with just [1 8]. Another way to think about it is that the Basis for a line will always be of dimension 1, therefore a basis for a line will always consist of a just a single vector.
I&#39;m pretty sure that Span {[1 8]} <b>⊆ </b>{ [1 0], [0 8] } but Span { [1 0], [0 8] } <b>⊄ </b>{[1 8]} therefore the two spans are not equal. This is because there are linear combinations of [1 0], [0 8] which can&#39;t be obtained with just [1 8]. Another way to think about it is that the Basis for a line will always be of dimension 1, therefore a basis for a line will never have more than 1 vector."
"Inverse: Can we say that the A is not invertible if n ≠ m? (Since we saw left inverse and right inverse in the assignment, but not defined in the textbook or lecture)","<p>I am assuming you mean if A is an n x m matrix, then if n $$\neq$$ m, then A is not invertible. Yes, because in the invertibility criteria,  it says A is an n x n matrix</p>
<p></p>
I am assuming you mean if A is an n x m matrix, then if n $$\neq$$ m, then A is not invertible. Yes, because in the invertibility criteria, if you notice the dimension of A, it is n x n.
I am assuming you mean if A is an n x m matrix, then if n $$\neq$$ m, then A is not invertible. Yes, because in the invariability criteria, if you notice the dimension of A, it is n x n. 
For a matrix to be invertible it must be square. "
"WP10: Q5c: <p>I don&#39;t understand how [ ]$$_B$$ is onto</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2Fd91ef07a9c7625f9ea2ceb2f7635406e5daec54cbb81bdc74c6a6aa9d78a92a8%2FScreenshot_2024-04-10_at_6.28.45_PM.png"" alt=""Screenshot_2024-04-10_at_6.28.45_PM.pngNaN"" /></p>
<p></p>
<p>I also don&#39;t understand how to arrive at the proofs. Can I please get some advice about how I should I think about the questions and get to the proofs? I feel especially stumped on the proofs starting from chapter 8. </p>","<p>From part b), we prove that an arbitrary vector $$\vec x$$ is a linear combination of the basis, so it is within the span of the basis, which means it can be mapped with the linear transformation associated to the basis. In other words, we can arrive at any $$\vec x$$ using the linear transformation which is the definition of onto / surjective.</p>
<p></p>
<p>In terms of proof tips:</p>
<p></p>
<p>There&#39;s a lot of different definitions to be familiar with in this course, but my approach has never been to simply practice memorizing them. Try the practice problems and see what pieces are needed to bring everything together. We need to go from point A to point B using the knowledge we&#39;ve gained through the term, so I find trying a variety of problems helps me see that path more clearly. Once you see the given info, try to lay out everything you know about the start and the destination and see if you can recall any theorems we have to get between the two. It can definitely be tricky! </p>
<p></p>
<p>Office hours have also been my go-to-- if there&#39;s something you don&#39;t fully understand, ask! Not sure what office hours are left at this point but Piazza may be a great resource</p>
<p></p>
<p>Bottom line: practice problems and office hours is the best advice I can give, although maybe others have good tips to share</p>
<p>From part b), we prove that an arbitrary vector $$\vec x$$ is a linear combination of the basis, so it is within the span of the basis, which means it can be mapped with the linear transformation associated to the basis. In other words, we can arrive at any $$\vec x$$ using the linear transformation which is the definition of onto / surjective.</p>
<p></p>
<p>In terms of proof tips:</p>
<p></p>
<p>There&#39;s a lot of different definitions to be familiar with in this course, but my approach has never been to simply practice memorizing them. Try the practice problems and see what pieces are needed to bring everything together. We need to go from point A to point B using the knowledge we&#39;ve gained through the term, so I find trying a variety of problems helps me see that path more clearly. Once you see the given info, try to ly out everything you know about the start and the destination and see if you can recall any theorems we have to get between the two. It can definitely be tricky! </p>
<p></p>
<p>Office hours have also been my go-to-- if there&#39;s something you don&#39;t fully understand, ask! Not sure what office hours are left at this point but Piazza may be a great resource</p>
<p></p>
<p>Bottom line: practice problems and office hours is the best advice I can give, although maybe others have good tips to share</p>
<p>From part b), we prove that an arbitrary vector $$\vec x$$ is a linear combination of the basis, so it is within the span of the basis. In other words, any $$\vec x$$ is in the span which is the definition of onto / surjective.</p>
<p></p>
<p>In terms of proof tips:</p>
<p></p>
<p>There&#39;s a lot of different definitions to be familiar with in this course, but my approach has never been to simply practice memorizing them. Try the practice problems and see what pieces are needed to bring everything together. We need to go from point A to point B using the knowledge we&#39;ve gained through the term, so I find trying a variety of problems helps me see that path more clearly. Once you see the given info, try to ly out everything you know about the start and the destination and see if you can recall any theorems we have to get between the two. It can definitely be tricky! </p>
<p></p>
<p>Office hours have also been my go-to-- if there&#39;s something you don&#39;t fully understand, ask! Not sure what office hours are left at this point but Piazza may be a great resource</p>
<p></p>
<p>Bottom line: practice problems and office hours is the best advice I can give, although maybe others have good tips to share</p>"
"WP10 Q7: <p>I don&#39;t know if my proof is valid for q7: Let v ∈ F$$^n$$ be a non-zero vector. Prove that there is an invertible matrix A ∈ M$$_(n \times n)$$(F) that has v as one of its columns. </p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2F144b426accfff87f3edc2815b6333d0147c6eb69b201e43f54eba0053cb9771a%2FIMG_1578.heic"" alt=""IMG_1578.heicNaN"" /></p>","<p>I don&#39;t believe this is right. You assumed $$A$$ was invertible and had $$\vec{v}$$ as one of its columns in the first sentence.</p>
<p>I don&#39;t believe this is right. You assumed $$A$$ was invertible and had $$\vec{v}$$ as one of its columns in the first sentence.</p>
<p></p>
<p>Hint: we know there exists $$v_i\ne 0$$. Make $$\vec{v}$$ the $$i$$-th column of $$A$$ and use the fact upper/lower triangular matrices have $$\det(A)=a_{11}a_{22}\cdots a_{nn}$$.</p>
<p>I don&#39;t believe this is right. You assumed $$A$$ was invertible and had $$\vec{v}$$ as one of its columns in the first sentence.</p>
<p></p>
<p>Hint: we know there exists $$v_i\ne 0$$. Make $$\vec{v}$$ the $i$-th column of $$A$$ and use the fact upper/lower triangular matrices have $$\det(A)=a_{11}a_{22}\cdots a_{nn}$$.</p>
I don&#39;t believe this is right. You assumed $$A$$ was invertible and had $$\vec{v}$$ as one of its columns in the first sentence. Hint: we know there exists $$v_i\ne 0$$. Make $$\vec{v}$$ the $i$-th column of $$A$$ and use the fact upper/lower triangular matrices have $$\det(A)=a_{11}a_{22}\cdots a_{nn}$$.
I don&#39;t believe this is right. You assumed $$A$$ was invertible and had $$\vec{v}$$ as one of its columns in the first sentence. 
I don&#39;t believe this is right. You assumed $$A$$ was invertible and had $$\vec{v}$$ as one of its columns in the first sentence. Hint: Choose the $$n$$-th column of $$A$$ to be $$\vec{v}$$. Can you use the fact that upper/lower triangular matrix is invertible?
I don&#39;t believe this is right. You assumed $$A$$ was invertible and had $$\vec{v}$$ as one of its columns in the first sentence.
In addition to the error pointed out by the student I will also add that just because a vector is not the zero vector does not imply that all of its entries are non-zero. "
"Question about change-of-basis matrix from B to C: In general, two bases for Fn are going to be n vectors of n size, right?<br /><br />Therefore, determining the change-of-basis matrix from B coords to C coords is basically constructing a super-augmented matrix with the vectors [C|B], right (this notation is a bit loose but you get what I mean)? And that is because we are looking to find the C-coordinates of each vector in B.<br /><br />Let me know if I am right on these assumptions",See below. 
"Chap 9: <p style=""text-align:left"">I am kind of struggling to understand the concepts of chapter 9. Does anyone have any tips/videos that I could watch to figure this out?</p>","This video by 3Blue1Brown may be useful <a href=""https://www.youtube.com/watch?v=PFDu9oVAE-g"">https://www.youtube.com/watch?v=PFDu9oVAE-g</a>
I post recordings of my lectures for students to review.  I also have some solutions to practice problems on my YouTube channel which may be helpful. "
"Warm up problem 2 W11 PP: We are told to calculate P and then to get D. If  a different order or eigenpairs are used, so in the case of Q2b, the order of P&#39;s columns are swapped, making D = diag (-1,4). would this still be correct? how do we know which one comes first if that&#39;s not the case.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fllnqx80eduxbu%2Fabe627791d7db5b99064000fe81cfb0c7085c306b360afeabb040d6e638db3ee%2FScreenshot_2024-04-10_at_7.55.48_PM.png"" alt=""Screenshot_2024-04-10_at_7.55.48_PM.png"" />","Yes, your answer would be correct.  If you put the columns of $$P$$ in a different order, you will still get a diagonal matrix, but your diagonal entries will be in a different order. "
Quiz 5 Summary Statistics: What was the mean and standard deviation for quiz 5?,@1146
"PP11 Q7: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdovqgxv1a8%2Fc1b475bd646254fbc7526ecce8a2f9c4852741bbeb4bccbf2de6f98aba2bada6%2Fimage.png"" alt=""image.pngNaN"" width=""264"" height=""24"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdovqgxv1a8%2Fa0c58335856645195cfc5c3c55aa28e29f48f5d14c237b940ca6dcd486ee394f%2Fimage.png"" alt=""image.pngNaN"" width=""267"" height=""183"" /></p>
<p></p>
<p>Above is the PP11 Q7 question and solution. The solution was quite lengthy than I thought. Since I thought that we could show that $$det(A-\lambda I) = det(A^T-\lambda I)$$ then, both have the same characteristic polynomial and hence the same algebraic multiplicity and geometric multiplicity, which proves A is diagonalizable iff A^T is diagonalizable. I understand the solution given, but I still can&#39;t find why my proof doesn&#39;t work(since if my proof works, then I assume the solution might be much shorter, that&#39;s why I thought that my proof was wrong)</p>
<p></p>","Actually I could see how a proof could be done using this approach.   
I&#39;d be curious to see how you proved that $$\det(A- \lambda I) = \det(A^T-\lambda I)$$.  I don&#39;t think it&#39;s true in general.
I&#39;d be curious to see how you proved that $$\det(A- \lambda I) = \det(A^T-\lambda I)$$.  It&#39;s not true in general. 
I&#39;d be curious to see how you proved that $$\det(A- \lambda I) = \det(A^T-\lambda I)$$.  It&#39;s not true. "
"Practice Final Question 2c): <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F839a80bc139c131617f43f60b5c49eaef3359973c611a4e1256a1315b33981da%2Fimage.png"" alt=""image.png"" /></p>
<p>For the basis for Null(A), could I not just use the set of vectors containing the last three columns of A, since those are the columns without pivots?</p>",No.  You need to find a basis for the solution set to the homogenous system. 
"PP5 Recommended Q8: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwarf9r4h3w4%2F70a9ead3493df4ac2acab8a134d89ac7bad506e10226fa09b8e334e8396fd8ee%2FScreenshot_2024-04-10_at_9.10.59_PM.png"" alt=""Screenshot_2024-04-10_at_9.10.59_PM.pngNaN"" width=""719"" height=""366"" /></p>
<p></p>
<p>Can someone explain the highlighted part? I really do not get it. Thank you. :D</p>","<p>We want to find a transformation $$T$$ such that if we apply it twice on some vector $$\vec{x}$$, it has the effect of multiplying $$\vec{x}$$ by $$-1$$. It is just saying that if we can find a matrix $$A$$ such that $$A^2 = -I_2$$, then $$T_A$$ will have this property since applying $$T_A$$ twice is just multiplying by $$A$$ twice, and</p>
<p></p>
<p>$$AA\vec{x} = -I_2\vec{x}=-\vec{x}$$ like we want</p>
<p>We want to find a transformation $$T$$ such that if we apply it twice on some vector $$\vec{x}$$, it has the effect of multiplying $$\vec{x}$$ by $$-1$$. It is just saying that if we can find a matrix $$A$$ such that $$A^2 = -I_2$$, then $$T_A$$ will have this property since applying $$T_A$$ twice is just multiplying by $$A$$ twice, and</p>
<p></p>
<p>$$AA\vec{x} = -I_2\vec{x}$$</p>"
"Is the complement of a subspace in union with the zero vector a subspace?: <p>$$\text{If V is a subspace of }\mathbb{F}^n, \text{ then }\bar{V} \cup\{\vec{0}\} \text{ is a subspace of }\mathbb{F}^n$$</p>
<p></p>
<p>Is this true or false?</p>","What do you think?  Compare your solution to mine:  https://youtu.be/p9ZdlFQELZI?si=_k8j-3ttCzU-mgRq
<p>I would think it is false but i may be wrong.</p>
<p>$$U=\mathbb{F}^n \backslash V$$ is the set of all elements in $$F^n$$ but not in V. Even if we have the zero vector, $$U$$ might not be closed under addition or scalar multiplication since the sum of two vectors in U may lie in V. </p>"
"WP11 Q2B Warm-up: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8j0ruc25io%2Fa49483c76edb56685e165155855a8c5bc56cc48b88aa6e80b32e300f45581059%2FScreenshot_2024-04-10_at_9.59.15_PM.png"" alt=""Screenshot_2024-04-10_at_9.59.15_PM.png"" width=""378"" height=""152"" /></p>
<p>Could someone explain the last step of this question?</p>","<md>The diagonal matrix is the identity:

$$P I P^{-1} = PP^{-1} = I$$</md>"
"PP Q6, can i use this method?: <p>For the recommended questions, number 6.</p>
<p></p>
<p>Since the rank is less than n, it is not equal to n which means it is not invertible. So the determinant is equal to zero. </p>
<p>since the determinant is the product of eigenvalues. One of the eigenvalues is zero.</p>
<p></p>
<p>There was also a theorem that said that if the matrix is invertible, 0 is an eigenvalue.</p>","yeah i think you can.<br />also just a lil typo but if a matrix is <strong>not </strong>invertible, then 0 is an eigenvalue.
yeah i think you can.<br />also just a lil typo but if a matrix is not invertible, then 0 is an eigenvalue."
"w10pp q6: I was just wondering how you get the Null in this sort of situation when there’s only one row?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqw87safwb3ll%2Fbwdvwflbxaao%2FIMG_4083_2024_04_11.jpeg"" /><p></p>","<p>same way you find solution sets for other matrices.<br />if you let $$x_2,x_3$$ be free variables, say $$s,t$$ you&#39;ll get that <br />$$x_1 = \frac{2}{3}s - \frac{4}{3}t$$<br />$$x_2 = s$$</p>
<p>$$x_3 = t$$</p>
<p>and you&#39;ll get the solution set in the picture.</p>
<p>same way you find solution sets for other matrices.<br />if you let $$x_2,x_3$$ be free variables, say $$t,s$$ you&#39;ll get that <br />$$x_1 = \frac{2}{3}s - \frac{4}{3}t$$<br />$$x_2 = s$$</p>
<p>$$x_3 = t$$</p>
<p>and you&#39;ll get the solution set in the picture.</p>
<p>same way you find solution sets for other matrices.<br />if you let $$x_2,x_3$$ be free variables, say $$t,s$$ you&#39;ll get that <br />$$x_1 = \frac{2}{3}t - \frac{4}{3}s$$<br />$$x_2 = s$$</p>
<p>$$x_3 = t$$</p>
<p>and you&#39;ll get the solution set in the picture.</p>
<p>same way you find solution sets for other matrices.<br />if you let $$x_2,x_3$$ be free variables, say $$t,s$$ you&#39;ll get that <br />$$x_1 = \frac{2}{3}t - \frac{4}{3}s$$<br />$$x_2 = t$$</p>
<p>$$x_3 = s$$</p>
<p>and you&#39;ll get the solution set in the picture.</p>
<p>same way you find solution sets for other matrices.<br />if you let $$x_2,x_3$$ be free variables, you&#39;ll get that <br />$$x_1 = \frac{2}{3}t - \frac{4}{3}$$<br />$$x_2 = t$$</p>
<p>$$x_3 = s$$</p>
<p>and you&#39;ll get the solution set in the picture.</p>"
"w11pp Q6: <md>Can someone help explaining why Ax=0x has non-trivial solution implies that A has an eigenvector with eigenvalue 0? 
![Screenshot_2024-04-11_at_12.15.25_AM.png](/redirect/s3?bucket=uploads&prefix=paste%2Flkqwaylnf7g558%2Ffb434234fac1c14500df150c7f9f75b00b1358168762a6d088e2166ecbbbf4fa%2FScreenshot_2024-04-11_at_12.15.25_AM.png)</md>","If $$A\vec{x} = \vec{0}$$ has non-trivial solutions it means $$Null(A) \neq \vec{0}$$ so the matrix is not invertible, so it has an eigenvalue 0."
"Why we need to extend the matrix? week10 wu q7: For this question, why we need to extend the matrix by add extra standard matrix?<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8n9v69s68t%2F616dbb9ebac1be16ef47021110e75145a42f580f1542269b19006c2eb633d1b8%2Fimage.png"" alt=""image.pngNaN"" />","<p>I think it&#39;s because to have a basis for R4, we require 4 vectors. So we add 4 vectors which we know span R4(the standard basis) and then we can row reduce to determine which of the 6 vectors are linearly independant(by looking at which columns have pivots in them), creating our basis for R4. </p>
<p></p>
<p>Side note: I think because we put the two vectors we want at the first two columns of the matrix, that it ensures they are included in the basis since we are considering an ordered basis.</p>
We need it to be a spanning set for $$\mathbb{R}^4$$.  With only two vectors it won&#39;t be a spanning set. "
"Mobius Q: How to do this? How did they arrive on this answer? <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2Fa44ad10fbfd5c63266ab7fa071e6860bf55f215824e32eb3ecedf6288bb6d979%2Fimage.png"" alt=""image.pngNaN"" />","<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcbcphb86sf%2F574fbd0bebc8dd47f8e44d7de1f7cf0bbc30c5ec911dd8eece04b019f5db5ef6%2Fimage.png"" alt=""image.png"" />Pg139 Course Notes"
"practice final: In this question, why we can claim that the provided set is 0?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fxumbfkkeqrpu%2F57723b9c3e9915d313e11459bf6847ba_2024_04_11.jpeg"" /><p></p>","<p>They&#39;re making a claim and proving it later, kinda like a finished product after messing around with the steps in the proof.<br /><br />the solutions for Q3(c) from Quiz 5 made this easier to understand for me : <br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcbcphb86sf%2F824023120892f7448dcbfeb47eb63ed823b0ec288efc3ff5f590a76cd51cd709%2Fimage.png"" alt=""image.pngNaN"" /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcbcphb86sf%2F4b54ac6f6cd40e533e907b9b5fc0c5a36fc23ca68ff011443934d09733508d3c%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>you can use this to extend an argument as to why all entries of x must be 0</p>"
"Mobius Q: <p>You can add multiple of one column to another? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2Fba776fbbba860732453263e591f66d345452ae3fb1dd7ae70def7ae68651e599%2Fimage.png"" alt=""image.pngNaN"" /></p>","<p>We have the standard theorem for the effect of EROs here:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F063ba295fdf09ce5eb39a5e7b8f6815a38c493a1f64cb3f8774a6f86c05aa714%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>But then right underneath that we have this remark:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2Fb8783dd652ff54a159e2e80d2b54103641b99b961ea8ec495be1df229bc9c591%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>So this is more of a theoretical question than an actual application since we shouldn&#39;t be doing column operations</p>"
"Week 5 pp - q8 - part b: <p>I was wondering if the following is a valid solution for part (b) of q8:</p>
<p></p>
<p>Suppose there exists such a linear map. Then $$T \circ T = \textrm{id}_3 \\$$ meaning $$[T \circ T]_\varepsilon = [T]_\varepsilon [T]_\varepsilon = [-\textrm{id}_3]_\varepsilon = -I_3$$ but then we take the determinant of both sides so $$\det([T]_\varepsilon [T]_\varepsilon) = \det([T]_\varepsilon)\det([T]_\varepsilon) = \det([T]_\varepsilon)^2 = \det(-I_3) = - 1$$ but the determinant is real so we have a contradiction.</p>","Yes, that is a good solution (you have a few typos in what you wrote but I get what you meant).  With determinants this becomes a pretty easy thing to consider. "
"Quiz 5: Could someone please further expand on this for some reason despite the solution I&#39;m not bale to understand. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fdb6865f012e7ea647852fb0f50c0da5e2a13e7be53f34884611b19e214a3a62f%2FScreenshot_2024-04-11_at_4.55.07_AM.png"" alt=""Screenshot_2024-04-11_at_4.55.07_AM.pngNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F80d947947d9a1870ef02926c99a34eabbd9c4b5a9bee29d71ca7dafc849c01eb%2FScreenshot_2024-04-11_at_4.54.54_AM.png"" alt=""Screenshot_2024-04-11_at_4.54.54_AM.pngNaN"" />","<p>We have $$m$$ vectors in the spanning set, and we are working in $$\mathbb{F}^n$$. Recall that the dimension of a subspace is the number of vectors in the basis. </p>
<p></p>
<p>We can get a basis from $$V$$ by taking the vectors in the spanning set as columns of a matrix, row reducing to RREF, then taking the original vectors whose columns contain a pivot. This means that the dimension cannot be greater than the number of vectors in the spanning set, since this process is not going to add any vectors, only potentially take them away. So we have that $$\dim(V)\leq m$$</p>
<p></p>
<p>The second part comes from this theorem: </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F475655322c77348d4c2ab859eb04f322d39d6bf43a2325f1e76e31679385487a%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>We can&#39;t have a subspace of $$\mathbb{F}^n$$ that has a greater dimension than $$\mathbb{F}^n$$ itself. </p>
<p></p>
<p>Putting those two facts together, we have that $$\dim(V) \leq \min\{m,n\}$$</p>
<p></p>
<p>Hope that helps!</p>"
"Quiz 5: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fae4f6ed11d677b5a49fdb9863a7779c4234bd40f8ff500a59635231627bea046%2FScreenshot_2024-04-11_at_5.01.14_AM.png"" alt=""Screenshot_2024-04-11_at_5.01.14_AM.pngNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F98d9bc40a08367ae86445f64a7771bff5cebae2635e39f8c2923bc376d839440%2FScreenshot_2024-04-11_at_5.00.02_AM.png"" alt=""Screenshot_2024-04-11_at_5.00.02_AM.pngNaN"" /></p>
<p>why is dim(w) = nullity(A^T)</p>","<p>I was confused about this for a while as well, but note that $$W$$ is defined as the solution set to the homogenous system provided, which is by definition Null(A). W is not a matrix, it is actually a subspace. </p>
<p></p>
<p>Then we use this theorem to equate nullity and dimension:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F99b40cf6c30c7ba9f816bc14b57d83389e178d130f96a0f16f25069d388c472a%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Hope that helps! Feel free to follow up if you have further questions</p>"
"Practice final Q1c: <p>I don&#39;t understand how we can deduce that det(A)= 2:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2F62af5ce0a2102a816199b702ac65d15112c610b4cec99ecfe761970eafa77870%2FScreenshot_2024-04-11_at_10.05.09_AM.png"" alt=""Screenshot_2024-04-11_at_10.05.09_AM.png"" /></p>",Because the matrix is given as being diagonal with 2&#39;s on the diagonal.
"det of lower triangular matrix: We know how to calculate for the upper triangular matrix, would it be the same for the lower triangular matrix?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwayddv4r53q%2F21aecae0a978615b25f6c1f06f5f44d6e0ffabdb584f071bd411147cfbdc9c2a%2FScreenshot_2024-04-11_at_10.11.52_AM.png"" width=""1920"" height=""702"" alt="""" />",Remember that $$det(A)=det(A^T)$$. 
"Practice final Q4c: One of the criteria in the One-to-one test is that rank(A) = n, in an m x n matrix. So for 4c, would it be enough to say that the rank of [T]$$_\varepsilon$$ is 2 so its one-to-one?",Yes
"WP6 Q4 - Correct Proof?: <p>Hi, is this proof correct, or do I have to prove both containments</p>
<p>separately?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa900l7one%2F8e0902e9cb7187589cdba04d65674273a028207d98f2e796f85824f86501644b%2FScreenshot_2024-04-11_at_10.33.56_AM.png"" alt=""Screenshot_2024-04-11_at_10.33.56_AM.pngNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwa900l7one%2Fb014eb539ebfbe87b2a8d1f719ff79a1d0882542c48e4252c9765950bcfc28ec%2Fimage.png"" alt=""image.pngNaN"" /></p>",Every time you say set $$A$$ is equal to set $$B$$ you should argue that the two sets are in fact equal to each other by justifying they are contained in each other.  I personally don&#39;t recommend writing your proof in this way because it hides a lot of this justification. 
"diagnolizability test: For the condition of h(lambda) is a constant polynomial. Can we understand in this way: the whole CA(lambda) equation will have root over F off h(lambda) is a constant polynomial? Thanks.<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fpjdbbpgdoqgv%2F958aa276ecc482aa6dc56cfe85c43682_2024_04_11.jpeg"" /><p></p>","<p>Yes, basically if $$C_A(\lambda)$$ has $$n$$ roots in $$\mathbb{F}$$, it is diagonalizable over $$\mathbb{F}$$</p>
<p></p>
<p>The only case where this would not happen given some characteristic polynomial is if there is an irreducible quadratic polynomial, and this would mean it is not factorable over $$\mathbb{R}$$.</p>
<p></p>
<p>So $$h(\lambda)$$ has to be the constant polynomial for it to factor completely, as you said</p>
<p>Yes, basically if $$C_A(\lambda)$$ has $$n$$ roots in $$\mathbb{F}$$, it is diagonalizable over $$\mathbb{F}$$</p>
<p></p>
<p>The only case where this would not happen if we have the characteristic polynomial is if there is an irreducible quadratic polynomial, and this would mean it is not factorable over $$\mathbb{R}$$.</p>
<p></p>
<p>So $$h(\lambda)$$ has to be the constant polynomial for it to factor completely, as you said</p>
We need to have $$n$$ roots with repetition to have any hope of diagonalizing the matrix.  So if you are factoring over the reals and some of the roots are non-real, then the matrix will not be diagonalizable.  If you have $$n$$ real roots, then you will need to go on and check that all of the geometric and algebraic multiplicities match. "
"Practice Final Q10: <p>This was my answer to practice final q10:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F9f30fef46e82c69a5dc6979d2448ebcf62652ef0e69485df1eac88ff95ea6fa9%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>I have this theorem from class (here&#39;s a picture of my lecture notes):</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2Fd2a3ed9ec2eb893a881360a3778a79ad85f8e4ebd8606c45f92f61d1a8f46368%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Apologies for the bad handwriting, but the gist of it is that if the sum of the geometric multiplicities is $$n$$ then $$A$$ is diagonalizable. It&#39;s a little more powerful / useful than the Diagonalizability Test but I am unsure of whether the markers will recognize / accept this. The sample solutions are a lot more detailed than mine since they use the regular Diagonalizability Test.</p>
<p></p>
<p>I know it was mentioned in previous posts that we can just cite &#34;by a theorem in lecture&#34; but I just wanted to double check</p>",I think you would want to explain this a bit more to be on the safe side. 
"[Final Q8]: Hi, anyone remember where did we learn about this fact? (and can we use it directly without justification on the exam?)<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F2ca8ffe1c2a53888ef917815cc6c33dfc3ad195b4af9723862648b514f264b6c%2F1712846597140.png"" alt=""1712846597140.pngNaN"" />","<p>Not sure if this was explicitly stated somewhere, so I am not sure if you can use it without proof. There are a couple ways to go about proving this though, connecting a few different theorems, and it is not very long.</p>
<p></p>
<p>Would you like to see how I would approach this?</p>
I think you can use this fact.  If you want to justify it consider the rank of $$A$$. "
"week 11 q1: <p>hi everyone, how we obtained [1 0 1] and [0 1 0]? Thanks!</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8n9v69s68t%2F2811847350b81110e99b21faa29b00bedc42f1a986045d0d6b684579c5538220%2Fimage.png"" alt=""image.pngNaN"" /></p>",The equation of the plane is $$x-z=0$$ and those are two possible direction vectors of the plane. 
Complements of a subspace: What have we learnt about the complement of a subspace? I haven&#39;t seen anything in the book,We haven&#39;t learned anything about it.   One good math skill in general is to know how to apply what you know to a new definition. 
"Practice final Q7b: <p>Is y an eigenvector of T(y) because it is in the form of Ay = $$\lambda$$y? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2F7f4ae8bcf60a366d91f88ed98a951b4ac6b8e7296afd1ddcfc0454eca84bfcc6%2FScreenshot_2024-04-11_at_11.25.16_AM.png"" alt=""Screenshot_2024-04-11_at_11.25.16_AM.pngNaN"" /></p>","<p>Yep! We have a theorem that the eigenvectors (and eigenvalues) of $$T(\vec x)$$ are the same as the eigenthings of $$A$$ where $$A$$ is the standard matrix associated to the linear transformation $$T$$</p>
<p></p>
<p>Basically to find the eigenthings of T, find the matrix for T and then find the eigenthings like we usually do with characteristic polynomials and such</p>
<p></p>
<p>Technically the form would be $$T(\vec x) = \lambda \vec x$$ but we already know that we can represent T as a matrix</p>"
"Sample Final Q1: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2F602bc7a98a5338d33e681f21b71f7c3f7a8a17611835c84b3943c3aeb4f71daf%2Fimage.png"" alt=""image.pngNaN"" width=""505"" height=""96"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbzfpydn4na%2F6dba79d0b641ef643db160c7998b7ab46a3d11ae64f92bad4e6d8211e0b23a37%2Fimage.png"" alt=""image.pngNaN"" width=""513"" height=""76"" /></p>
<p></p>
<p>For this question, I don&#39;t get intuitively how it makes sense that Aadj(A) is just the diagonal matrix with det(A) entries.</p>","<p>I lack any intuition for this course honestly, but this follows directly from the following theorem:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F7cece8ba34935333991dca2c96426400aa6dc2fb663498971c7022d61f703e09%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Perhaps someone else can jump in with some actual intuition though! </p>"
quiz 5: may I ask what is P over here? ,"<md>If you mean question 2b, $$\vphantom{}_{\mathcal E}[P]_{\mathcal B}$$ is the change of basis matrix from $$\mathcal B$$ to $$\mathcal E$$ (more commonly denoted as $$\vphantom{}_{\mathcal E}[I]_{\mathcal B}$$)</md>"
"Question about rank of augmented matrix: When asking about the rank of an augmented matrix [A|b], if the last pivot is in augmented part, do we consider it&#39;s rank same as rank(A) or rank(A)&#43;1?","<p>if your matrix looks like this: $$\begin{bmatrix} 1 &0 |& 3 \\ 0 & 0|& 1 \\ \end{bmatrix}$$, then the rank of A (without the the augmented section) is 1. but rank([A|b]) = 2. </p>
<p></p>
<p>Does that answer your question?</p>
<p>if your matrix looks like this: $$\begin{bmatrix} 1 &0 |& 3 \\ 0 & 0|& 3 \\ \end{bmatrix}$$, then the rank of A (without the the augmented section) is 1. but rank([A|b]) = 2. </p>
<p></p>
<p>Does that answer your question?</p>"
"Weekly Practice Problem 10, Warmup Q5: <p>Hi, can someone please explain to me why the set B is a basis? Since it is in R^2 then would the basis need 2 vectors instead of one? Thanks for explanation.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb00ovad5em%2F68354b41a07d06db3628506de2f6540240ea789a34639dbd1a3be6d3bf292fdf%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwb00ovad5em%2Fd23a4086af2363aac8f9b86b11d971bd04fdc6d8f27634513d2ae4b1cf14950a%2Fimage.png"" alt=""image.pngNaN"" /></p>","We aren&#39;t saying it is a basis for R^2, we are saying it is a basis for the span of the line. Since a line has a dimension of 1, then we only require 1 vector in the basis"
"Theorem 6.4.5: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2F96a79c14eb6876aa666ce90de552db4b813e7fa7829fbf14955f76ecccd404c4%2FScreenshot_2024-04-11_at_12.16.09_PM.png"" alt=""Screenshot_2024-04-11_at_12.16.09_PM.pngNaN"" />Why this theorem can be stated without proof? This result is not obvious at all to me. Could someone prove or give me some intuition of this theorem? Thank you","If you are asking why it&#39;s not proved then I&#39;m not sure. If you were in Professor Bauman&#39;s class, I believe we did an example proof(or an engineering proof) where we considered one example with a matrix A, solved for the adj(A) and we found that Aadj(A) was equal to diag(det(A),det(A)...). I agree the result may not be super obvious, but maybe work through some examples so convince yourself it&#39;s true.
If you are asking why it&#39;s not proved then I&#39;m not sure. If you were in Professor Bauman&#39;s class, I believe we did an example proof(or an engineering proof) where we considered one example with a matrix A, solved for the adj(A) and we found that we got diag(det(A),det(A)...). I agree the result may not be super obvious, but maybe work through some examples so convince yourself it&#39;s true.
This result is tedious to prove. "
"Quiz 5: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fd1163e15c00f20ef52590c379563c50c38ab068e067eecc3c9ec2657844d620e%2FScreenshot_2024-04-11_at_1.06.26_PM.png"" alt=""Screenshot_2024-04-11_at_1.06.26_PM.pngNaN"" />Could someone please further explain this question<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fb27086f22023f8c6e6c8aae6e44912cc85cb30155f55ac9955af0e6f54c4f438%2FScreenshot_2024-04-11_at_1.06.12_PM.png"" alt=""Screenshot_2024-04-11_at_1.06.12_PM.pngNaN"" />","<p>Intuitively, you can think of it like this,</p>
<p></p>
<p>$$\mathbb{F}^n$$ is a $$n$$ dimensional space. We need at least $$n$$ vectors, each &#34;giving access to a new dimension&#34; to reach all of $$\mathbb{F}^n$$. </p>
<p></p>
<p>In particular, adding a linearly independent vector a span is what gives access to a &#34;new dimension&#34;, so $$n$$ linearly independent vectors will span $$\mathbb{F}^n$$. Therefore if you had more than $$n$$ vectors, they must be linearly dependent, since once $$n$$ are linearly independent, the next one has to be in the span of the first $$n$$ since they span $$\mathbb{F}^n$$</p>
<p></p>
<p>You can also think of it in terms of matrices, if $$\dim(V)=n$$, then forming the corresponding matrix with the linearly independent vectors in base of $$V$$, the rank would equal $$n$$ so the matrix is consistent for all $$\vec{b}\in\mathbb{F}^n$$, so it spans $$\mathbb{F}^n$$. If there were less than $$n$$, the rank would be less than $$n$$, and if there were more than $$n$$, there have to be free variables, meaning they would be linearly dependent, contradicting that they from a basis</p>
<p></p>
<p>Intuitively, you can think of it like this,</p>
<p></p>
<p>$$\mathbb{F}^n$$ is a $$n$$ dimensional space. We need at least $$n$$ vectors, each &#34;giving access to a new dimension&#34; to reach all of $$\mathbb{F}^n$$. </p>
<p></p>
<p>In particular, adding a linearly independent vector a span is what gives access to a &#34;new dimension&#34;, so $$n$$ linearly independent vectors will span $$\mathbb{F}^n$$. Therefore if you had more than $$n$$ vectors, they must be linearly dependent, since once $$n$$ are linearly independent, the next one has to be in the span of the first $$n$$ since they span $$\mathbb{F}^n$$</p>
<p></p>"
Inverse for non-square matrices: Do we need to know how to deal with inverses of non-square matrices? I noticed in one of our written assignments there was a question related to inverse of non-square matrices but I don&#39;t see anything about it in our notes,"Non square matrices are not invertible.<div>@1262<br /><div><br /></div></div>
Non square matrices are not invertible.<div><br /></div>"
"Diagonaziability test: <p>Hey, I’m a bit confused to why there is condition for a $$h(\lambda)$$ non-irreducible polynomial for $$C_{A}(\lambda)$$. Why is the condition necessary (since 1 could always be considered a constant)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fcbeb77a0b3a2b76a9a00d8b421d9eb8e0649e2047ab15a315a02804361498c0a%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p></p>","<p>In this case, $$h(\lambda)$$ is a constant, but this might not always be the case. If the polynomial had complex roots, then $$h(\lambda)$$ would not be constant (working in $$\mathbb{R}$$)</p>
<p></p>
<p>Also, I think you mean to say, &#34;there is a condition for $$h(\lambda)$$ to <strong>not</strong> be an irreducible polynomial&#34;</p>
In this case, $$h(\lambda)$$ is a constant, but this might not always be the case. If the polynomial had complex roots, then $$h(\lambda)$$ would not be constant (working in $$\mathbb{R}$$)
In this case, $$h(\lambda)$$ is a constant, but this might not always be the case."
"Eigenspace Basis: <p>To clarify this idea, is the basis of an eigenspace simply the vectors spanning the eignspace?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fddf691bfc78b8b4b1732ba9abd1691de0c4b0a53ef41b6ecfea8bbe8f001f60a%2Fimage.png"" alt=""image.pngNaN"" /></p>","Yes, provided these vectors are linearly independent. "
"[span PP10Q2]: <p>Why we can have span(s) $$\subseteq$$ span(t)? (Is T a subspace of F$$^{n}$$)<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F771d32f4f200b94114f42bc91517da4fd4baaf4ac0ace730af1c9abd732ec390%2F1712857827570%281%29.png"" alt=""1712857827570(1).pngNaN"" /></p>
<div>
<div></div>
</div>","<md>The span of any set of elements is always a subspace, but to see why $$\text{Span}(S) \subseteq \text{Span}(T)$$, see that $$S \subseteq T$$, so any linear combination of elements in $$S$$ is also a linear combination of elements of $$T$$ (namely, the same elements, which happen to all be elements of $$T$$ anyway).</md>"
"Mobius question: <p>What am i doing wrong? Arent I supposed to be doing b[I]e multiplied by [T]e? how did they get that matrix instead?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwekj0scwfe%2F929378bffca1e667b444ea4ec08bb15dd8460002696561b6802d5d67c843aeca%2Fimage.png"" alt=""image.png"" /> </p>","$$[T]_B = {_B}[I]_E$$  $$[T]_E$$  $${_E}[I]_B$$
$$[T]_B = {_B}[I]_E [T]_E {_E}[I]_B$$
$$[T]_B = {_B}[I]_E [T]_E {_E}[I]_B$$
$$[T]_B = _{B}[I]_{E} [T]_E _{E}[I]_{B}$$"
"Writing Steps for Reducing to RREF: <p>Will we lose marks for not writing the steps/operations explicitly when reducing to RREF (e.g., if I&#39;m multiplying row 2 by 1/3, can I simply give the matrix that results from this operation without stating R2-&gt;1/3 R2)?</p>
<p></p>
<p>I know this may seem like a small thing, but when there are many steps involved in reducing to RREF, it can get quite time consuming, especially if the reduction is just one part of the question.</p>",It would be good to still write down the steps.  I don&#39;t think you will be penalized for not writing them.  But that may be just my opinion and other instructors may not agree. 
"W10pp - Question 7: Could someone explain why v not being equal to the zero vector for sure implies that there will be a pivot in that column? For that matter none of the standard basis vectors are zero vectors either.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F85eaa5aa838a48acd600cbbbf33f06a303c43d400273a29b9e6431f06b32a5a8%2FScreenshot_2024-04-11_at_2.12.43_PM.png"" alt=""Screenshot_2024-04-11_at_2.12.43_PM.pngNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fd3ee0f7c812a4dc468699c81e270601077f08f45867140972015b26cf04332d0%2FScreenshot_2024-04-11_at_2.12.50_PM.png"" alt=""Screenshot_2024-04-11_at_2.12.50_PM.pngNaN"" />",The first entry in at least one of the rows of the matrix must be non-zero so there must be a pivot in this column. 
"W10pp - Question 10: Could someone please explain this more clearly, I&#39;m not able to understand the part, as to how the basis B for V is also a basis for F^n. <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fd2381449c1a559bccd019106c5942d25ef21ff19951cf271a450a0e7099cd0a1%2FScreenshot_2024-04-11_at_2.15.22_PM.png"" alt=""Screenshot_2024-04-11_at_2.15.22_PM.pngNaN"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Fddd0bd45524a6547f105d35fb21eeef809d593c2ab1d552ec7e7f08390e662fa%2FScreenshot_2024-04-11_at_2.15.31_PM.png"" alt=""Screenshot_2024-04-11_at_2.15.31_PM.pngNaN"" />","Linearly independent implies that these vectors spans $$F^n$$ (see notes for the proposition that proves this). Then, by the definition of a basis, B is a basis for $$F^n$$."
"Would appreciate hints for b: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2F337e344d57853b6e1e5dd0878fe5399599bf74796e6e8273f0deb69e35f901f2%2FScreenshot_2024-04-11_at_2.20.52_PM.png"" width=""1800"" height=""268"" alt="""" />",Part b) is chapter 10 material which is not covered this year. 
"WP11 - Warmup - Q1: <p>Could someone explain this in more detail.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F6e2bf3b7207363cbc115bea831b29b63277d34323a10c5ac90667939dca55ab6%2FScreenshot_2024-04-11_at_2.23.51_PM.png"" alt=""Screenshot_2024-04-11_at_2.23.51_PM.pngNaN"" /></p>","<md>Do you mean to explain the question statement, or to explain the solution method?

The solution is that if we want to find a basis $$\mathcal B$$ so that $$[\text{refl}_{\vec n}]_{\mathcal B}$$, then we should expect that $$\mathcal B$$ is a basis of eigenvectors.

Then, we can think about which vectors should be eigenvectors based on this reflection. Any vector on the plane should stay in the same spot after the reflection, and any normal vector should be reflected across the origin, i.e. $$T(\vec n) = -\vec n$$.</md>"
"W11pp - Question 1: Could someone explain the last step where they write refln B<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2Ff7bede475e2acce48439d8056381bb103161736502cbc1d16f5d32c81a3756d0%2FScreenshot_2024-04-11_at_3.15.15_PM.png"" alt=""Screenshot_2024-04-11_at_3.15.15_PM.pngNaN"" />",See @1118 and @1257_f1.
"Calculating eigenvalues: Is there any way to simplify the calculation for this to find the eigenvalues, consider det(A-lambdaI) but the calculation is still lengthy could somone show how to efficiently calculate this <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F438726bd4ff5ce1817c53bea6bdc12e33e799b0c6fa8a277e7a8118a55962226%2FScreenshot_2024-04-11_at_3.18.37_PM.png"" alt=""Screenshot_2024-04-11_at_3.18.37_PM.png"" />",Subbing in $$u = 4 - \lambda$$ might help but I dont know for sure
"Final sample test q8: Does this proof make sense?<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweal2mh66cw%2F1dde3d6e942d6ddab8d7ba81dac9e7a248688314bd8792e83426eb556f71fd3c%2F_____20240411151813.png"" width=""2360"" height=""1640"" alt="""" />","i dont think you can be sure that your scalars $$c_1,...,c_n$$ are the same for both bases"
"find eigenvalues: How to know what is Cn-2 and other C in those two properties?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fsodfioaajqox%2F3eb58a9cd74a961368df291808bb37eb_2024_04_11.jpeg"" /><img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fmkbbeqhrpely%2F5bb0b6f5a7fc01013dbc9adb403934b0_2024_04_11.jpeg"" /><p></p><p></p>",You dont know unless you explicitly calculate $$det(A - \lambda I)$$.
"[Similar Matrices and Characteristic Polynomial]: <p>What would a proof for the fact that similar matrices have the same characteristic polynomial look like?  <br /><br />Heres what I came up with but it feels too messy<br /><br />$$A = P^{-1}BP$$</p>
<p>Say $$B\vec{x} = \lambda \vec{x}$$. Since $$B=PAP^{-1}$$ we have <br />$$PAP^{-1}\vec{x} = B\vec{x}$$<br />$$PAP^{-1}\vec{x} = \lambda\vec{x}$$<br />(multiplying by $$P^{-1}$$) we get </p>
<p>$$A(P^{-1}\vec{x}) = \lambda(P^{-1}\vec{x})$$<br />So A and B share the same eigenvalues<br /><br />anyone have an easier way?</p>","<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqweal2mh66cw%2F27d742059213ef775c8515e2e9b1b4516869f1f508ba87f7751267d43fbb99a0%2FScreenshot_2024-04-11_152818.png"" width=""1175"" height=""871"" alt="""" /></p>
<p>a proof from shane&#39;s slides.</p>"
"Having difficulty with most proofs in practice problems: <p>Feeling a bit discouraged as lots of the proofs in the recommended problems don&#39;t come naturally or intuitively to me even if I&#39;ve seen the answer before. </p>
<p></p>
<p>Some of them, i understand that theyre true or false intuitively but to prove it mathematically is a challenge.</p>
<p></p>
<p>The questions on the practice final weren&#39;t as proof heavy so the final won&#39;t be as proof heavy right?</p>","40% of the marks on the practice final were proof based, I think you can expect the same on the actual final."
"W09: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2Fce88fd14895b1ede054c48930982e3c0245ff550aceabc3fb4e79a2909bc4633%2FScreenshot_2024-04-11_at_3.27.45_PM.png"" alt=""Screenshot_2024-04-11_at_3.27.45_PM.pngNaN"" /></p>
<p>I understand the solution to this problem, but I still feel a little confused about how to begin with problems as such. Would appreciate any tips</p>",where is this problem
"[Quiz 5 Q4]: <p>For this question, can we consider c1v1 &#43; c2v2 &#43; ... &#43;cncn = 0 vector where c1=...=cn=0, then take the Transformation for both sides: T(c1v1) &#43; ... = T(0 vector) = 0 vector (since zero maps to zero). Finally, moving the ci outside and getting c1T1(v1) &#43; .... cnT(vn) = 0 vector. Will this process be a valid one?</p>
<p>Thx in advance :D<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwehdad3p7l6%2F739c7732e88f6679dbf164873430bcca7ab35cdae14b055ecffcb1bc4e0e185d%2F1712864242758.png"" alt=""1712864242758.pngNaN"" /></p>",Yeah I am pretty sure this is valid. This is actually what I did on the quiz and I got full marks.
"How do we find the basis for the nullspace and range for this question: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2F15816f8a243bd29ea7df11bacd584e70a15f3ab5278a575eeedb646a00ac9241%2FScreenshot_2024-04-11_at_4.11.57_PM.png"" width=""1338"" height=""220"" alt="""" />What does $$P_2, P_3$$ mean","<md>It looks like $$P_i$$ is the set of all real polynomials of degree at most $$i$$.

Different vector spaces aren't covered on our final.</md>"
"Incorrect total points: I wanted to calculate the score id get on the practice final to see where im at, however I think it is out of more than 50 points despite the information on the title page. Is this an error?",It sure is!  I believe it is out of 63. 
"W11 - question3: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F640c7322c5c1e974ec4bf82397622c9812384ef6ad12efa6edbaad07d5d6f568%2FScreenshot_2024-04-11_at_4.26.25_PM.png"" alt=""Screenshot_2024-04-11_at_4.26.25_PM.pngNaN"" /></p>
<p>For this question, since A is a nxn matrix, means it should have n eigenvalues (repeated possibly) in F, since it says we only have one eigenvalue for A, that essentially means that we have n repeated eigenvalues all equal to lambda, such that D = diag(lambda, ...., lambda) right?</p>",Yeah sounds good 
"practice final: in part b, why we can know that dim(Span(S)) = rank(A) ?<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fnfwukxixgeca%2F1c5fc80c33674ba4e14a24f4a65369c2_2024_04_11.jpeg"" /><p></p>","If we consider a matrix A whos columns are the vectors in the Span(S), then by the Pivots and LI Theorem, we would pick all the vectors which have pivots in them to form our basis. By definition, we would have as many vectors in the basis as pivots, and the number of pivots is just the rank(A). "
"WP11: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F7cf3964232ecb198cbad6cf3a6634396010b85d983bf9d3a2f6aeece78e8fd90%2FScreenshot_2024-04-11_at_4.34.53_PM.png"" alt=""Screenshot_2024-04-11_at_4.34.53_PM.pngNaN"" />can someone explain why exactly is this so obvious?","I think its because a diagonal matrix cares that all entries not on the diagonal are 0, and in the zero matrix this is trivial."
"Unable to Access Past Exams on MathSoc Exam Bank: <p>Good afternoon.</p>
<p>I am trying to access some of the previous finals posted in the MathSoc Exam bank for additional review, but am unable to do so. I am prompted to download the file, but when I try to open it, the file does not load. I have tried in Microsoft, Google Chrome, and a private browser and nothing seems to work.</p>
<p>Any advice or suggestions would be greatly appreciated. </p>
<p>Thanks!</p>","I’ve found that some open fine and some just won’t, so it might be a problem with the files on their end. I’d suggest emailing the mathsoc office to see if they can fix it but it probably won’t be in time for the exam tomorrow <div><br /></div><div>Have you been able to open any of them?</div>
I’ve found that some open fine and some just won’t, so it might be a problem with the files on their end. I’d suggest emailing the mathsoc office to see if they can fix it but it probably won’t be in time for the exam tomorrow "
"Similarity of A and $$A^{T}$$: On the final, are we allowed to use without proof, the fact that A and $$A^{T}$$ are similar over F?","While this fact is true, we don&#39;t have such a result and to prove it involves more than we have in this course. "
"Linear Transformations (One-to-One/Onto): <p>If we are told that a linear transformation is one-to-one then does that mean it is linearly independent.</p>
<p></p>
<p>And what information could we use in a proof if given that the transformation was onto (I know it means the range(T) = F^n but I am a bit confused on how to utilize this in a proof).</p>
<p>Thanks.</p>","<md>- "" If we are told that a linear transformation is one-to-one then does that mean it is linearly independent.""

If the matrix is a square matrix then we can say it is linearly independent by the invertibility criteria and Pivot and LI theorem.

- ""And what information could we use in a proof if given that the transformation was onto (I know it means the range(T) = F^n but I am a bit confused on how to utilize this in a proof).""

We can also say $$Col(A) = \mathbb F^n$$, $$rank(A) = m$$, refer to the Onto Criteria.</md>
<md>- "" If we are told that a linear transformation is one-to-one then does that mean it is linearly independent.""

If the matrix is a square matrix then we can say it is linearly independent by the invertibility criteria and Pivot and LI theorem.

- ""And what information could we use in a proof if given that the transformation was onto (I know it means the range(T) = F^n but I am a bit confused on how to utilize this in a proof).""

We can also say $$Col(A) = \mathbb F^n$$, $$rank(A) = n$$, refer to the Onto Criteria.</md>"
"How do I solve this?: <p>Q1. Let A be a n x n invertible matrix. Determine det(adj(A)).</p>
<p></p>
<p>Q2. Let $$X = [ \vec{v}_1 \dots \vec{v}_k]$$ and $$Y = [\vec{u}_1 \dots \vec{u}_k]$$ be n x k matrices. Prove that if $$ Span\{\vec{v}_1, \dots, \vec{v}_k\} \subseteq Span\{\vec{u}, \dots, \vec{u_k} \}$$, then there exists Z such that X = YZ</p>","<p>Hints:</p>
<ol><li>We know $$\mathrm{adj}(A)\cdot A = \det(A)\, I_n$$. Throwing determinant <em>carefully</em> gives $$\det(\mathrm{adj}(A))=...$$</li><li>Since the span of $$v_i$$ is contained the span of $$u_i$$, each $$v_i$$ can be linearly expressed by $$u_i$$. The coefficients of these linear expressions naturally give you the desired matrix $$Z$$. </li></ol>
<p>I will leave the details for you all to complete.</p>
<p>Hints:</p>
<ol><li>We know $$\mathrm{adj}(A)\cdot A = \det(A)\, I_n$$. Throwing determinant <em>carefully</em> gives $$\det(\mathrm{adj}(A))=...$$</li><li>Since the span of $$v_i$$ is contained the span of $$u_i$$, each $$v_i$$ can be linearly expressed by $$u_i$$. The coefficients of these linear expression naturally gives you the desired matrix $$Z$$. </li></ol>
<p>I will leave the details for you all to complete.</p>"
"Practice Final Q10: Can we directly get a1 = a2 = g1 = g2 = 1 with a quick proof right? How can we ensure that the h(lambda) is constant in this question? Thanks<img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flqzes2lznxt50s%2Flkqwehxe2xz7ok%2Fjamouqbdbcat%2Fbb16ab529d40beff62cc77aa0db8d91e_2024_04_11.jpeg"" /><p></p>","<md>- How can we ensure that the h(lambda) is constant in this question?

The algebraic multiplicities $$a_1 + a_2 = n$$ implies that $$\lambda_1$$ and $$\lambda_2$$ are the only roots, so the $$h(\lambda)$$ have to be a constant term. If not, $$a_1 + a_2 \neq n$$.

see @1292</md>
<md>- How can we ensure that the h(lambda) is constant in this question?

The algebraic multiplicities $$a_1 + a_2 = n$$ implies that $$\lambda_1$$ and $$\lambda_2$$ are the only roots, so the $$h(\lambda)$$ have to be a constant term. If not, $$a_1 + a_2 \neq n$$.</md>
<md>- How can we ensure that the h(lambda) is constant in this question?

The algebraic multiplicities $$a_1 + a_2 = n$$ implies that $$\lambda_1$$ and $$\lambda_2$$ are the only roots, so the $$h(\lambda)$$ have to be a constant team. If not, $$a_1 + a_2 \neq n$$.</md>"
Simple question: What can we know about s if s is a subset of F^n and s = F^n?,"<p>If the subspace is just $$F^n$$ then you could say that the basis for S would contain n vectors(hence dim(S) = n).</p>
<p></p>
<p>I don&#39;t really get why it&#39;s important to mention that S is a subset of F^n is S = F^n though</p>"
"Week 11 PP - Det: <p>What is the easiest way to calculate the eigenvalues here - I know you can row reduce a bit - but the process is still pretty long/algebraic?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwcmng7k212i%2Fed6e9727d065c59ea7ff2a32182cd41478487bf9bf58135c0d2e115294720ded%2Fimage.png"" alt=""image.png"" /></p>","I would say for the most part yeah it&#39;s just pretty tedious work. I did see someone suggest substituting u = $$4 - \lambda$$ if thats something you want to try.
<p>$$ \det\begin{pmatrix} 4-\lambda & 2 & 2\\ 2 & 4-\lambda &2\\ 2 & 2 & 4-\lambda \end{pmatrix}$$</p>
<p>$$= \det\begin{pmatrix} 4-\lambda & 2 & 2\\ 2 & 4-\lambda &2\\ 0 & 0 & 2-\lambda \end{pmatrix} + \det\begin{pmatrix} 4-\lambda & 2 & 2\\ 2 & 4-\lambda &2\\ 2 & 2 & 2 \end{pmatrix} $$</p>
<p>$$= (2-\lambda)\det\begin{pmatrix} 4-\lambda & 2 \\  2 & 4-\lambda \end{pmatrix} + \det\begin{pmatrix} 2-\lambda & 0 & 0\\ 0 & 2-\lambda &0\\ 2 & 2 & 2 \end{pmatrix} $$</p>
<p>$$= (2-\lambda) (\lambda^2-8\lambda+12)+2(2-\lambda)^2 = (2-\lambda)(\lambda^2 - 10\lambda +16) = (2-\lambda)(\lambda-2)(\lambda-8)$$</p>
<p></p>
<p>where the formula</p>
<p>$$ \det\begin{pmatrix} a_1+b_1 & a_2+b_2 & a_3+b_3 \\ \ast & \ast & \ast \\ \ast & \ast & \ast \end{pmatrix} = \det\begin{pmatrix} a_1 & a_2 & a_3 \\ \ast & \ast & \ast \\ \ast & \ast & \ast \end{pmatrix} + \det\begin{pmatrix} b_1 & b_2 & b_3 \\ \ast & \ast & \ast \\ \ast & \ast & \ast \end{pmatrix}$$</p>
<p>holds for any row and column. In fact this directly follows from the row/column expansion of determinant.</p>
<p>$$ \det\begin{pmatrix} 4-\lambda & 2 & 2\\ 2 & 4-\lambda &2\\ 2 & 2 & 4-\lambda \end{pmatrix}$$</p>
<p>$$= \det\begin{pmatrix} 4-\lambda & 2 & 2\\ 2 & 4-\lambda &2\\ 0 & 0 & 2-\lambda \end{pmatrix} + \det\begin{pmatrix} 4-\lambda & 2 & 2\\ 2 & 4-\lambda &2\\ 2 & 2 & 2 \end{pmatrix} $$</p>
<p>$$= 
(2-\lambda)\det\begin{pmatrix} 4-\lambda & 2 \\ 
 2 & 4-\lambda \end{pmatrix}
 + 
\det\begin{pmatrix} 2-\lambda & 0 & 0\\ 2 & 4-\lambda &2\\ 2 & 2 & 2 \end{pmatrix} 
$$</p>
<p>$$= (2-\lambda) (\lambda^2-8\lambda+12)+(2-\lambda)(4-2\lambda) = (2-\lambda)(\lambda^2 - 10\lambda +16) = (2-\lambda)(\lambda-2)(\lambda-8)$$</p>
<p></p>
<p>where the formula</p>
<p>$$ \det\begin{pmatrix} a_1+b_1 & a_2+b_2 & a_3+b_3 \\ \ast & \ast & \ast \\ \ast & \ast & \ast \end{pmatrix} 
=  \det\begin{pmatrix} a_1 & a_2 & a_3 \\ \ast & \ast & \ast \\ \ast & \ast & \ast \end{pmatrix}
+
\det\begin{pmatrix} b_1 & b_2 & b_3 \\ \ast & \ast & \ast \\ \ast & \ast & \ast \end{pmatrix}$$</p>
<p>holds for any row and column. In fact this directly follows from the row/column expansion of determinant.</p>"
"Part Marks: <md>![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Flkqwabn8bhm141%2F289d907e0cdc5e8284c4cb963c58d964f4ac1b7b76ab320b3ccbf8273989e29b%2Fimage.png)

Assuming one gets the incorrect matrix for part a, would doing the calculation in part b for example result in full or part marks for part b, as the final answer **is** incorrect, even if the calculation is was done right?</md>","Likely there will be part  marks process. <div><br /></div><div>If you look at the solutions for the quizzes you can see they are nice about questions that have parts and are sequential in a way. If you got the wrong matrix but used it the right way in the following, you’ll definitely receive part marks.</div><div><br /></div><div>Just don’t expect full marks for the following, because it is still wrong.</div><div><br /></div>"
Linear Independence =&gt; Invertibility??: Why does Linear Independence of a set B imply inevitability of the matrix A whose columns are the vectors in B. what if A is a 5 x 3 matrix. Why does the fact that the col(A) is independent imply that A is invertible? ,"Only square matrices are invertible. Besides that, if you have n vectors in $$\mathbb{F}^n$$ which are linearly independent then yes, the nxn matrix formed by inserting those vectors as columns is invertible.<br /><br />To understand why just think of rank. The matrix A would row reduce to the identity since the vectors are linearly independent. So RREF(A) = In which is in our invertibility criteria. Thus the matrix is invertible."
"WP 11 Q3: <p>On the warmup exercise for q3, how do we get these set of matrices? </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkggoe9bxe5d42u%2Feb3acfcbb670912e5ebb822cb61ac1aa54a98af3406dcdf00401b71d13a4e560%2FScreenshot_2024-04-11_at_6.38.56_PM.png"" alt=""Screenshot_2024-04-11_at_6.38.56_PM.pngNaN"" /></p>",A = PDP^-1<div><br /></div><div>P - Eigenvectors</div><div>D - Diag(eigenvalues)</div><div>P^-1 - Eigenvectors but inverse ofc</div>
"Follow up: for this Q, @1309, isnt [x]B = B[I]E[x]E? Why is it different suddenly",See @1139.
"week 11 - PP 10: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwc2gwxor57c%2Fd80aa57f460509076a8b0da0577530d75c908c75788b1f2bd6f823cdb1398e80%2Fimage.png"" alt=""image.png"" /></p>
<p>Shouldn&#39;t (d1 In - D) = diag(0, d1-d2, d1-d3, ..., d1-dn)?</p>",Yes!  Good catch!
"&#34;bounding&#34; theorems and their use: <p>Hello,</p>
<p></p>
<p>We have a number of theorems which place a bound on certain values. For example, rank bounds restricts the rank of a mxn matrix to min{m, n}. Another is how geometric multiplicity is always bound by algebraic multiplicity.</p>
<p></p>
<p>I noticed these theorems are especially useful when making contradictions in proofs, so I wanted to ask if there are any more &#34;bounding&#34; theorems. I tried thinking about one for dimension but was a little confused so I figured I should ask instead.</p>
<p></p>
<p>Thanks!</p>","<p>The dimension is bounded by $$n$$ (from $$\mathbb{F}^n$$) and 0. </p>
<p></p>
<p>Here&#39;s a proposition:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8iusqnz5hk%2F765d7c63c2be8202efde31bba5acba7d5106f845d91e607ea9000a5d1fd1b54c%2Fimage.png"" alt=""image.png"" /></p>
<p></p>
<p>Can&#39;t recall any more of these &#34;bound&#34; propositions off the top of my head, but if someone has any to add feel free! </p>"
"questions branch: in the finals, if we compute one subquestion (a) wrongly, and the next subquestion (b) uses the wrong answer from (a), but with the correct method, how would the marks be calculated in such scenario?
 thanks",The marker should carry through your wrong answer and and give you full marks.  (Provided that your wrong answer doesn&#39;t greatly simplify the next problem.)
"WA5 Q2(b): <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2Ff40bfeca6771902c9c01c7e5241c2caf6b06ded0fe91a25adec30223ef42a461%2Fe7c60746a9b02efede9664f95965af2.jpg"" alt=""e7c60746a9b02efede9664f95965af2.jpgNaN"" /><br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwej5x9pg6c%2F63ab514b4bccff11ccee9a0bfb1e3b6f60586b067652155350d8c1a38f4f4950%2Fd9795038545ad39658dda4c48616341.jpg"" alt=""d9795038545ad39658dda4c48616341.jpgNaN"" /><br />Is this a valid proof? Thanks so much!","<md>I don't see how you went from the third last line to the last line.

As a hint, since $$[\vec x]_{\mathcal B} = [\vec x]_{\mathcal C}$$ for all $$\vec x\in \mathbb R^n$$, what if you take $$\vec x = \vec v_1$$? Then take $$\vec x = \vec v_2$$, and so on.</md>"
"W11: It need not be invertible but this does imply that A is diagonalizable right? Also from what I&#39;ve come to realize there is no such connection between invertibility and the matrix being diagonalizable.<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F75b83600fe1b346db5666690ccf25709f602b4fd43e6f84d6fdd760825b67f26%2FScreenshot_2024-04-11_at_8.16.13_PM.png"" alt=""Screenshot_2024-04-11_at_8.16.13_PM.pngNaN"" />","<p>Yeah if the algebraic multiplicities are all 1, then A is diagonalizable since the geometric multiplicities will also always equal to 1.</p>
<p></p>
<p>Also yeah, in general there isn&#39;t really a connection between diagonalizability and invertibility.</p>
<p>Yeah if the algebraic multiplicities are all 1, then A is diagonalizable since the geometric multiplicity will always equal to 1.</p>
<p></p>
<p>Also yeah, in general there isn&#39;t really a connection between diagonalizability and invertibility.</p>
<p>Yeah if the algebraic multiplicities are all 1, then A is diagonalizable since the geometric multiplicity will always equal to 1, hence A will be diagonalizable. </p>
<p></p>
<p>Also yeah, in general there isn&#39;t really a connection between diagonalizability and invertibility.</p>"
"WA5 Q2a: <p>For the question</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2Ffee554340148a79b36e73a40a3c4dda1cf0434c982a899ca711708fafc938c27%2Fimage.png"" alt=""image.pngNaN"" width=""928"" height=""78"" /></p>
<p></p>
<p>Why is it not true that </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8m8y7to62l%2F9e8114f5b09a661cc9429c4eed216e972bd4e7f4e64bfce63c855f490a347537%2Fimage.png"" alt=""image.png"" width=""275"" height=""63"" /></p>
<p>?</p>","What is B?, a basis cannot be itself
What is B?"
WPP W11 Q1: I&#39;m wondering why similar matrices have same characteristic polynomial. ,The student answer in @1320 shows a proof of this
"Chapter 8: Could someone please tell the solution for this question<img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw929po8912g%2F466b6b38c48197e6144606d8ad6cfff28710d082343299b03af4b79ca0cfb9a3%2FScreenshot_2024-04-11_at_8.56.19_PM.png"" alt=""Screenshot_2024-04-11_at_8.56.19_PM.pngNaN"" />","If you row reduce $$A$$, what can you say about the pivots?"
Finding eigenvalues: If I got an equation of x^3 &#43; 6x^2 &#43; x &#43; 1 (polynomial with power of 3) how can I transform this equation to the form like:<div>(x - num)^num * (x - num)^num…..? Thanks</div>,"Ah yes, good ol factoring a cubic. I actually watched this video today and it helped me a lot, highly recommend:<div><br /></div><div><a href=""https://youtu.be/tMYmkRKfipo?si=bQl53M-5d5AWhapM"">https://youtu.be/tMYmkRKfipo?si=bQl53M-5d5AWhapM</a></div>"
"Pivot columns and linear independence: <div dir=""auto"">
<div>
<div>
<div>
<div>
<div>
<div dir=""auto"">
<div>if a 3x3 matrix has rank 2, can the non pivot column always be written as a linear combination of the two pivot columns?</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>",Yes! This follows from our theorem pivots and linear independence 
"Typo/Error in Texbook?: <p>On Page 179, Example 7.4.1 (below). Wouldn&#39;t the eigenvector be  {[−11]} instead of it flipped?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqw8irwlap5h0%2F48a5c8cc0e2ed062435cc478c26bb614a83c8d6cede1707ba91c4ef2e2292714%2Fimage.png"" alt=""image.png"" /></p>",@854
"Question about diagonalization equality: If P diagonalizes A, is it true that PAP⁻¹ = P⁻¹AP","No
No
yes
<md>The student answer is incorrect; in general, it is not true that $$P AP^{-1} = P^{-1} A P$$.

You should try coming up with some small counterexamples.</md>"
"WP 11 Q10 what question is this talking about?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Fa7b7fc7dacc08374aa3421fdd3edad91ce6b60274d435976901dd1a3b16c609a%2Fimage.png"" alt=""image.pngNaN"" width=""1259"" height=""378"" /></p>
<p>There is no WP08 Q12, is it Q11?</p>",<md>yes I think it is Q11 b)</md>
"If B contains only the zero vector, then shouldn’t it be linearly dependent ?: <p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwbxz9qtm4du%2F8bc1408c591167d4fffdface274d8a9de382f6113221761fd3f2f337fbdee4c7%2Fimage.png"" alt=""image.pngNaN"" /></p>
<p></p>
<p>If B contains only the zero vector, then shouldn’t it be linearly dependent ?</p>
<p>btw this is example 8.6.4</p>","But B doesn’t only contain the zero vector. It’s saying that the linear combination of the vectors in B equals to zero if and only if the coefficient are zero, which is the definition of a LI set."
"Proving linear independence vs proving spanning set for basis: <p>Hello,</p>
<p></p>
<p>In my lectures, I remember my prof would say that when we are proving a basis spans $$\mathbb{F^n}$$ we need n vectors, and then we could prove they are linearly independent or that they are a spanning set.</p>
<p></p>
<p>However, he basically always chose to prove linear independence. Could someone explain how the other option would work? How do we prove the vectors are a spanning set? Or is proving linear independence the only way?<br /><br />Thank you!</p>","There’s a theorem that states n vectors span F^n iff they are linearly independent. If you prove they are LI, they are also a spanning set."
"if det(A - $$\lambda I$$) = 0, then how is $$a_{\lambda}$$ = 2: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdbxatel6pr%2F499f29d347a5fe051e8c57d14b07c71f84794a25a12498e74a4f6d920d12791f%2FScreenshot_2024-04-11_at_10.56.22_PM.png"" alt=""Screenshot_2024-04-11_at_10.56.22_PM.pngNaN"" />","<md>The algebraic multiplicity $$a_\lambda$$ is determined by looking at the degree of the root in the characteristic polynomial.

The characteristic polynomial is $$c_A(x) = (x - \lambda)^2$$ (note that this is a polynomial in $$x$$, not $$\lambda$$, since our usual parameter $$\lambda$$ is already a variable). Thus, we can see that the root $$x = \lambda$$ has degree $$2$$ in the polynomial.</md>"
"Geometric and Algebraic Multiplicities Inequality proof p247: <img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwelvposqnn%2F33723d599089e2e546f69829443d5bf5f915569280dbd0ab9c6e626798caf6ee%2FScreenshot_2024-04-12_at_12.04.37_AM.png"" alt=""Screenshot_2024-04-12_at_12.04.37_AM.pngNaN"" />Could someone explain why the characteristic polynomials is $$(\lambda_i-\lambda)^kC_{M2}(\lambda)$$? I understand the upper left matrix&#39;s characteristic polynomials is $$(\lambda_i-\lambda)$$, but how can we just multiple the two characteristic polynomials(cp) to get the  cp for the whole matrix? Thank you","<p>Consider actually finding the characteristic polynomial and expanding by the first column, then the second column, and so on.</p>
<p></p>
<p>It should seem similar to this</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=paste%2Flkqwdweou4p2j4%2Feb493ccba6cde4049cf769c6cc13c7173aff9f7f69bacfb7982dce61c3f7d514%2Fimage.png"" alt=""image.png"" width=""545"" height=""242"" /></p>"
"Adjucate?: which chapter was the adjucate in? also, what’s the adjucate?","Chapter 6 I think, and its the transpose of the cofactor matrix of A.
Chapter 6 I think, and its the transpose of the cofactor matrix of A."
"Post-exam info: Will we be able to see our graded exam, or just the mark? Or neither?",You will see neither unless you make an appointment with your instructor after the semester is over. 
"Multiple part questions: <p>I know it&#39;s early but I was wondering what the marking will be like if there was a small arithmetic error. </p>
<p></p>
<p>For Q1, I think I made an error in my row reductions to get A-1 and I was wondering whether marks would be deducted for part b since you needed to use the inverse determined in part a. Also how many marks are usually deducted for arithmetic errors in matrix calculations</p>
<p></p>
<p>Thanks</p>
<p></p>
<p></p>","<p>I think for Q1 if you made a small mistake in the row reductions, it would probably be 1/2 or 1 mark off, and I don&#39;t think they would remove a point when you find $$\vec{x}$$ with the incorrect matrix.</p>
<p></p>
<p>You can also check your work during the test to see if $$A^{-1}$$ and $$\vec{x}$$ are correct, though it does take some time</p>
<p>I think for Q1 if you made a small mistake in the row reductions, it would probably be 1/2 or 1 mark off, and I don&#39;t think they would remove a point when you find $$\vec{x}$$ with the incorrect matrix.</p>
<p></p>
<p>Though, you can also check your work during the test to see if $$A^{-1}$$ and $$\vec{x}$$ are correct, though it does take some time</p>
They should carry your error through and give you full marks for part b if you used your wrong answer correctly. "
